---
title: 'Smol 训练手册：<br/> 构建世界级大语言模型的秘诀'
subtitle: >-
  一段贯穿挑战、决策以及训练最先进语言模型背后杂乱现实的实战旅程
description: 'Smol 训练手册：构建世界级大语言模型的秘诀'
authors:
  - name: Loubna Ben Allal

    url: 'https://huggingface.co/loubnabnl'
    affiliations:
      - 1
  - name: Lewis Tunstall

    url: 'https://huggingface.co/lewtun'
    affiliations:
      - 1
  - name: Nouamane Tazi

    url: 'https://huggingface.co/nouamanetazi'
    affiliations:
      - 1
  - name: Elie Bakouch

    url: 'https://huggingface.co/eliebak'
    affiliations:
      - 1
  - name: Ed Beeching

    url: 'https://huggingface.co/edbeeching'
    affiliations:
      - 1
  - name: Carlos Miguel Patiño

    url: 'https://huggingface.co/cmpatino'
    affiliations:
      - 1
  - name: Clémentine Fourrier

    url: 'https://huggingface.co/clefourrier'
    affiliations:
      - 1
  - name: Thibaud Frere

    url: 'https://huggingface.co/tfrere'
    affiliations:
      - 1
  - name: Anton Lozhkov

    url: 'https://huggingface.co/anton-l'
    affiliations:
      - 1
  - name: Colin Raffel

    url: 'https://huggingface.co/craffel'
    affiliations:
      - 1
  - name: Leandro von Werra

    url: 'https://huggingface.co/lvwerra'
    affiliations:
      - 1
  - name: Thomas Wolf

    url: 'https://huggingface.co/thomwolf'
    affiliations:
      - 1

affiliations:
  - name: Hugging Face

    url: 'https://huggingface.co'

translator:
  - name: Max

    url: 'https://github.com/max1ab'
    
published: 'Oct. 30, 2025'
tags:
  - research-article-template
  - scientific paper
  - data visualization

tableOfContentsAutoCollapse: true
seoThumbImage: /thumb.png
pdfProOnly: false
---

import Sidenote from '../components/Sidenote.astro';
import Image from '../components/Image.astro';
import HtmlEmbed from '../components/HtmlEmbed.astro';
import Note from '../components/Note.astro';
import Wide from '../components/Wide.astro';
import Quote from '../components/Quote.astro';
import Reference from '../components/Reference.astro';
import FullWidth from '../components/FullWidth.astro';
import image_27c1384e_bcac_807c_807b_fac08be1d884 from './assets/image/image_27c1384e-bcac-807c-807b-fac08be1d884.png';
import image_2931384e_bcac_80c4_ab02_f22c53e6fdee from './assets/image/image_2931384e-bcac-80c4-ab02-f22c53e6fdee.png';
import image_2931384e_bcac_8062_bc11_d1ee3706d996 from './assets/image/image_2931384e-bcac-8062-bc11-d1ee3706d996.png';
import Capture_decran_2025_10_20_a_13_25_47_2921384e_bcac_8087_83e5_fa7a40c1f342 from './assets/image/Capture_decran_2025-10-20_a_13_25_47_2921384e-bcac-8087-83e5-fa7a40c1f342.png';
import Capture_decran_2025_10_20_a_13_26_08_2921384e_bcac_80b5_ac36_fb73d6374208 from './assets/image/Capture_decran_2025-10-20_a_13_26_08_2921384e-bcac-80b5-ac36-fb73d6374208.png';
import img_75ae60ff_50be_48e1_aad2_a8fc56120d3d_2921384e_bcac_80c2_984b_d81404e4bb7c from './assets/image/75ae60ff-50be-48e1-aad2-a8fc56120d3d_2921384e-bcac-80c2-984b-d81404e4bb7c.png';
import Capture_decran_2025_10_21_a_11_11_38_2931384e_bcac_8008_ad8d_d5ab0c539d3a from './assets/image/Capture_decran_2025-10-21_a_11_11_38_2931384e-bcac-8008-ad8d-d5ab0c539d3a.png';
import Capture_decran_2025_10_21_a_15_34_27_2931384e_bcac_8066_834b_c485ae8d1fa5 from './assets/image/Capture_decran_2025-10-21_a_15_34_27_2931384e-bcac-8066-834b-c485ae8d1fa5.png';
import image_2471384e_bcac_8059_84a4_d4ce5ae3847c from './assets/image/image_2471384e-bcac-8059-84a4-d4ce5ae3847c.png';
import Capture_decran_2025_10_27_a_22_10_05_2991384e_bcac_802a_a6e6_dab0f9f410ec from './assets/image/Capture_decran_2025-10-27_a_22_10_05_2991384e-bcac-802a-a6e6-dab0f9f410ec.png';
import Screenshot_2025_10_24_at_09_37_24_2961384e_bcac_8055_9e8e_ffbd3a1aa368 from './assets/image/Screenshot_2025-10-24_at_09_37_24_2961384e-bcac-8055-9e8e-ffbd3a1aa368.png';
import Screenshot_2025_09_26_at_22_36_40_27a1384e_bcac_8063_94e0_f1c689e7d9b9 from './assets/image/Screenshot_2025-09-26_at_22_36_40_27a1384e-bcac-8063-94e0-f1c689e7d9b9.png';
import GtU8DnoWsAAruEG_28e1384e_bcac_8051_8122_ed6cacf8f632 from './assets/image/GtU8DnoWsAAruEG_28e1384e-bcac-8051-8122-ed6cacf8f632.png';
import Screenshot_2025_10_01_at_11_31_19_28e1384e_bcac_8005_8c5e_f0af3bf70372 from './assets/image/Screenshot_2025-10-01_at_11_31_19_28e1384e-bcac-8005-8c5e-f0af3bf70372.png';
import Screenshot_2025_10_30_at_13_02_36_29c1384e_bcac_80d6_a72d_ff34bc221b60 from './assets/image/Screenshot_2025-10-30_at_13_02_36_29c1384e-bcac-80d6-a72d-ff34bc221b60.png';
import image_2881384e_bcac_80d6_84fe_d705cb1eae0a from './assets/image/image_2881384e-bcac-80d6-84fe-d705cb1eae0a.png';
import image_2881384e_bcac_801d_9f3d_c875181b9dd1 from './assets/image/image_2881384e-bcac-801d-9f3d-c875181b9dd1.png';
import image_2881384e_bcac_80ed_9bdf_c077977d77b8 from './assets/image/image_2881384e-bcac-80ed-9bdf-c077977d77b8.png';
import h100_dgx_2891384e_bcac_80cf_9f86_ccf0653a79e5 from './assets/image/h100_dgx_2891384e-bcac-80cf-9f86-ccf0653a79e5.gif';
import lstopo_29c1384e_bcac_80c9_9715_cbfe9e73d86b from './assets/image/lstopo_29c1384e-bcac-80c9-9715-cbfe9e73d86b.jpg';
import image_2891384e_bcac_80e2_9cc5_c2c46c7ab39b from './assets/image/image_2891384e-bcac-80e2-9cc5-c2c46c7ab39b.png';
import image_27d1384e_bcac_80b1_9ffb_ec29d0021ccc from './assets/image/image_27d1384e-bcac-80b1-9ffb-ec29d0021ccc.png';

## 前言

在今天，训练一个高性能的大语言模型（LLM）究竟需要什么？

<Sidenote>

阅读时长：2-4 天。
</Sidenote>

已有的研究论文让这一切看起来很简单：战略性的架构选择、精心策划的数据集以及充足的算力。得到的结果光鲜亮丽，消融实验结构清晰。事后看来，每一个决策都显而易见。但这些报告只展示了成功的经验，它们没有捕捉到凌晨两点调试数据加载器（dataloader）的漫长时刻、损失函数（loss）的异常峰值，或是那个悄悄破坏训练的细微张量并行（tensor parallelism）Bug（详见后文！）。现实往往更加杂乱、更加繁琐，大量的决策都没能写进最终的论文

跟随我们一起揭开 [SmolLM3](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) 训练的幕后故事。这是一个用了 11T tokens 训练的 3B 多语言推理模型。这不仅仅是一篇普通的博客文章，更像是在解开由决策、发现和死胡同交织而成的乱麻，这些经历为“如何构建世界级语言模型”提供了深刻的洞察。

这也是我们模型训练长篇系列的终章：我们已经探讨了大规模构建数据集（[FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1)）、编排数千个 GPU 协同工作（[Ultra Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)），以及在流程的每一步选择最佳评估方案（[Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook)）。现在，我们将这一切揉合在一起，构建一个强大的 AI 模型。我们将带你走过完整的旅程——不仅是最终成功的秘诀，还有那些塑造了每一个决策的失败经历、基础设施故障和调试过程。

这个故事读起来像一部戏剧：你会看到极具前景的小规模消融实验有时无法推广到大规模训练；看到我们为什么在训练 1T Token 后选择重启；看到我们如何在保持强大英语性能的同时，平衡多语言、数学和代码这些相互竞争的目标；以及最后，我们如何后期训练（post-train）一个混合推理模型。

我们也尽量避免枯燥地罗列工作内容，而是倾向于将我们的冒险经历组织成一个故事。你可以把这看作是一份指南，旨在帮助任何试图从“我们拥有优秀的数据集和 GPUs”过渡到“我们构建了一个非常强大的模型”的人。我们希望这种开放性能够缩小研究与生产之间的差距，并让你下一次的训练运行少一些混乱。 

### 如何阅读这篇博客

你不需要从头到尾阅读这篇博客，而且事实上它现在太长了，很难一次性读完。博客被划分为几个不同的部分，可以跳着看，也可以单独阅读：

-  **训练指南（Training compass）：** 首先必须要讨论的是你是否应该预训练自己的模型。为避免盲目开始耗尽你的VC资金，我们会带你思考一些基本问题，并教你如何系统地思考决策过程。这是一个偏向战略的章节，如果你想直接看技术内容，可以快速跳过这一部分。
-  **预训练（Pretraining）：** 紧随训练指南之后的部分涵盖了构建稳定预训练方案所需的一切知识：如何运行消融实验、选择评估指标、混合数据源、选择架构、调整超参数，以及最终如何熬过漫长的训练马拉松。即使你不打算从头开始预训练，而是对持续预训练（continued pretraining，又称 mid-training）感兴趣，这一部分同样适用。
-  **后训练（Post-training）：** 在这部分内容中，你将学习如何最大限度地发挥预训练模型的性能。我们将介绍从 SFT、DPO 到 GRPO 的全套后训练流程，以及模型合并（model merging）的“黑魔法”与炼金术。让这些算法发挥作用的大部分知识都源于痛苦的教训，我们将分享这些经验，希望能让你少走弯路。
-  **基础设施（Infrastructure）：** 如果说预训练是蛋糕，后训练是奶油和顶部的樱桃，那么基础设施就是那台工业级的烤箱。没有它，一切都无从谈起；如果它坏了，你愉快的周末烘焙就会变成火灾隐患。关于理解、分析和调试 GPU 集群的知识散布在互联网的各种库、文档和论坛中。本节将详细讲解 GPU 布局、CPU/GPU/节点/存储之间的通信模式，以及如何识别并克服瓶颈。 

那么，我们该从哪里开始呢？选择你觉得最激动人心的部分，让我们出发吧！

<Sidenote>

如果你有任何问题或建议，欢迎在 <a href="https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook/discussions">社区标签页</a> 发起讨论！
</Sidenote>

## 训练指南：why → what → how



<HtmlEmbed frameless src="/embeds/training-compass.html" />

机器学习领域对“优化”有着近乎执着的关系。我们沉迷于损失曲线、模型结构以及训练吞吐量——毕竟，机器学习的根本目标正是在于优化模型的损失函数。但在深入这些技术细节之前，更根本的一个问题往往被忽视了： *我们真的需要训练这个模型吗？*

如下方的热力图所示，开源 AI 生态系统几乎每天都在发布世界级的模型：Qwen、Gemma、DeepSeek、Kimi、Llama 🪦、Olmo，这个名单每个月都在变长。这些不仅仅是研究原型或玩具示例：它们是生产级的模型，涵盖了从多语言理解到代码生成和推理的惊人广度。其中大多数都带有宽松的许可证，并有活跃的社区随时准备帮助你使用它们。

[热力图](https://cfahlgren1-model-release-heatmap.hf.space/)


这就引出了一个令人不安的事实：也许你*并不需要训练自己的模型*。

这作为一份“LLM 训练指南”的开头似乎有些奇怪。但许多失败的训练项目并非败在糟糕的超参数或 Bug 频出的代码上，而是败在有人决定训练一个他们根本不需要的模型。因此，在你致力于训练并钻研*how*如何执行之前，你需要回答两个问题：*why*你为什么要训练这个模型？以及*what*你要训练什么样的模型？如果没有明确的答案，你将浪费数月的算力和工程时间去构建一个世界上已经存在的东西，甚至更糟——一个没人需要的东西。

让我们从“why”开始，因为如果不理解训练模型的目的，你就无法在后续的所有事情上做出连贯的决策。

<Note title="关于本节" emoji="📍" variant="info">

本节与博客的其余部分不同：它较少涉及实验和技术细节，
更多关于战略规划。我们将引导你决定<b>是否需要从头开始训练，以及要构建什么样的模型。</b>
如果你已经深入思考过“why”和“what”， 
可以随意跳到 [伟大的模型都始于微小的消融实验](#every-big-model-starts-with-a-small-ablation) 章节进行技术深挖。
但如果你还心存疑虑，在这里投入时间将为你以后节省大量的精力。
</Note>

###  **why：那个没人想回答的问题** 

让我们坦率地谈谈实际发生的情况。某人（如果运气好的话）获得了 GPU 集群的使用权，也许是通过研究资助，也许是通过公司的闲置容量，其思考过程大致如下：“我们有 100 张 H100，可以用三个月。让我们训练一个模型吧！”模型大小是随意选定的，数据集是从现有的任何资料中凑齐的。训练开始了。六个月后，在耗尽了算力预算和团队士气之后，得到的模型却被束之高阁，因为从来没有人问过*why*。

以下是一些你不应该训练模型的理由：

<HtmlEmbed frameless src="/embeds/wrong-reasons.html" />



“我们训练了自己的模型”这种诱惑是巨大的，但在投入大量时间和资源之前，问一句“**你为什么需要训练这个模型？**”是有意义的。

下面的流程图引导了在启动大型预训练项目之前应该经历的思考过程。从技术角度来看，你基本上应该首先确认是否已经存在一个可以通过提示词（prompt）或微调（fine-tune）来完成工作的现有模型。

<HtmlEmbed frameless src="/embeds/train-model-decision-flowchart.html" />



<Sidenote>

我们讨论的“为什么”是关于从头开始训练。本博客不涉及蒸馏（distillation）或剪枝（pruning）。这些是实现高效模型的有效路径，但代表了与从头训练不同的工作流。我们推荐阅读 NVIDIA 的 <a href="https://arxiv.org/abs/2408.11796" target="_blank">Minitron 论文</a> 以了解这些话题的概况。
</Sidenote>

自定义预训练通常在以下三个领域具有意义：你想进行前沿研究、你有非常具体的生产用例需求，或者你想填补开源模型生态系统的空白。让我们快速浏览一下： 

####  **研究：你想了解什么？** 

在 LLM 领域有很多研究可以做。LLM 研究项目的共同点是，你通常从一个清晰定义的问题开始：

- 我们能否将这种新优化器的训练规模扩展到 10B+ 模型？来自 [Muon is Scalable for LLM Training ](https://huggingface.co/papers/2502.16982)
- 仅靠强化学习（不带 SFT）能否产生推理能力？来自 [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://huggingface.co/papers/2501.12948)
- 我们能否仅在合成教科书数据上训练出优秀的小模型？来自 [Textbooks Are All You Need ](https://huggingface.co/papers/2306.11644)
- 我们能否仅通过训练公开许可的数据来达到具有竞争力的性能？来自[ The Common Pile v0.1: An 8TB Dataset of Public Domain and Openly Licensed Text ](https://huggingface.co/papers/2506.05209)

让假设尽可能具体，并思考必要的实验规模，可以增加成功的机会。

####  **产出：为什么不能用现有的模型？** 

公司无法在其实际用例中使用现成模型的原因主要有三个。其中两个是技术性的，另一个是由于治理（governance）原因。

训练自己模型的第一个原因是**领域专业性（domain specificity）：** 当你的数据或任务涉及现有模型无法很好处理的高度专业化的词汇或结构时。例如：

* 具有独特词汇量和长程依赖关系的 DNA 模型。

* 需要深入熟悉领域特定术语和逻辑的法律或金融模型。

第二个相关原因是部署限制：当你需要一个根据你的硬件、延迟或隐私要求量身定制的模型时。例如，运行在无人机或使用 FPGA 等定制硬件的本地系统上的 LLM。

这里有一个简单的测试：花几天时间基于 Qwen3, Gemma3 或其他当前的 SOTA 模型进行构建。你能通过提示词、工具调用（tool-use）或后训练达到你的性能目标吗？如果不能，那可能就是时候训练你自己的模型了。

即使满足你要求所需的后训练预算巨大，它可能仍然比从头开始便宜。对模型进行 1T tokens 的微调仍然比从头开始训练 10T+ tokens 更经济。

<Sidenote>

在这一阶段，LLM 训练者们开始神奇地称之为“mid-training”
而不是“post-training”
</Sidenote>

构建自有内部语言模型的第三个原因是**安全与治理（safety and governance）：** 因为你处于受监管的行业或高风险应用中，你需要对训练数据、模型行为和更新周期拥有完全的控制权。你需要*确切地*知道模型中包含了什么，并能够向监管机构证明这一点。在某些情况下，除了构建自己的模型，你可能别无选择。

这些是公司训练内部模型的主要原因，那么那些发布开源模型的公司或组织呢？

####  **战略性开源：你是否看到了可以填补的空白？** 

经验丰富的 AI 实验室发布新开源模型的最常见原因之一是：他们在开源生态系统中发现了一个特定的空白或新的 AI 用例。

这种模式通常如下：你注意到一个未被充分探索的领域，也许还没有具备超长上下文的强大端侧模型，或者虽然存在多语言模型但在低资源语言上表现很弱，又或者领域正朝着 [Genie3](https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/) 等交互式世界模型发展，而目前还没有优秀的开源权重模型。 

你有理由相信自己可以做得更好；也许你策划了更好的训练数据，开发了更好的训练方案，或者有算力在别人无法做到的地方进行过度训练（overtrain）。你的目标是具体的：不是“有史以来最好的模型”，而是“最适合端侧使用的 3B 模型”或“第一个具备 1M 上下文的小模型”。

这是一个真实的目标，如果成功会创造价值：开发者采用你的模型，它成为他人的基础设施，或者它建立了技术公信力。但成功需要经验。你需要知道什么是实际可行的，以及如何在竞争激烈的空间中可靠地执行。为了让这一点更具体，让我们看看 Hugging Face 是如何思考这个问题的。

####  **Hugging Face 的旅程** 

那么，为什么 Hugging Face 要训练开源模型呢？答案很简单：我们构建对开源生态系统有用的东西，并填补那些很少人会去填补的空白。

<Sidenote>

虽然有数百万个开源权重模型，但训练完全开放模型的组织却很少。除了 Hugging Face，还有 [Ai2](https://allenai.org/) 和 [斯坦福大学的 Marin 社区](https://marin.community/)。
</Sidenote>

 这包括数据集、工具和训练模型。我们启动的每一个 LLM 训练项目都始于发现一个空白，并相信我们可以做出有意义的贡献。

我们在 GPT-3 [[@gpt3](https://huggingface.co/papers/2005.14165)] 发布后启动了第一个 LLM 项目。当时感觉似乎没有人在构建开源替代方案，我们担心这些知识最终会被锁在少数几个工业实验室里。因此，我们启动了 [BigScience workshop](https://bigscience.huggingface.co/) 来训练 GPT-3 的开源版本。最终诞生的模型是 [Bloom](https://huggingface.co/bigscience/bloom)，它是由数十名贡献者历时一年构建训练栈、分词器（tokenizer）和预训练语料库，并预训练出一个 175B 模型。

Bloom 的继任者是 2022 年的 StarCoder [[@starcoder](https://huggingface.co/papers/2305.06161)]。OpenAI 为 GitHub Copilot 开发了 Codex [[@codex](https://huggingface.co/papers/2107.03374)]，但它是闭源的。构建一个开源替代方案显然会为生态系统提供价值。因此，我们与 ServiceNow 合作，在 [BigCode](https://huggingface.co/bigcode) 的框架下构建了 [The Stack](https://huggingface.co/datasets/bigcode/the-stack) 数据集，并训练了 [StarCoder 15B](https://huggingface.co/bigcode/starcoder) 来复现 Codex。[StarCoder2](https://huggingface.co/collections/bigcode/starcoder2-65de6da6e87db3383572be1a) [[@starcoder2](https://huggingface.co/papers/2402.19173)] 源于我们意识到可以训练更长时间，并认识到训练时间更长的较小模型可能比一个大模型更有价值。我们训练了一个系列（3B/7B/15B），使用了数万亿个 Token，远超当时任何人对开源代码模型所做的工作。

[SmolLM 系列](https://huggingface.co/HuggingFaceTB) 遵循了类似的模式。我们注意到当时非常缺乏强大的小模型，而我们刚刚构建了 [FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu) [@fineweb]，这是一个强大的预训练数据集。[SmolLM](https://huggingface.co/collections/HuggingFaceTB/smollm-6695016cad7167254ce15966) (135M/360M/1.7B) 是我们的第一个版本。[SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9) [@smollm2] 专注于更好的数据和更长的训练时间，在多个方面达到了 SOTA（最先进）性能。[SmolLM3](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) 扩展到了 3B，同时增加了混合推理、多语言和长上下文，这些都是社区在 2025 年所看重的特性。

这种模式延伸到了预训练之外：我们训练了 [Zephyr](https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha) [@zephyr] 以展示 DPO 在大规模下的有效性；启动了 [Open-R1](https://github.com/huggingface/open-r1) 以复现 DeepSeek R1 的蒸馏流水线，并发布了用于竞赛编程的 [OlympicCoder](https://huggingface.co/open-r1/OlympicCoder-7B)，在国际信息学奥林匹克竞赛（IOI）中表现达到了 SOTA。我们还探索了其他模态，包括用于视觉的 [SmolVLM](https://huggingface.co/collections/HuggingFaceTB/smolvlm-6740bd584b2dcbf51ecb1f39) [@smolvlm] 和用于机器人的 [SmolVLA](https://huggingface.co/lerobot/smolvla_base) [@smolvla]。

<Sidenote>

如果你对此感兴趣，你可以在这里找到概览：<a href="https://huggingface.co/science" target="_blank">https://huggingface.co/science</a>
</Sidenote>

希望本节内容已经让你确信，深入思考为什么要训练一个模型是有价值的。

在博客接下来的部分中，我们将假设你已经做好了这种“灵魂拷问”，并拥有正当的训练理由。

### what：将目标转化为决策

现在你知道了*为什么*要训练，那么该训练*什么*呢？所谓“what”，我们指的是：模型类型（稠密模型、MoE 混合专家模型、混合架构或某种新架构）、模型大小、架构细节以及数据混合比例。一旦确定了“why”，你就可以推导出“what”，例如：

- 端侧快速模型 → 小型高效模型
- 多语言模型 → 大容量分词器词表
- 超长上下文 → 混合架构

除了由用例驱动的决策外，还有一些选择是为了优化训练本身，使其更稳定、采样效率更高或速度更快。这些决策并不总是那么明确，但你可以将决策过程大致分为两个阶段：

 **规划：** 在运行实验之前，将你的用例映射到你需要决定的组件上。你的部署环境决定了模型大小的约束。你的时间线决定了你可以承担哪些架构风险。你的目标能力决定了数据集的需求。这个阶段的核心是将你“why”中的每个约束条件连接到“what”中的具体规格。

 **验证：** 一旦有了起点和潜在修改列表，就要进行系统性测试。由于测试成本高昂，请专注于那些能够显著提高你的用例性能或优化训练的更改。这就是消融实验的用武之地，我们将在 [消融实验章节](#every-big-model-starts-with-a-small-ablation) 中详细介绍。

<Note title="学习辨识什么值得测试，而不只是学习如何测试。" emoji="📍" variant="info">

在无关紧要的选择上进行完美的消融实验，与在重要的选择上进行马虎的消融实验一样浪费算力。
</Note>

在接下来的章节中，你将了解到定义模型的所有选项，以及如何通过系统性实验缩小选择范围。在深入讨论之前，我们想分享一些关于如何组建团队和项目的经验，这些经验源于我们自己的模型训练，以及对其他构建出色 LLM 的优秀团队的观察。 

### 超能力：速度与数据

条条大路通罗马，但我们发现，让成功的 LLM 训练团队始终脱颖而出的是**迭代速度**。训练 LLM 实际上是一种“在训练中学习”的学科，你训练得越频繁，你的团队就会变得越优秀。 
因此，在每年训练一个模型的团队和每季度训练一个模型的团队之间，后者进步的速度要快得多。你可以看看 Qwen 和 DeepSeek 的团队。他们现在已是家喻户晓，在以快速节奏持续发布新模型方面有着悠久的记录。

除了迭代速度，到目前为止，LLM 训练中影响最大的方面是**数据策划（data curation）**。人们往往倾向于钻研架构选择以改进模型，但在 LLM 训练中表现出色的团队，通常是那些比任何人都更痴迷于高质量数据的团队。 

另一个与迭代速度相关的方面是团队规模：对于主要的预训练任务，你只需要少数几个拥有足够算力的人员即可。训练像今天 Llama 3 这样的模型，你可能只需要 2-3 个人。只有当你开始涉足更多元化的训练和下游任务（多模态、多语言、后训练等）时，你才会慢慢需要增加更多人手，以便在每个领域都表现卓越。 

因此，从一个规模虽小但装备精良的团队开始，每 2-3 个月构建一个新模型，在很短的时间内你就能攀上顶峰。现在，本博客的其余部分将专注于这支团队的技术日常！

## 伟大的模型都始于微小的消融实验

在开始训练 LLM 之前，我们需要做出许多决定，这些决定将塑造模型的性能和训练效率。哪种架构最适合我们的用例？该使用哪种优化器和学习率调度？又该如何混合不同的数据源？

如何做出这些决定是一个经常被问到的问题。人们有时期望这些决定是通过深思熟虑得出的。虽然战略性思考至关重要——正如我们在 [上一节](#training-compass-why--what--how) 中讨论如何识别值得测试的架构更改时所提到的——但仅靠推理是不够的。在 LLM 领域，事情并不总是符合直觉，关于“什么应该起作用”的假设在实践中有时并不能奏效。

例如，使用看起来“质量最高的数据”并不总能产生更强大的模型。以 [arXiv](https://arxiv.org/) 为例，它是人类科学知识的巨大宝库。直觉上，在如此丰富的 STEM 数据上进行训练应该能产生更优越的模型，对吧？但在实践中并非如此，尤其是对于较小的模型，这甚至可能损害性能 [@grpo]。为什么呢？原因在于，虽然 arXiv 论文充满了各种知识，但它们高度专业化，并且是以一种狭隘的学术风格编写的，这与能让模型学习效果最好的多样化、通用的文本截然不同。

那么，如果苦思冥想没有帮助，我们该如何知道什么有效呢？当然是像优秀的经验主义者一样运行大量实验！机器学习不是纯数学，而非常像一门实验科学。

<Sidenote>

在许多方面，机器学习类似于统计力学发现之前的热力学：我们拥有可靠的经验法则和设计原则，它们运行得异常出色，即使更深层次的理论解释仍在探索中。
</Sidenote>

由于这些实验将引导我们做出许多关键决策，因此设置好这些实验非常重要。我们主要希望它们具备两个属性：

1.  **速度：** 要运行得尽可能快，以便我们能够频繁迭代。运行的消融实验越多，能测试的假设就越多。 
1.  **可靠性：** 实验应该有很强的辨别力。如果我们关注的指标无法在早期就显著区分不同配置，那么我们的消融实验可能收获甚微（而且如果指标存在较大的噪声，我们可能陷入不停追逐噪声的困境！）。更多细节可以查看 [FineTaks 博客文章](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks)。

在设置消融实验之前，需要对架构类型和模型大小做出一些基础选择。这些决策（在我们的“指南”指引下）会影响训练框架的选择、如何分配算力以及从哪个基准（baseline）开始。

对于 SmolLM3，因为我们的目标是小型端侧模型，所以选择了 3B 参数的稠密 Llama 式架构。但正如你在 [设计模型架构章节](#designing-the-model-architecture) 中将看到的，MoE 或混合架构或许会更适合你的用例，不同的模型大小也伴随着不同的权衡。稍后我们将深入探讨这些选择。现在，让我们从第一步开始：选择你的 baseline。

###  **选择 baseline** 

每个成功的模型都建立在一个经过充分验证的基准模型上，并根据自身需求进行修改。 Qwen 从 Llama 架构开始训练他们的第一个模型系列 [@qwen1] 。Meta 从Llama 2 开始训练 Llama 3。Kimi K2 从 DeepSeek-V3 的 MoE 架构开始。这种方法适用于架构，也适用于训练超参数和优化器。

因为优秀的架构和训练设置设计需要跨组织多年的迭代。标准的 Transformer 和 Adam 等优化器通过成千上万次实验得到了完善。人们发现了它的失效，调试了不稳定性，并优化了实现方式。从经过验证的基础开始意味着继承了所有积累的知识。从零开始则意味着你需要自己重新发现每一个问题。

以下是优秀参考架构应具备的特点：

- 符合你的需求：与你的部署目标和用例相匹配。
- 经过大规模验证：在相似或更大规模下完成过数万亿 tokens 的运行。
- 文档齐全：已在开源模型中证明有效的已知超参数。
- 框架支持：理想情况下，它应该在你考虑的训练框架以及计划使用的推理框架中都得到支持。

下面是一份不完全的针对 2025 年各种架构和模型大小的 baseline 选项列表：

| 架构类型 | 模型系列 | 大小 |
| --- | --- | --- |
| **Dense** | [Llama 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f) | 8B, 70B |
| **Dense** | [Llama 3.2](https://huggingface.co/collections/meta-llama/llama-32-66f448ffc8c32f949b04c8cf) | 1B, 3B |
| **Dense** | [Qwen3](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) | 0.6B, 1.7B, 4B, 14B, 32B |
| **Dense** | [Gemma3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d) | 12B, 27B |
| **Dense** | [SmolLM2](https://huggingface.co/collections/HuggingFaceTB/smollm2-6723884218bcda64b34d7db9), [SmolLM3](https://huggingface.co/HuggingFaceTB/SmolLM3-3B) | 135M, 360M, 1.7B, 3B |
| **MoE** | [Qwen3 MoE](https://huggingface.co/collections/Qwen/qwen3-67dd247413f0e2e4f653967f) | 30B-A3B, 235B-A122B |
| **MoE** | [GPT-OSS](https://huggingface.co/collections/openai/gpt-oss-68911959590a1634ba11c7a4) | 21B-A3B, 117B-A5B |
| **MoE** | [Kimi Moonlight](https://huggingface.co/moonshotai/Moonlight-16B-A3B-Instruct) | 16B-A3B |
| **MoE** | [Kimi-k2](https://huggingface.co/collections/moonshotai/kimi-k2-6871243b990f2af5ba60617d) | 1T-A32B |
| **MoE** | [DeepSeek V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) | 671B-A37B |
| **Hybrid** | [Zamba2](https://huggingface.co/Zyphra/models?search=zamba2) | 1.2B, 2.7B, 7B |
| **Hybrid** | [Falcon-H1](https://huggingface.co/collections/tiiuae/falcon-h1-6819f2795bc406da60fab8df) | 0.5B, 1.5B, 3B, 7B, 34B |
| **MoE + Hybrid** | [Qwen3-Next](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) | 80B-A3B |
| **MoE + Hybrid** | [MiniMax-01](https://huggingface.co/MiniMaxAI/MiniMax-Text-01) | 456B-A46B |

根据你的架构类型选择一个参数量接近你预想目标的 baseline。也不用过度纠结，因为你开始选择的架构并非一成不变。在下一节中，我们将看到如何从 baseline 演进出最适合你的最终架构。

####  **改进 baseline：不要冒险** 

现在你已经有了一个行之有效且符合用例的 baseline。你可以就此止步，用你的混合数据集训练（假设数据质量还不错），也能得到一个不错的模型。这正是许多成功项目的做法。但 baseline 并非针对你的特定场景而优化，它是根据构建者的用例和部署目标而设计的。因此，进行一些修改以更好地对齐你的目标是更有价值的。然而，每一次架构更改都带有风险：它可能提升性能，也可能导致性能暴跌，或者在白白浪费消融实验算力的同时毫无作为。

让你保持在正确道路上的准则是 **不要冒险（derisking）**：不要改任何内容，除非你的测试证明它确实有用。

<Note title="怎样才算 derisking？" emoji="📍" variant="info">

当测试表明某项更改要么提升了某些性能，要么在不损害性能的情况下提供了实质性好处
（如更快的推理、更低的内存占用、更好的稳定性），且性能损失没有超出你可接受范围时，才算没有冒险（derisking）。
</Note>

最棘手的地方在于，baseline 和训练设置中有太多可以修改的组件：注意力机制、位置编码、激活函数、优化器、训练超参数、归一化方案、模型布局等等。每一个都意味着一个潜在的实验，而且这些组件往往以非线性方式相互作用。你既没有时间也没有算力去测试所有的东西或探索每一种组件之间的影响。

首先针对当前的 baseline 测试那些看起来最有希望的更改。当某项更改奏效时，将其集成以创建一个新的 baseline，然后再针对新的 baseline 测试下一项更改。如果算力预算允许，你可以单独测试各项更改并用“留一法”（leave-one-out）分析。

<Sidenote>

参考 ScaleRL 论文 [@scalerl] 了解这种方法论在实践中的应用。
</Sidenote>

不要想着对每一个超参数进行详尽网格搜索，或测试每一个新出的架构变体。

<Note title="战略性实验" emoji="🎯" variant="success">

如果你不知道哪些实验值得做，那么知道怎么运行实验也无济于事。
在测试任何修改之前，问自己两个问题：
- 这对特定用例有帮助吗？
- 这能优化训练吗？

如果某项修改无法明确回答这两个问题，请放弃它。
</Note>

现在你已经知道如何通过战略规划识别有潜力的更改，是时候进入 **经验验证** 阶段了。在接下来的章节中，我们将向你展示在实践中*如何*真正测试这些更改。我们将介绍如何建立可靠的实验、解读结果以及避免常见坑点。在随后的章节中，我们将通过具体案例，带你测试流行的架构、数据、基础设施和训练决策。

让我们建立一个简单的、可用于实验的消融实验设置。第一步，决定选择哪个训练框架。

### 选择训练框架

我们需要做的第一个决定是使用哪个框架来训练模型，进而运行我们所有的消融实验。这个选择需要平衡三个关键考虑因素：

<Sidenote>

不要逞能，在消融实验和最终正式运行之间切换训练框架。那是死路一条。
</Sidenote>

1. 框架必须支持我们的目标架构，或者能让我们轻松扩展。 
1. 要稳定且具备生产能力，不容易在训练中途莫名其妙地崩溃。
1. 能提供强大的吞吐量，以便我们能够快速迭代并充分利用算力。

在实践中，这些要求可能会相互冲突，需要权衡。让我们来看看现有的选项。

<Wide>

| 框架 | 特性 | 经受过实战检验 | 经过优化 | 代码行数 (核心 / 总计) | 扩展性与调试 |
| --- | --- | --- | --- | --- | --- |
| **Megatron-LM** | ✅ 极其丰富 | ✅ Kimi-K2, Nemotron | ✅ 3D 并行化的先驱 | 93k / 269k | ⚠️ 对初学者较难 |
| **DeepSpeed** | ✅ 极其丰富 | ✅ BLOOM, GLM | ✅ ZeRO 与 3D 并行化的先驱 | 94k / 194k | ⚠️ 对初学者较难 |
| **TorchTitan** | ⚡ 特性不断增加 | ⚠️ 较新，但由 PyTorch 团队测试 | ⚡ 针对稠密模型优化，MoE 改进正在进行中 | 7k / 9k | ⚡ 中等：需要并行化知识 |
| **Nanotron** | 🎯 极简，为 HF 预训练量身定制 | ✅ 是 (StarCoder, SmolLM) | ✅ 经过深度优化 (UltraScale Playbook) | 15k / 66k | ⚡ 中等：需要并行化知识 |

</Wide>

上表总结了流行框架之间的关键权衡。前三个框架的代码行数引用自 TorchTitan 技术报告 [@torchtitan]。让我们更详细地讨论一下：

Nvidia 的 [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) 已存在多年且久经沙场。它为 Kimi K2 [@kimik2] 等模型提供动力，拥有稳定的吞吐量和我们想要的大多数特性。但成熟也伴随着复杂性：当你刚开始接触时，代码库可能很难定位和修改。

[DeepSpeed](https://github.com/deepspeedai/DeepSpeed) 属于类似类别。它是 ZeRO 优化的先驱，曾助力 BLOOM 和 GLM 等模型。与 Megatron-LM 一样，它经过了广泛的实战检验和优化，但也面临同样的复杂性挑战。庞大的代码库（总计 19.4 万行）对于新手来说可能令人生畏，特别是在实现自定义特性或调试意外行为时。

另一方面，PyTorch 最近推出的 [TorchTitan](https://github.com/pytorch/torchtitan) 库要轻量得多，得益于其精简且模块化的代码库，它更容易定位代码。它具备预训练所需的核心特性，非常适合快速实验。然而，由于它比较新，还没有经过太多的实战检验，且随着活跃开发，可能仍然存在一些不稳定性。

我们走了一条不同的路，从头开始构建了自己的框架 nanotron。这赋予了我们完全的灵活性和对大规模预训练的深刻理解；这些见解后来演变成了 [Ultra Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)。自从我们开源了这个库，我们也从社区获得了宝贵的反馈，尽管在大多数情况下我们必须先自己对特性进行实战测试。该框架现在支持训练所需的所有生产特性，但我们仍在完善 MoE 支持等领域。

从零开始构建是有意义的，但它需要团队在专业知识上投入巨大，并花费大量时间来调试问题和添加缺失特性。一个强有力的替代方案是 fork 一个现有框架并根据你的需求进行增强。例如，Thinking Machines Lab 将他们的内部预训练库构建为 TorchTitan 的 fork（[来源](https://x.com/cHHillee/status/1949470943291805832)）。

最终，你的选择取决于团队的专业知识、目标特性，以及相对于直接使用最成熟产品来讲，你愿意在框架开发上投入多少时间。

如果多个框架都能满足你的需求，请在你的特定硬件上对比它们的吞吐量。对于快速实验和快速运行，更简洁的代码库通常更胜一筹。

### 消融实验配置

选定框架后，我们现在需要设计消融实验配置。我们需要既能快速迭代，又具有足够规模、能提供有效信号并迁移到最终模型的实验。让我们看看如何配置。

#### 配置消融实验框架

消融实验的目标是进行小规模实验，并获得我们能够自信地外推到最终生产运行的结果。

主要有两种方法。我们可以采用目标模型的大小，但使用较少的 tokens 进行训练。在 SmolLM3 的消融实验中，我们用 100B tokens 训练了完整的 3B 模型，而不是最终的 11T tokens。如果目标模型太大，也可以训练一个小型的代理模型（proxy model）进行消融。例如，当 Kimi 在开发拥有 1T 参数、32B 激活参数的 Kimi K2 模型时，在消融实验中使用完整规模会非常昂贵，因此他们在 3B MoE（0.5B 激活参数）上运行了一些消融实验 [@kimik2]。

关键问题是这些小规模发现是否真的能外推。根据我们的经验，如果某项改动在小规模下就损害了性能，那基本就可以很自信的放弃它了。但如果某项改动在小规模下奏效，你仍需确保已经训练了足够数量的 tokens，才能以高概率断定这些发现会外推到更大规模。训练时间越长、消融模型与最终模型越接近，效果就越好。

在本博客中，我们将使用 baseline Vanilla Transformer 进行所有消融。我们的主要设置是 1B Transformer，遵循 [Llama3.2 1B](https://huggingface.co/meta-llama/Llama-3.2-1B) 架构，并在 45B tokens 上进行训练。使用 8 张 H100 显卡的单节点运行此 nanotron [配置](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/baseline_config_1B.yaml) 大约需要 1.5 天（每张 GPU 每秒处理 4.2 万 tokens）。在 SmolLM3 训练期间，我们在 3B 模型上运行了这些消融实验，训练了 100B tokens（配置见[此处](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs)）。我们将在每章末尾分享这些结果（你会发现结论是一致的）。

<Sidenote>

我们训练 45B tokens 是为了确保获得稳定的信号，尽管对于这种模型大小，~35B tokens 就已经符合 <a href="https://arxiv.org/abs/2203.15556" target="_blank">Chinchilla 最优</a> 比例了。
</Sidenote>

我们的 1B baseline 配置以 YAML 格式记录了所有必须的训练细节。以下是关键部分：

```yaml
## Datasets and mixing weights
data_stages:
- data:

    dataset:
      dataset_folder:
      - fineweb-edu
      - stack-edu-python
      - finemath-3plus

      dataset_weights:
      - 0.7
      - 0.2
      - 0.1

## Model architecture, Llama3.2 1B configuration
model:
  model_config:
    hidden_size: 2048
    num_hidden_layers: 16
    num_attention_heads: 32
    num_key_value_heads: 8  
    intermediate_size: 8192
    max_position_embeddings: 4096
    rope_theta: 50000.0
    tie_word_embeddings: true

## Training hyperparameters, AdamW with cosine schedule
optimizer:
  clip_grad: 1.0
  learning_rate_scheduler:
    learning_rate: 0.0005
    lr_decay_starting_step: 2000
    lr_decay_steps: 18000
    lr_decay_style: cosine
    lr_warmup_steps: 2000
    lr_warmup_style: linear
    min_decay_lr: 5.0e-05
  optimizer_factory:
    adam_beta1: 0.9
    adam_beta2: 0.95
    adam_eps: 1.0e-08
    name: adamW

## Parallelism, 1 node
parallelism:
  dp: 8  # Data parallel across 8 GPUs
  tp: 1  # No tensor or pipeline parallelism needed at 1B scale
  pp: 1 

## Tokenizer
tokenizer:
  tokenizer_max_length: 4096
  tokenizer_name_or_path: HuggingFaceTB/SmolLM3-3B

## Batch size, sequence length and total training for 30B tokens
tokens:
  batch_accumulation_per_replica: 16
  micro_batch_size: 3 # GBS (global batch size)=dp * batch_acc* MBS * sequence=1.5M tokens
  sequence_length: 4096
  train_steps: 20000 # GBS * 20000 = 30B
 
 ...(truncated)
```
对于我们的消融实验，我们将根据测试内容修改不同的部分，同时保持其他所有内容不变：`model` 部分用于[架构选择](#architecture-choices)，`optimizer` 部分用于[优化器和训练超参数](#optimiser-and-training-hyperparameters)，`data_stages` 部分用于[数据整理](#the-art-of-data-curation)。

<Note title="一次只修改一个变量" emoji="☝️" variant="danger">

在每次消融实验中只改变一个变量，同时保持其他所有内容不变。
如果你改变了多项内容且性能有所提升，你将无法确定是哪项改动
起到了作用。应单独测试各项修改，然后将成功的修改组合起来并重新评估。
</Note>

在运行消融实验时，某些架构更改可能会显著改变参数数量。例如，从绑定嵌入（tied embeddings）切换到非绑定嵌入会使嵌入参数翻倍，而从 MHA 切换到 GQA 或 MQA 则会大幅减少注意力参数。为了确保公平比较，我们需要跟踪参数数量，并偶尔调整其他超参数（如隐藏层大小或层数），以使模型规模大致相同。以下是我们用于估算不同配置参数数量的一个简单函数：

```python
from transformers import LlamaConfig, LlamaForCausalLM

def count_parameters(
    tie_embeddings=True,
    num_key_value_heads=4,
    num_attention_heads=32,
    hidden_size=2048,
    num_hidden_layers=16,
    intermediate_size=8192,
    vocab_size=128256,
    sequence_length=4096,
):
    config = LlamaConfig(
        hidden_size=hidden_size,
        num_hidden_layers=num_hidden_layers,
        num_attention_heads=num_attention_heads,
        num_key_value_heads=num_key_value_heads,
        intermediate_size=intermediate_size,
        vocab_size=vocab_size,
        max_position_embeddings=sequence_length,
        tie_word_embeddings=tie_embeddings,
    )
    model = LlamaForCausalLM(config)  
    return f"{sum(p.numel() for p in model.parameters())/1e9:.2f}B"
```
我们还提供了一个交互式工具，用于可视化稠密 Transformer 的 LLM 参数分布。这在做出架构决策或配置消融实验时会派上用场。

<HtmlEmbed src="/embeds/parameter-calculator.html" />



<Sidenote>

此计算器假设了标准的架构选择：门控前馈网络（gated feedforward networks）、每个注意力头的维度（hidden_size / num_heads），以及每个 Transformer 层 2 个层归一化（layer norms）。它不包含偏置项（如果使用了的话）。
</Sidenote>

####  **洞察成效：评估** 

一旦我们启动了消融实验，我们如何知道哪些方案有效，哪些无效呢？

任何模型训练者的第一直觉可能是查看损失（loss），这确实重要。你希望看到的损失平稳下降，没有剧烈的波动或不稳定性。对于许多架构选择，损失与下游性能（微调阶段）高度相关，作为评估基准足够有效 [@chen2025]。但仅仅依靠损失并不总是可靠的。以数据消融为例，你会发现训练维基百科比训练网页的损失更低（因为下一个 token 更容易预测），但这并不意味着你会得到一个更强大的模型。同样，如果我们更换了分词器，因为文本的切分方式不同，运行之间的损失就无法直接比较。某些改动还可能专门影响特定能力（如推理和数学），而这些影响可能会被平均损失所掩盖。更重要的是，即使在预训练损失收敛之后，模型在下游任务上的表现仍可以继续提升 [@liu2022]。

我们需要更细粒度的评估来把控全局并理解这些微妙的影响。一种自然的方法是使用下游评估（downstream evaluations），测试模型在知识、理解、推理以及任何我们关心的领域的表现。

对于消融实验，最好专注于那些能提供良好早期信号的任务，并避开高噪声的基准测试。在 [FineTasks](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks) 和 [FineWeb2](https://arxiv.org/pdf/2506.20920) 中，可靠的评估任务由四个关键原则定义：

-  **单调性（Monotonicity）：** 随着训练时间的增加，基准测试的分数应持续提高。
-  **低噪声（Low noise）：** 当我们使用相同设置但不同随机种子时，分数不应剧烈波动。
-  **高于随机的表现（Above-random performance）：** 许多能力只会在训练的较后阶段才逐渐显现，因此，那些在很长一段训练过程中都只表现为随机水平的任务，并不适合用于消融实验。例如，采用多项选择格式的 MMLU 就属于这种情况，具体原因我们将在后文中解释。
-  **排名一致性（Ranking consistency）：** 如果某种方法在早期阶段优于另一种方法，那么随着训练的继续，这种优劣顺序应保持稳定。

任务的质量还取决于任务的形式（我们如何向模型提问）和指标的选择（我们如何计算得分）。

三种常见的任务形式是：多项选择格式（MCF）、完形填空形式（CF）和自由生成（FG）。多项选择格式要求模型从提示中明确给出并带有 A/B/C/D 前缀的选项中选择一个（例如 MMLU）。在完形填空形式中，我们比较不同选项的似然度（likelihood）来判断哪个更合理，而无需在提示中提供选项。在 FG 中，我们观察针对给定提示的贪婪生成（greedy generation）的准确性。FG 需要模型具备大量的潜藏知识，对于完整训练前的短期预训练消融实验中的模型来说，这种任务通常难度过大，无法提供有用信号。因此，在运行小规模消融实验时，我们侧重于多项选择形式（MCF 或 CF）。 

<Note title="提醒" emoji="📍" variant="info">

对于后训练（post-trained）模型，FG 成为主要的任务形式，因为
我们要评估的是模型能否真正生成有用的回答。
我们将在[后训练章节](#beyond-base-models--post-training-in-2025)中介绍针对这些模型的评估。
</Note>

研究还表明，模型在训练初期难以掌握 MCF，只有在经过大量训练后才能学会这项技能，因此 CF 更适合作为早期信号 [@olmes; @du2025; @datacomp]。我们对小型消融实验使用 CF，并在主训练运行中整合 MCF，因为在模型越过特定阈值并获得足够高的信噪比后，MCF 能提供更好的中期训练信号。另外说明一下，为了在 CF 等序列似然评估中为模型的答案打分，我们将准确率计算为：在多少比例的问题中，正确答案的对数概率（按字符数归一化）最高。这种归一化可以防止模型偏向较短的答案。

<Sidenote>

MMLU MCF 表现变得非随机的时间点取决于模型规模和训练数据。对于 7B Transformer，OLMES 论文 [@olmes] 发现模型在 500B tokens 后开始显示非随机性能。对于 1.7B 模型，我们在 SmolLM2 [@smollm2] 中发现这发生在 6T tokens 之后。@du2025 认为这从根本上取决于预训练损失是否达到了某个阈值。
</Sidenote>

我们的消融评估套件包括来自 [FineWeb](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) 消融实验的基准测试，但去掉了 SIQA，因为我们发现它噪声太大。我们添加了数学和代码基准测试，如 GSM8K 和 HumanEval，以及用于长文本消融的长文本基准 RULER。这些任务的集合跨越多种格式，测试了世界知识、推理和常识，如下表所示。为了在牺牲少量噪声的情况下加快评估速度，我们对每个基准仅评估 1,000 个问题（GSM8K、HumanEval 和 RULER 除外，它们在 3B SmolLM3 消融中完整使用，但在下文的 1B 实验中省略）。我们还如前所述，对所有多项选择基准使用完形填空（CF）评估。需要注意的是，对于多语言消融和实际训练，我们增加了更多基准来测试多语言能力，详情见后文。这些评估使用 [LightEval](https://github.com/huggingface/lighteval) 运行，下表总结了每个基准的关键特征：

| 基准 | 领域 | 任务类型 | 问题数量 | 测试内容 |
| --- | --- | --- | --- | --- |
| MMLU | 知识 | 多项选择 | 14k | 跨 57 个学科的广泛学术知识 |
| ARC | 科学与推理 | 多项选择 | 7k | 小学水平的科学推理 |
| HellaSwag | 常识推理 | 多项选择 | 10k | 对日常情境的常识推理（叙事补全） |
| WinoGrande | 常识推理 | 二元选择 | 1.7k | 需要世界知识的代词消解 |
| CommonSenseQA | 常识推理 | 多项选择 | 1.1k | 对日常概念的常识推理 |
| OpenBookQA | 科学 | 多项选择 | 500 | 包含推理的基础科学事实 |
| PIQA | 物理常识 | 二元选择 | 1.8k | 关于日常物品的物理常识 |
| GSM8K | 数学 | 自由生成 | 1.3k | 小学数学应用题 |
| HumanEval | 代码 | 自由生成 | 164 | 根据 docstrings 合成 Python 函数 |

让我们从每个基准中看几个示例问题，以便直观了解这些评估到底在测什么：

<iframe src="https://huggingface.co/datasets/HuggingFaceTB/llm-benchmarks-viewer/embed/viewer/default/mmlu" class="card card--p0" frameborder="0" width="100%" height="450px"></iframe>


浏览上面的示例，看看每个基准中的问题类型。注意 MMLU 和 ARC 如何通过多项选择测试事实知识，GSM8K 如何要求计算数学题的数值答案，以及 HumanEval 如何要求生成完整的 Python 代码。这种多样性确保了我们在整个消融实验中能够测试模型能力的不同维度。

 **消融实验使用哪种数据混合策略？** 

对于 *架构消融实验*，我们使用固定比例的高质量数据集混合进行训练，这些数据集能在各种任务中提供早期信号。我们使用了英语 ([FineWeb-Edu](https://huggingface.co/datasets/HuggingFaceFW/fineweb-edu))、数学 ([FineMath](https://huggingface.co/datasets/HuggingFaceTB/finemath)) 和代码 ([Stack-Edu-Python](https://huggingface.co/datasets/HuggingFaceTB/stack-edu))。架构层面的发现能很好地推演到其他数据集和领域（包括多语言数据），因此我们可以保持简单的数据混合策略。

对于 *数据消融实验*，我们采取相反的方法：固定架构，并系统地改变数据混合策略，以理解不同的数据源如何影响模型性能。

<Sidenote>

有时评估结果的差异可能很小。如果你有足够的计算资源，可能值得用不同的随机种子重新运行相同的消融实验，以观察结果的波动情况。
</Sidenote>

一套稳健的消融实验设置，其价值远不止于构建一个优秀的模型。当我们的主训练运行不可避免地出现问题时（无论准备得多么充分，总会出问题），我们希望对之前做的每一个决策都有信心，并能迅速识别出哪些组件未经过妥善测试，从而可能导致了当前的问题。这种准备能节省调试时间，也为我们未来的心理健康提供了保障。

#### 估算消融实验成本

消融实验虽然效果显著，但需要耗费 GPU 时间，了解这些实验的成本很有必要。下表显示了 SmolLM3 预训练的完整算力成本明细：主训练运行（计入了偶尔的停机时间）、训练前和训练期间的消融实验，以及由于意外的扩展问题导致的重启和部分调试所花费的算力（后文详述）。

| Phase | GPUs | Days | GPU-hours |
| --- | --- | --- | --- |
| 主预训练运行 | 384 | 30 | 276,480 |
| 消融实验（预训练阶段） | 192 | 15 | 69,120 |
| 消融实验（训练中期阶段） | 192 | 10 | 46,080 |
| 训练重置与调试 | 384/192 | 3/4 | 46,080 |
| **总成本** | - | - | **437,760** |

<Sidenote>

我们估算评估成本略低于 10,000 GPU 小时。我们的全套评估（英语、多语言、数学和代码）每张 GPU 耗时约 1.5 小时。在 11T tokens 的完整训练过程中，除了大量的消融实验外，我们每 10B tokens 进行一次评估。长文本评估尤其昂贵，每次运行需 8 张 GPU 耗时约 1 小时。
</Sidenote>

这些数字揭示了一个重要事实：消融和调试总共消耗了 161,280 GPU 小时，**超过了我们主训练运行成本的一半**（276,480 GPU 小时）。在 SmolLM3 的开发过程中，我们总共运行了 100 多次消融实验：预训练消融耗时 20 天，训练中期消融耗时 10 天，还有 7 天用于从意外的训练问题中恢复（该问题导致了重启和部分调试，后文详述）。

这突显了为什么消融成本必须纳入算力预算：计划训练成本 + 消融成本 + 应对意外情况的缓冲。如果你追求 SOTA 性能、尝试新的架构方案，或者还没有一套经过验证的方案，那么消融实验将成为一个主要的成本中心，而非次要的零星实验。

<Sidenote>

当 [DeepSeek-V3](https://huggingface.co/deepseek-ai/DeepSeek-V3) 发布时，[全球都聚焦于](https://www.forbes.com/sites/markminevich/2025/02/06/the-6-million-ai-bombshell-how-deepseek-shook-wall-street-and-ai-leadership/)其报道的 560 万美元训练成本。许多人将这个数字解读为完整的研发成本。实际上，它仅反映了最终那次训练运行。更大——且通常是隐形的——开销在于研究本身：那些导向最终方案的消融实验、失败的运行和调试。考虑到该模型的规模和创新性，其研究成本无疑要高得多。
</Sidenote>

在进入下一节之前，让我们为每个进行实验的人建立一些基本准则。

### 参与准则

<Quote>

TL;DR: 保持偏执。
</Quote>

 **验证你的评估套件。** 在训练任何模型之前，请确保你的评估套件能够复现你要对比的已发布模型的结果。如果任何基准测试具有生成性质（如 GSM8k），要格外小心，手动检查一些样本，确保提示格式正确，且后续处理提取了正确的信息。由于评估将指导你的每一个决策，这一步能否做对，对项目的成败至关重要！

 **测试每一项改动，无论多小。**  不要低估那个看似无害的库升级，或者那个“只改了两行代码”的提交的影响。这些小改动可能引入微妙的 bug 或性能波动，从而污染你的实验结果。你需要一个在关键案例上拥有强大测试套件的库，以避免出现性能倒退（regression）。

<Sidenote>

在某些情况下，升级到最新版本可以解决 bug。关于这一点，Elana Simon 的[博文](https://elanapearl.github.io/blog/2025/the-bug-that-taught-me-pytorch/?t=1)提供了一个极佳的案例，其中包含了精彩的侦探式调试。
</Sidenote>

**一次只改变一个变量。** 在实验之间保持其他条件完全一致。不同的更改有时会以意想不到的方式相互作用，所以我们首先要评估每个更改的单独贡献，再尝试将它们组合，观察整体影响。

**用足够的 tokens 进行训练，并保证充分的评测。**  正如前文所述，我们需要确保评测集有良好的覆盖范围，并且训练足够长时间，才能获得可靠的信号。如果在这里偷工减料，会导致结果噪声很大，决策也会因此变差。

遵循这些规则可能显得过于谨慎，但如果不这样做，你可能会花上数天时间调试那些奇怪的性能下降，最终却发现是由几天前一次无关的依赖更新导致的。黄金原则是：一旦你有了确定好了配置，*没有经过更改的情况下不要做任何更改！*

## 设计模型架构

实验框架已经搭建完成，现在是时候做出那些将定义我们模型的重大决策了。我们做出的每一个选择——从模型大小到注意力机制再到分词器——都会产生约束和机遇，影响模型的训练和使用。

回想一下[训练指南](#training-compass-why--what--how)：在做任何技术决策之前，我们需要明确 *为什么* 和 *做什么*。我们为什么要训练这个模型？它应该是什么样子？

这听起来很显而易见，但正如我们在训练指南中解释的那样，在这里深思熟虑会塑造我们的决策，避免我们在无穷无尽的实验空间中迷失方向。我们的目标是做一个英语领域的 SOTA 模型吗？长上下文是优先事项吗？还是我们在尝试验证一种新架构？在这些情况下，训练循环可能看起来相似，但我们运行的实验和接受的权衡取舍会有所不同。尽早回答这个问题有助于我们决定如何在数据和架构工作之间分配时间，以及在开始正式训练之前在每个方面应该做多少创新。

让我们以身作则，逐步讲解指导 SmolLM3 设计的目标。我们想要一个强大的端侧应用模型，具有出色的多语言性能、可靠的数学和编程能力，以及鲁棒的长上下文处理能力。正如我们之前提到的，这使我们选择了一个 3B 参数的稠密模型：足够大以拥有强大的能力，但又足够小可以在手机上流畅运行。考虑到边缘设备的内存限制和我们的项目时间线（大约 3 个月），我们选择了稠密 Transformer 而不是 MoE 或混合架构。

我们之前有一个在较小规模（1.7B 参数）上用于英语的 SmolLM2 工作方案，但扩展规模意味着需要重新验证一切，并应对新的挑战，如多语言支持和扩展上下文长度。这是一个明确的例子，说明既定目标如何决定了我们的方向。例如，在 SmolLM2 中，我们在预训练最后，扩展上下文长度时遇到了困难，所以对于 SmolLM3，我们从一开始就做出了架构选择——比如使用 NoPE 和文档内掩码（稍后会介绍）——以最大化成功的可能性，事实证明这是非常有效的。

<Sidenote>

SmolLM2 是我们上一代的小型语言模型，有三个变体：135M、360M 和 1.7B 参数，专为端侧部署设计。它们仅支持英语，上下文长度为 8k。
</Sidenote>

一旦我们的目标明确，就可以开始做出将其变为现实的技术决策。在本章中，我们将逐步介绍我们对这些核心决策的系统化方法：架构、数据和超参数。把这看作是我们的战略规划阶段，把这些基础工作做好将帮我们避免在实际训练马拉松中犯下代价高昂的错误。

### 架构选择

如果你研究一下最近的模型，比如 Qwen3、Gemma3 或 DeepSeek v3，你会发现尽管它们各有不同，但都共享同一个基础——2017 年提出的 Transformer 架构 [@transformer]。这些年来变化的不是基本结构，而是对其核心组件的改进。无论你是在构建稠密模型、混合专家模型还是混合架构，你都在使用这些相同的构建模块。

这些改进来自于各个团队为追求更好的性能而应对特定挑战：推理时的内存限制、大规模训练的不稳定性，或者处理更长上下文的需求。一些修改，比如从多头注意力（MHA）转向计算效率更高的注意力变体如分组查询注意力（GQA）[@gqa]，现在已经被广泛采用。其他的，比如不同的位置编码方案，仍在争论中。最终，今天的实验将凝结成明天的基准。

那么现代 LLM 今天实际上使用什么呢？让我们来看看领先模型的共同选择。遗憾的是，并非所有模型都公开其训练细节，但我们从 DeepSeek、OLMo、Kimi 和 SmolLM 等家族获得了足够的信息来了解当前的格局：

<Wide>

<Reference id="llms-landscape-pretrain" caption="">

| Model | Architecture | Parameters | Training Tokens | Attention | Context Length (final) | Position Encoding | Precision | Init (std) | Optimizer | Max LR | LR Schedule | Warmup Steps | Batch Size |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| DeepSeek LLM 7B | Dense | 7B | 2T | GQA | 4K | RoPE | BF16 | 0.006 | AdamW | 4.2×10⁻⁴ | Multi-Step | 2K | 9.4M |
| DeepSeek LLM 67B | Dense | 67B | 2T | GQA | 4K | RoPE | BF16 | 0.006 | AdamW | 3.2×10⁻⁴ | Multi-Step | 2K | 18.9M |
| DeepSeek V2 | MoE | 236B (21B active) | 8.1T | MLA | 128K | Partial RoPE | - | 0.006 | AdamW | 2.4×10⁻⁴ | Multi-Step | 2K | 9.4M→37.7M (warmup 225B) |
| DeepSeek V3 | MoE | 671B (37B active) | 14.8T | MLA | 129K | Partial RoPE | FP8 | 0.006 | AdamW | 2.2×10⁻⁴ | Multi-Step + Cosine | 2K | 12.6M→62.9M (warmup 469B) |
| MiniMax-01 | MoE + Hybrid | 456B (45.9 active) | 11.4T | Linear attention + GQA | 4M | Partial RoPE | - | Xavier init with deepnorm scaling | AdamW | 2×10⁻⁴ | Multi-Step | 500 | 16M→32M→64M→128M |
| Kimi K2 | MoE | 1T (32B active) | 15.5T | MLA | 128K | Partial RoPE | BF16 | likely 0.006 | MuonClip | 2×10⁻⁴ | WSD | 500 | 67M |
| OLMo 2 7B | Dense | 7B | 5T | MHA | 4K | RoPE | BF16 | 0.02 | AdamW | 3×10⁻⁴ | Cosine | 2K | 4.2M |
| SmolLM3 | Dense | 3B | 11T | GQA | 128K | NoPE | BF16 | 0.02 | AdamW | 2×10⁻⁴ | WSD | 2K | 2.3M |
</Reference>

</Wide>

如果你还不理解其中一些术语，比如 MLA、NoPE 或 WSD，不用担心。我们将在本节中逐一解释。现在，只需注意其多样性：不同的注意力机制（MHA、GQA、MLA），位置编码（RoPE、NoPE、partial RoPE），以及学习率调度（Cosine、Multi-Step、WSD）。

面对这一长串架构选择，要弄清楚从哪里开始确实让人有些不知所措。和大多数类似情况一样，我们将一步一步来，逐渐积累所有必要的知识。我们将首先关注最简单的基础架构（稠密模型），详细研究每个架构方面。之后，我们将深入探讨 MoE 和混合模型，并讨论何时使用它们是一个好的选择。最后我们将探索分词器，这是一个经常被忽视和低估的组件。我们应该使用现有的分词器还是训练自己的？我们如何评估我们的分词器是否足够好？

<Note title="消融实验设置" emoji="📍" variant="info">

在本章的其余部分，我们通过使用上一章描述的设置进行消融实验来验证大多数架构选择：我们的 1B 基线模型（遵循 Llama3.2 1B 架构），在来自 FineWeb-Edu、FineMath 和 Python-Edu 混合的 45B tokens 上进行训练。对于每个实验，我们展示训练损失曲线和下游评估分数，以评估每个修改的影响。你可以在 [HuggingFaceTB/training-guide-nanotron-configs](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/tree/main) 找到所有运行的配置。
</Note>

<Sidenote>

Sebastian Raschka 的这篇[博客文章](https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html)很好地概述了 2025 年现代 LLM 架构。
</Sidenote>

现在让我们从每个 LLM 的核心开始：注意力机制。    

####  **注意力机制** 

Transformer 架构研究中最活跃的领域之一就是注意力机制。虽然前馈层在预训练期间占主导计算量，但注意力在推理时成为主要瓶颈（尤其是在长上下文场景下），它会拉高计算成本，而 KV 缓存会快速消耗 GPU 内存，降低吞吐量。让我们快速浏览一下主要的注意力机制，以及它们如何在容量和速度之间进行权衡。

 **我的注意力需要多少头？** 

 *多头注意力（Multi-head attention，MHA）* 是与原始 Transformer 一起引入的标准注意力 [@transformer]。主要思想是你有 N 个注意力头，每个头独立执行相同的检索任务：将隐藏状态转换为查询（queries）、键（keys）和值（values），然后使用当前查询通过匹配键来检索最相关的 token，最后转发与匹配 token 关联的值。在推理时，我们不需要为过去的 token 重新计算 KV 值，而是可以重用它们。存储过去 KV 值的内存称为 *KV-Cache*。随着上下文窗口的增长，这个缓存很快就会成为推理瓶颈并消耗大量 GPU 内存。以下是一个简单的计算，用于估算使用 MHA 和序列长度为 8192 的 Llama 3 架构的 KV 缓存内存 $s_{KV}$：

<Sidenote>

查看 [Jay Alamar 的著名博客](https://jalammar.github.io/illustrated-transformer/) 快速回顾一下！
</Sidenote>

$$
\begin{equation} 
\begin{aligned} 
s_{KV} &= 2 \times n_{bytes} \times seq \times n_{layers} \times n_{heads} \times dim_{heads} \\
&= 2 \times 2 \times8192 \times 32 \times 32 \times 128  =4 \text{ GB} \textit{ (Llama 3 8B)} \\
&= 2 \times 2 \times8192 \times 80 \times 64 \times 128  =20 \text{ GB} \textit{ (Llama 3 70B)}
\end{aligned} 
\end{equation}
$$

第一个 2 是因为需要同时存储键缓存和值缓存。正如你所见，缓存随序列长度线性增加，但上下文窗口呈指数级增长，现在已达到数百万 tokens。因此，提高缓存的效率将使推理时的上下文扩展变得更加容易。

一个自然的问题是：我们真的需要为每个头都生成新的 KV 值吗？可能不需要，多查询注意力（Multi-Query Attention，MQA）[@mqa] 和分组查询注意力（Grouped Query Attention，GQA）[@gqa] 都解决了这个问题。最简单的情况是在所有头之间共享 KV 值，从而将 KV 缓存的大小除以 $n_{heads}$，例如对于 Llama 3 70B 来说这是 64 倍的减少！这就是 MQA 的思想，被用在一些模型中如 StarCoder，作为 MHA 的替代方案。然而，我们可能会放弃比预期更多的注意力容量，所以可以考虑折中方案，在头的组之间共享 KV 值，例如 4 个头共享相同的 KV 值。这就是 GQA 的方法，在 MQA 和 MHA 之间取得了平衡。

最近，DeepSeek-v2（也用于 v3）引入了 *多潜变量注意力（Multi-Latent Attention，MLA）* [@deepseekv2]，它使用不同的策略来压缩缓存：不是减少 KV 值的数量，而是减小它们的大小，只存储一个潜变量，在运行时解压缩成 KV 值。通过这种方法，他们成功地将缓存减少到相当于 2.25 组的 GQA，同时性能比 MHA 更强！为了使这与 RoPE 兼容，需要一个额外的小潜变量向量的小调整。在 DeepSeek-v2 中，他们选择 $4*dim_{head}$ 作为主潜变量，$1/2*dim_{head}$ 用于 RoPE 部分，所以总共是 $4.5*dim_{head}$，同时用于 K 和 V，因此去掉了前面的因子 2。

<Sidenote>

RoPE（旋转位置嵌入）是一种通过根据序列中的位置旋转查询和键向量来编码位置信息的方法。它在当今的 LLM 中被广泛使用。
</Sidenote>

你可以在下图中看到每种注意力机制的可视化解释：

<Wide>

<Reference caption="Simplified illustration of Multi-Head Attention (MHA), Grouped-Query Attention (GQA), Multi-Query Attention (MQA), and Multi-head Latent Attention (MLA). Through jointly compressing the keys and values into a latent vector, MLA significantly reduces the KV cache during inference.">

<HtmlEmbed src="embeds/attention-mechanisms.html"/>
</Reference>

</Wide>



下表比较了我们刚才在本节讨论的注意力机制。为简单起见，我们比较每个 token 使用的参数大小，如果你想计算总内存，只需乘以每个参数的字节数（通常为 2）和序列长度：

| Attention Mechanism | KV-Cache parameters per token |
| --- | --- |
| MHA | $= 2 \times n_{heads} \times n_{layers} \times dim_{head}$ |
| MQA | $= 2 \times 1 \times n_{layers} \times dim_{head}$ |
| GQA | $= 2 \times g \times n_{layers} \times dim_{head} \text { (typically g=2,4,8 )}$ |
| MLA | $= 4.5 \times n_{layers} \times dim_{head}$ |

现在让我们看看这些注意力机制在实际实验中表现如何！

 **消融实验 - GQA 优于 MHA** 

在这里我们比较不同的注意力机制。我们的[baseline](https://huggingface.co/datasets/HuggingFaceTB/ablations-training-configs/blob/main/baseline_config_1B.yaml)模型使用 32 个头和 8 个 KV 头，对应于比率为 32/8=4 的 GQA。如果我们使用 MHA，或者使用更少的 KV 头和更高的 GQA 比率，性能会如何变化？

<Sidenote>

一些库将 GQA 比率称为：Query groups = Query heads / 
KV heads
</Sidenote>

改变 KV 头的数量会影响参数数量，特别是在 MHA 的情况下。为了保持一致性，我们调整了 MHA 运行的层数，否则会有超过 100M 的参数差异；对于其余的，我们保持默认的 16 层。

| Attention Type | Query Heads | KV Heads | Layers | Parameter Count | Notes |
| --- | --- | --- | --- | --- | --- |
| MQA | 32 | 1 | 16 | 1.21B |
| GQA (ratio 16) | 32 | 2 | 16 | 1.21B |
| GQA (ratio 8) | 32 | 4 | 16 | 1.22B | **Our baseline** |
| GQA (ratio 4) | 32 | 8 | 16 | 1.24B |
| GQA (ratio 2) | 32 | 16 | 15 | 1.22B | Reduced layers |
| MHA | 32 | 32 | 14 | 1.20B | Reduced layers |
| GQA (ratio 2) | 32 | 16 | 16 | 1.27B | Too large - not ablated |
| MHA | 32 | 32 | 16 | 1.34B | Too large - not ablated |

所以我们比较了 MHA、MQA 和 4 种 GQA 设置（比率 2、4、8、16）。你可以在[这里](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/tree/main/attention)找到 nanotron 配置。

观察消融结果，我们发现 MQA 和 16 组的 GQA（分别只留下 1 和 2 个 KV 头）明显不如 MHA。另一方面，2、4 和 8 组的 GQA 配置与 MHA 性能大致相当。

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/attention_loss.csv",
xDomain: [0, 45e9],
yDomain: [2.1, 2.7],
smoothing: true,
title: "Attention Loss"
}}
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/attention_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: true,
smoothingWindow: 15
}}
/>
</Wide>



结果在损失曲线和下游评估中都是一致的。我们在 HellaSwag、MMLU 和 ARC 等基准测试中清楚地观察到这一点，而 OpenBookQA 和 WinoGrande 等基准测试则显示出一些噪声。

基于这些消融实验，GQA 是 MHA 的可靠替代方案。它在保持性能的同时，在推理时更加高效。一些最近的模型已经采用了 MLA 以实现更大的 KV 缓存压缩，尽管它尚未被广泛采用。我们没有对 MLA 进行消融实验，因为在进行消融实验时它还没有在 nanotron 中实现。对于 SmolLM3，我们使用了 4 组的 GQA。

除了注意力架构本身，我们在训练期间使用的注意力模式也很重要。让我们来看看注意力掩码。

 **文档掩码** 

我们如何在训练序列中应用注意力会影响计算效率和模型性能。这就引出了 *文档掩码* 以及我们如何在数据加载器中组织训练样本这个更广泛的问题。

在预训练期间，我们使用固定的序列长度进行训练，但我们的文档长度是不一样的。一篇研究论文可能有 10k tokens，而一个短代码片段可能只有几百 tokens。我们如何将不同长度的文档放入固定长度的训练序列中？直接对较短的文档进行 padding（填充）以达到目标长度，会浪费计算资源，因为这些填充 token 没有实际意义。为此，我们使用 **打包（packing）** 方法：先将文档打乱并用 EOS（序列结束）token 连接起来，然后将得到的长序列按固定长度切分，得到与训练序列长度匹配的块。

<Sidenote>

我们也可以在文档开头添加 BOS token。在这种情况下，你会注意到模型/分词器配置中存在不同的 `bos_token_id`。
</Sidenote>

实践中看起来像这样：

```markdown
File 1: "Recipe for granola bars..." (400 tokens) <EOS>
File 2: "def hello_world()..." (300 tokens) <EOS>  
File 3: "Climate change impacts..." (1000 tokens) <EOS>
File 4: "import numpy as np..." (3000 tokens) <EOS>
...

After concatenation and chunking into 4k sequences:
Sequence 1: [File 1] + [File 2] + [File 3] + [partial File 4]
Sequence 2: [rest of File 4] + [File 5] + [File 6] + ...
```
如果一个训练序列足够长可以填满我们的 4k 上下文，它可能包含一个完整的文件，但在大多数情况下文件很短，所以序列包含多个随机文件的连接。

在标准的因果掩码下，token 可以关注打包序列中所有先前的 token。以上例子里，文件 4 的那段 Python 函数中的一个 token，可以关注到燕麦棒配方、气候变化文章，以及其他碰巧被拼在一起的内容。我们快速看一下典型的 4k 预训练上下文会包含什么。对上下文长度做个快速[分析](https://www.harmdevries.com/post/context-length/)可知，CommonCrawl 和 GitHub 中相当大比例（约 80-90%）的文件都短于 2k token。

下图展示了本文所用较新数据集的 token 分布：

<HtmlEmbed src="embeds/token-distribution.html"/>



FineWeb-Edu、DCLM、FineMath 和 Python-Edu 中超过 80% 的文档少于 2k token。这意味着在 2k 或 4k 的训练序列、标准因果掩码下，绝大多数 token 的计算会花在关注被拼在一起但彼此无关的文档上。

<Note title="PDF 中的更长文档" variant="info">

虽然多数基于网页的数据集由短文档构成，但基于 PDF 的数据集包含更长的内容。[FinePDFs](https://huggingface.co/datasets/HuggingFaceFW/fineweb-2) 文档平均长度是网页文本的 2 倍，和 FineWeb-Edu、DCLM 混合后还能提升性能。
</Note>

除了计算低效之外，@zhao2024 发现这种做法会引入无关内容噪声，从而降低性能。他们建议使用 *intra-document masking*，即修改注意力掩码，让 token 只能关注同一文档内的前序 token。下面的可视化展示了这一差异：

<Wide>

<HtmlEmbed src="/embeds/doc-masking.html" />
</Wide>



SkyLadder 的 @skyladder 也发现文档内掩码带来类似收益，但给出了不同解释。他们发现较短的上下文长度更利于训练，而文档内掩码有效降低了平均上下文长度。 

<Wide>

<Reference caption="这些来自 SkyLadder 的图展示了多项结论：(a) 预训练中更短的上下文往往表现更好（验证困惑度更低），(b) 文档内掩码（IntraDoc）的困惑度低于随机拼接（Random）和语义分组（BM25），(c) 即使没有位置编码，短上下文优势仍然成立，(d) IntraDoc 形成偏向更短有效上下文长度的分布。">

<Image src={image_27c1384e_bcac_807c_807b_fac08be1d884} alt="Image" />
</Reference>

</Wide>

Llama3 [@llama3] 也采用了文档内掩码训练。他们发现对短上下文预训练影响有限，但在长上下文扩展上收益显著，因为此时注意力开销更突出。此外，ProLong 论文 [@prolong] 表明，在持续预训练中用文档掩码扩展 Llama3 8B 的上下文，对长上下文和短上下文基准都有提升。 

我们决定在 1B 基线模型上做消融，测试文档掩码是否影响短上下文性能。配置在[这里](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/doc_masking/doc_masking.yaml)。结果显示，与标准因果掩码相比，损失曲线和下游评估分数几乎一致，如下图所示。 

要在 nanotron 中启用文档掩码，只需在模型配置里把这个标志设为 `true`：

```diff
model_config:
  _attn_implementation: flash_attention_2
  _fused_rms_norm: true
  _fused_rotary_emb: true
- _use_doc_masking: false
+ _use_doc_masking: true

```

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/doc-masking_loss.csv",
xDomain: [0, 45e9],
yDomain: [2.1, 2.7],
smoothing: true,
title: "Doc Masking Loss"
}}
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/doc-masking_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: true,
smoothingWindow: 15
}}
/>
</Wide>



与 Llama3 类似，我们在短上下文任务上没有观察到明显影响，只有 PIQA 有小幅提升。但在扩展到长序列以加速训练时，文档掩码就变得至关重要。这对我们的长上下文扩展尤其关键，我们会把上下文从 4k 扩展到 64k token（详见[训练马拉松](#the-training-marathon)一章）。因此，我们在 SmolLM3 的完整训练过程中都采用了它。

本节我们讲了注意力如何处理序列。接下来看看 Transformer 里的另一个重要参数块：嵌入层。

####  **嵌入层共享** 

如果你查看我们的基线消融模型[配置](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/baseline_config_1B.yaml)，会发现与标准 Transformer 不同的一点是开启了 embedding sharing，由 `tie_word_embeddings` 这个标志控制。 

LLM 有两类嵌入组件：输入嵌入，作为 token 到向量的转换表（大小为 vocab_size × hidden_dim），以及输出嵌入，即最后一层线性层，把隐状态映射到词表 logits（hidden_dim × vocab_size）。在经典设置里，这两者是独立矩阵，嵌入参数总数为 2 × vocab_size × hidden_dim。因此在小模型里，嵌入会占据很大一部分参数量，词表越大越明显。这让 embedding sharing（在输出层复用输入嵌入）成为小模型的自然优化选择。 

<HtmlEmbed src="/embeds/embedding-calculator.html" />



较大的模型通常不会用这项技术，因为嵌入层只占其参数预算的一小部分。例如，不共享时，嵌入层在 Llama3.2 8B 中只占 13%，在 Llama3.1 70B 中只占 3%，如下方饼图所示。

<HtmlEmbed src="/embeds/parameter-comparison.html" />



 **消融实验 - 共享嵌入的模型可匹配更大的不共享版本** 

接下来我们评估 embedding sharing 对消融模型的影响。我们参考了 [MobileLLM](https://arxiv.org/abs/2402.14905) 在 125M 规模上对该技术的全面消融实验，他们表明共享嵌入能减少 11.8% 参数量，同时几乎不损伤准确率。

由于取消共享会把参数量从 1.2B 增加到 1.46B，我们将训练另一种不共享但层数更少的模型，使其参数量与基线 1.2B 匹配。我们将比较两个 1.2B 模型：我们的基线（共享嵌入、16 层）对比不共享但层数更少的版本（12 层）以保持相同参数预算；同时再加入一个 1.46B 的参考模型，不共享嵌入且层数与基线相同（16 层）。nanotron 配置在[这里](https://huggingface.co/datasets/HuggingFaceTB/training-guide-nanotron-configs/blob/main/baseline_config_1B.yaml)。

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/tied-embeddings_loss.csv",
xDomain: [0, 45e9],
yDomain: [2.1, 2.7],
smoothing: true,
title: "Tied Embeddings Loss"
}}
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/tied-embeddings_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: true,
smoothingWindow: 5
}}
/>
</Wide>



损失和评估结果表明，尽管参数量少了 18%，我们的 1.2B 共享嵌入基线在除 WinoGrande 外的所有基准上，都能与 1.46B 的不共享版本表现相当。相反，1.2B 的不共享且层数减少的模型（12 vs 16）在两种配置下都更差，损失更高、下游评估分数更低。这说明在相同参数预算下，增加模型深度带来的收益大于取消嵌入共享。

基于这些结果，我们在 SmolLM3 3B 模型中保留了共享嵌入。

我们已经讨论了嵌入共享策略及其权衡。但仅有嵌入无法表达序列中 token 的顺序，这部分信息由位置编码提供。在下一节，我们将看看位置编码策略如何演进：从标准的 RoPE 到 NoPE（No Position Embedding）等新方法，它们能更好地建模长上下文。

####  **位置编码 & 长上下文** 

当 Transformer 处理文本时，会面临一个根本性挑战：由于它们通过并行注意力机制同时处理整个序列，自身并没有“词语先后顺序”概念。这带来高效训练，但也造成问题。没有明确的位置信息时，“Adam beats Muon”和“Muon beats Adam”在模型眼里几乎一样。

解决方案是位置嵌入：用数学编码为每个 token 赋予序列中的唯一“地址”。但随着上下文越来越长——从早期 BERT 的 512 token 到今天的百万级 token 模型——位置编码的选择对性能和计算效率都愈发关键。

 **位置编码的演进** 

早期 Transformer 使用简单的 **绝对位置嵌入（APE）** [@transformer]，本质是一个学习到的查表，将每个位置（1、2、3…）映射到向量并加到 token 嵌入上。短序列下效果不错，但有一个重大限制：模型的最大输入长度被限制在训练时的最大长度，无法开箱即用地泛化到更长序列。

随后研究转向 **相对位置编码**，它关注的是 token 之间的距离而非绝对位置。这更符合直觉：两个词相差 3 个位置，比它们处在 (5,8) 还是 (105,108) 更重要。 

<Sidenote>

如果想更深入了解位置编码，[这篇博客](https://huggingface.co/blog/designing-positional-encoding) 从最基础的位置编码到旋转编码做了循序渐进的讲解。
</Sidenote>

**ALiBi**（Attention with Linear Biases）[@alibi] 通过 token 距离来调整注意力分数。两个 token 越远，注意力权重上就施加越大的线性惩罚。想看 ALiBi 的实现细节可参考[这份资料](https://nn.labml.ai/transformers/alibi/index.html)。

但近几年大语言模型里占主导地位的，是旋转位置编码 RoPE（Rotary Position Embedding）[@rope]。

 **RoPE：用旋转表示位置** 

RoPE 的核心洞见是：把位置信息编码为高维空间里的旋转角度。它不是把位置向量加到 token 嵌入上，而是按 token 的绝对位置，对 query 和 key 向量进行旋转。

直觉上，我们把嵌入向量的每一对维度视作二维平面上的圆坐标，然后按以下因素决定旋转角度：

- token 在序列中的位置
- 当前处理的是哪一对维度（不同维度对的旋转频率不同，是某个基准频率的指数）

```python
import torch

def apply_rope_simplified(x, pos, dim=64, base=10000):
    """
    Rotary Position Embedding (RoPE)

    Idea:
    - Each token has a position index p (0, 1, 2, ...).
    - Each pair of vector dimensions has an index k (0 .. dim/2 - 1).
    - RoPE rotates every pair [x[2k], x[2k+1]] by an angle θ_{p,k}.

    
    Formula:
      θ_{p,k} = p * base^(-k / (dim/2))

    - Small k (early dimension pairs) → slow oscillations → capture long-range info.
    - Large k (later dimension pairs) → fast oscillations → capture fine detail.

    """
    rotated = []
    for i in range(0, dim, 2):
        k = i // 2  # index of this dimension pair

        # Frequency term: higher k → faster oscillation
        inv_freq = 1.0 / (base ** (k / (dim // 2)))
        theta = pos * inv_freq  # rotation angle for position p and pair k

        cos_t = torch.cos(torch.tensor(theta, dtype=x.dtype, device=x.device))
        sin_t = torch.sin(torch.tensor(theta, dtype=x.dtype, device=x.device))

        x1, x2 = x[i], x[i+1]

        # Apply 2D rotation
        rotated.extend([x1 * cos_t - x2 * sin_t,
                        x1 * sin_t + x2 * cos_t])

    return torch.stack(rotated)
    
    
## Q, K: [batch, heads, seq, d_head]
Q = torch.randn(1, 2, 4, 8)
K = torch.randn(1, 2, 4, 8)

## 👉 apply RoPE to Q and K *before* the dot product
Q_rope = torch.stack([apply_rope(Q[0,0,p], p) for p in range(Q.size(2))])
K_rope = torch.stack([apply_rope(K[0,0,p], p) for p in range(K.size(2))])

scores = (Q_rope @ K_rope.T) / math.sqrt(Q.size(-1))
attn_weights = torch.softmax(scores, dim=-1)
```
这段代码看起来有些复杂，我们用一个具体例子拆解一下。句子 *"The quick brown fox"* 中的 *"fox"* 为例。在我们的 1B 基线模型里，每个注意力头使用 64 维的 query/key 向量。RoPE 会把它分成 32 对：(x₁, x₂)、(x₃, x₄)、(x₅, x₆) 等。之所以成对处理，是因为旋转发生在二维平面上。为简化说明，我们只看第一对 (x₁, x₂)。单词 "fox" 在句子中位于第 3 个位置，因此 RoPE 会把这一对维度旋转：

```python
rotation_angle = position × θ₀ 
                = 3 × (1/10000^(0/32))
                = 3 × 1.0 
                = 3.0 radians 
                = 172° degrees
```
我们的基准频率是 10000，但对第一对维度（k=0）来说指数为 0，所以基准频率不会影响计算（因为任何数的 0 次方都是 1）。下图展示了这一过程：

<HtmlEmbed src="/embeds/rope-demo.html" />



当两个 token 通过注意力交互时，“魔法”才开始发生。它们旋转后的表示之间的点积，会通过旋转角度的相位差直接编码相对距离（其中 `m` 和 `n` 是 token 位置）

```python
dot_product(RoPE(x, m), RoPE(y, n)) = Σₖ [xₖ * yₖ * cos((m-n) * θₖ)]
```
注意力模式只取决于 (m-n)，因此相距 5 个位置的 token，无论它们在序列中的绝对位置如何，都会有相同的角度关系。于是模型学到的是基于距离的模式，可在任意绝对位置生效，并能外推到更长序列。 

 **如何设置 RoPE 频率？** 

实践中，大多数 LLM 预训练从较短的上下文长度（2K-4K token）起步，并使用 10K 或 50K 这类数万级的 RoPE 基准频率。一开始就用很长序列训练会非常昂贵，因为注意力的计算复杂度随序列长度呈二次增长，而且长上下文数据（>4K token）也不充足——在[注意力](#attention)一节的文档掩码部分我们已经看到这一点。研究还表明，这可能伤害短上下文性能 [@skyladder]。模型通常先学习词语间的短程相关性，所以一开始用长序列并不会带来太多收益。常见做法是先用短序列完成大部分预训练，然后再持续预训练，或在最后几千亿 token 上使用更长序列。但随着序列长度增长，与 token 位置成正比的旋转角会不断变大，可能导致远距离 token 的注意力分数衰减过快 [@xiong2023effectivelongcontextscalingfoundation; @rozière2024codellamaopenfoundation]：

```python
θ = position x 1 / (base^(k/(dim/2)))
```
解决方案是随着序列长度增加，提高基准频率以防止衰减，可通过 ABF 和 YaRN 等方法实现。

 **RoPE ABF（Adjusted Base Frequency）** [@ropeabf]：通过提高 RoPE 公式中的基准频率来解决长上下文下的注意力衰减问题。该调整会减缓不同位置间的旋转角变化，从而避免远距离 token 的注意力分数过快衰减。ABF 可一次性提升频率，也可分阶段随着上下文增长逐步提升。它实现简单，并以更高粒度分布嵌入向量，使模型更容易区分远距离位置。但由于对所有维度做统一缩放，极长上下文下可能并非最优。

 **YaRN（Yet another RoPE extensioN）** [@yarn]：通过坡度/缩放函数对不同 RoPE 维度的不均匀插值来调整频率。不同于 ABF 的统一调整，YaRN 为不同频率分量应用不同缩放因子，优化扩展上下文窗口。它还加入动态注意力缩放与 attention logits 的温度调整，有助于在极长上下文中保持性能。YaRN 支持高效的“短训长测”策略，用更少 token 与更少微调实现稳健外推。虽然复杂度更高，但在极长上下文上通常表现更好，缩放更平滑且能缓解灾难性的注意力损失。它也可以仅在推理阶段使用，无需微调。

这些频率调整方法能减缓注意力分数的衰减，并维持远距离 token 的贡献。例如，Qwen3 训练时在序列长度从 4k 扩展到 32k 的过程中，用 ABF 把频率从 10k 提升到 1M（随后再用 YaRN 扩展到 131k，实现 4 倍外推）。需要注意的是，目前并没有统一的最优取值，共识是：在上下文扩展阶段最好尝试不同 RoPE 值，找到最适合你设置和评测基准的方案。

如今多数主流模型都使用 RoPE：Llama、Qwen、Gemma 等。它在不同规模与架构（Dense、MoE、Hybrid）上都验证了稳健性。下面看看近来出现的一些 RoPE 变体。

 **混合位置编码方案** 

然而随着模型迈向更长上下文 [@qwen1Million; @llama4]，即便是 RoPE 也开始遇到性能挑战。长上下文扩展时单纯提高 RoPE 频率，在比“海底捞针”（NIAH）[@niah] 更难的长上下文基准（如 Ruler 和 HELMET [@ruler; @helmet]）上存在局限。于是出现了新的技术来应对。

本节开头我们说过，Transformer 需要位置信息来理解 token 顺序，但近期研究挑战了这一假设：如果根本不需要显式位置编码呢？

 **NoPE（No Position Embedding）** [@nope]：在不使用任何显式位置编码的情况下训练 Transformer，让模型通过因果掩码和注意力模式隐式学习位置信息。作者显示，该方法在长度泛化上优于 ALiBi 和 RoPE。由于不依赖显式位置编码去外推训练长度之外，NoPE 自然能处理更长上下文。但在实践中，NoPE 在短上下文的推理与知识任务上往往弱于 RoPE（[Yang et al](https://arxiv.org/pdf/2501.18795).）。这表明，虽然显式位置编码可能会限制模型的外推能力，但对于处于训练上下文长度范围内的任务，它们提供了有益的归纳偏置（Inductive Biases）。

 **RNoPE 混合方案：** 在这些权衡下，@rnope 提出将不同位置编码策略结合起来。他们提出的 RNoPE 在模型层间交替使用 RoPE 和 NoPE。RoPE 层提供显式位置信息并带来局部最近性偏置，而 NoPE 层提升远距离信息检索能力。该技术最近被 Llama4、Command A 和 **SmolLM3** 采用。

<Note title="Naming convention" emoji="📍" variant="info">

为简化表述，本文后续将 RNoPE 统一称为 “NoPE”。（讨论中也常有人用 “NoPE” 指代 RNoPE。）
</Note>

 **消融实验 - NoPE 在短上下文上可与 RoPE 持平** 

我们来测试这种混合 NoPE 方案。将 RoPE 作为 1B 消融实验基线，与每隔 4 层去掉位置编码的 NoPE 变体做对比，再加上第三种配置：NoPE + 文档掩码，用来检验两种技术的交互效果。核心问题是：能否在保持强短上下文性能的同时获得长上下文能力？

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/nope_loss.csv",
xDomain: [0, 45e9],
yDomain: [2.1, 2.7],
smoothing: true,
title: "NoPE Loss"
}}
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/nope_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: true,
smoothingWindow: 5
}}
/>
</Wide>



损失与评估结果显示三种配置表现相近，说明 NoPE 能维持强短上下文能力，同时为更好的长上下文处理打下基础。基于这些结果，我们在 SmolLM3 中采用了 NoPE + 文档掩码的组合。

 **部分/分段 RoPE：** 另一个互补思路是只在模型维度的子集上应用 RoPE。不同于 RNoPE 在层级上交替 RoPE 与 NoPE，Partial RoPE 在同一层内部进行混合。近期模型如 GLM‑4.5 [@glm45] 或 Minimax-01 [@minimax01] 采用了这一策略，但更早的模型如 gpt-j [@gptj] 也使用过。在所有使用 MLA（多头潜在注意力）的模型中你都能看到这一点，因为它是实现合理推理成本的必备项。

<Note title="技术说明: 为什么 Partial RoPE 对于 MLA 是必须的" emoji="🔧" variant="info">

MLA 通过“投影吸收”实现高效推理：它不再存储每个 head 的 key  $k_i^{(h)}$ ，而是缓存一个小的共享潜变量  $c_i = x_i W_c \in \mathbb{R}^{d_c}$ ，并合并 head 的 query/key 映射，使每个分数计算更便宜。设  $q_t^{(h)} = x_t W_q^{(h)}$ ， $k_i^{(h)} = c_i E^{(h)}$ ，定义  $U^{(h)} = W_q^{(h)} E^{(h)}$ ，则：

$$
s_{t,i}^{(h)} \;=\; \tfrac{1}{\sqrt{d_k}}\,\big(q_t^{(h)}\big)^\top k_i^{(h)}\;=\; \tfrac{1}{\sqrt{d_k}}\,\big(x_t U^{(h)}\big)^\top c_i
$$

因此你用  $\tilde q_t^{(h)} = x_t U^{(h)} \in \mathbb{R}^{d_c}$  与小型缓存  $c_i$  做计算（不再存储每个 head 的 k）。RoPE 会破坏这一点，因为它在两次映射之间插入了与维度对相关的旋转：在全维 RoPE 下，

$$
s_{t,i}^{(h)} \;=\; \tfrac{1}{\sqrt{d_k}}\,\big(x_t W_q^{(h)}\big)^\top
\underbrace{R_{t-i}}_{\text{depends on } t-i}\,\big(c_i E^{(h)}\big)

$$

因此无法把  $W_q^{(h)}$  与  $E^{(h)}$  预先合并成固定的  $U^{(h)}$ 。解决办法：Partial RoPE。将 head 维度拆分为  $d_k = d_{\text{nope}} + d_{\text{rope}}$ ，在大块维度上不做旋转（像之前一样吸收： $(x_t U_{\text{nope}}^{(h)})^\top c_i$ ），只在小块维度上应用 RoPE。
</Note>

 

 **限制长上下文的注意力范围** 

到目前为止，我们已经讨论了长上下文的位置信息处理方式：启用 RoPE、禁用 RoPE（NoPE）、在部分层上启用（RNoPE）或在部分隐藏维度上启用（Partial RoPE），以及调节频率（ABF、YaRN）。这些方法通过改变位置编码来处理超过训练长度的序列。但还有一个互补思路：不改位置编码，而是限制哪些 token 彼此关注。

为了理解其重要性，请考虑一个使用 8 个标记（token）序列进行预训练的模型。在推理时，我们希望处理 16 个标记（超过了训练长度）。对于模型的位置编码而言，位置 8-15 属于‘分布外’（out of distribution）数据。虽然 RoPE ABF（调整基频）等技术通过调整位置频率来解决这一问题，但‘注意力范围方法’（attention scope methods）采取了不同的策略：它们有针对性地限制标记之间相互关注的范围，在处理完整序列的同时，将注意力模式保持在模型熟悉的范围内。这不仅降低了计算成本，还减少了内存需求。下图对比了在预训练窗口为 8 的情况下，处理 16 个标记序列的五种策略：

<HtmlEmbed src="embeds/position-masking.html"/>



 **分块注意力（Chunked Attention）** 将序列划分为固定大小的块，token 只能在各自块内相互关注。例子中，16 个 token 被分成两个 8 token 的块（0-7 与 8-15），每个 token 只能看到同块内的其他 token。注意 8-15 的 token 完全无法回看先前的块。这会形成在块边界重置的独立注意力窗口。Llama 4 [@llama4] 在 RoPE 层（四分之三的解码器层）中使用了 8192 个标记规模的分块注意力，而 NoPE 层则保持了全上下文访问。这种做法通过限制每一层的 KV Cache 大小来降低内存需求，但这也意味着标记无法关注到之前的分块，可能会对某些长上下文任务产生影响。”

 **滑动窗口注意力（SWA）**，由 Mistral 7B [@jiang2023mistral7b; @child2019generating] 推广，基于“最近 token 更相关”的直觉。不同于硬切块边界，每个 token 只关注最近的 N 个 token。图中每个 token 最多回看 8 个位置，形成沿序列连续移动的滑动窗口。注意 token 15 可关注位置 8-15，而 token 10 可关注位置 3-10。窗口不断向前移动，保留全序列的局部上下文，不会像分块那样形成硬边界。Gemma 3 在交替层中结合 SWA 与全注意力，类似于混合位置编码方案中的混合策略。

 **双块注意力（DCA）** [@dca] 是一种无需训练的方法，扩展了 **分块注意力** 并保持跨块信息流。在示例中使用块大小 s=4，把 16 个 token 分成 4 块（可视为对角线上的 4×4 方块）。DCA 结合三种机制：(1) 块内注意力：token 在本块内正常注意（对角线模式）；(2) 块间注意力：query 采用位置索引 c−1=7 来关注之前的块，使相对位置上限为 7；(3) 连续块注意力：用局部窗口 w=3 保持相邻块之间的局部性。这样所有相对位置都落在训练分布（0 到 7）内，同时保持跨块的平滑过渡。DCA 让 Qwen2.5 等模型在推理时支持高达 100 万 token 的超长上下文，而无需在百万 token 序列上持续训练。

<Note title="注意力汇聚（Attention Sinks）" emoji="📊" variant="info">

长上下文 Transformer 中会出现一个有趣现象：模型会给序列开头的 token 分配异常高的注意力分数，即使这些 token 在语义上并不重要。这种行为称为 <strong>attention sinks</strong>（<a href="https://arxiv.org/abs/2309.17453">Xiao et al.</a>）。这些初始 token 充当注意力分布的稳定机制，成为注意力可汇聚的“汇点”。

实践启示是：当上下文超过缓存大小时，只保留最开始少量 token 的 KV 缓存，并配合最近 token 的滑动窗口，就能在很大程度上恢复性能。这个简单改动让模型在不微调的情况下处理更长序列且不明显掉性能。

现代实现以不同方式利用 attention sinks。原始研究建议在预训练时加入一个专用占位 token 作为显式 attention sink。更近的做法是，像 <strong>gpt-oss</strong> 这样的模型把 attention sink 实现为<strong>每个 head 可学习的 bias logits</strong>，直接附加到注意力分数上，而不是真正加入输入 token。这样无需改动分词输入即可获得同样的稳定效果。

有意思的是，gpt-oss 还在注意力层本身使用了 bias 单元，这在 GPT-2 之后很少见。虽然这些 bias 单元通常被认为对标准注意力操作是多余的（<a href="https://arxiv.org/pdf/2302.08626">Dehghani et al.</a> 的实证结果显示对测试损失影响极小），但它们可以承担实现 attention sink 的专门功能。关键点是：无论通过特殊 token、可学习 bias，还是每个 head 的 logits，attention sink 都能在长上下文场景中为注意力分布提供稳定“锚点”，让模型即便在上下文无限增长时也能保留对全序列的通用信息。
</Note>

至此我们覆盖了注意力的核心组件：在内存与算力间平衡的不同 head 配置（MHA、GQA、MLA），帮助模型理解 token 顺序的位置编码策略（RoPE、NoPE 及其变体），以及让长上下文可行的注意力范围技术（滑动窗口、分块与 attention sinks）。我们也讨论了嵌入层的配置与初始化。这些架构选择定义了模型如何处理并表示序列。

但正确的架构只是胜利的一半。即使设计良好的模型，在规模化训练时也可能出现不稳定。下面我们看看帮助训练保持稳定的技术。

#### 提升稳定性

现在来看看 LLM 预训练中最大的挑战之一：不稳定性。它常表现为 loss 峰值或训练 loss 的突然跳变，在大规模训练中尤其常见。

我们会在 [Training Marathon](#the-training-marathon) 章节更深入地讨论不同类型的峰值以及如何处理（涉及浮点精度、优化器与学习率），但一些架构与训练技巧也能降低不稳定性，我们在这里先快速过一遍。以下是近期大规模训练（如 Olmo2 [@olmo2] 与 Qwen3 [@qwen3]）中常用的几种简单稳定化手段：Z-loss、对嵌入层移除权重衰减，以及 QK-norm。

 **Z-loss** 

Z-loss [@palm] 是一种正则化方法，通过在损失函数中加入惩罚项，防止最终输出 logits 过大。该正则项鼓励 logits 的 softmax 分母保持在合理范围内，有助于训练过程的数值稳定性。

$$
\mathcal{L}_{\text{z-loss}} = \lambda \cdot \log^2(Z)
$$

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/zloss_comparison.csv",
xColumn: "tokens",
yColumn: "loss",
runColumn: "run_name",
xScaleType: "linear",
yScaleType: "linear",
xAxisLabel: "Consumed tokens",
yAxisLabel: "Training Loss",
xDomain: [0, 45e9],
yDomain: [1, 5],
}}
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/zloss_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: false,
smoothingWindow: 15
}}
/>
</Wide>



我们在 1B 模型上的消融结果显示，加入 Z-loss 不会影响训练 loss 或下游性能。对于 SmolLM3，我们最终没有采用它，因为实现上的 Z-loss 带来了一定训练开销，而在开始训练前我们还没来得及优化。

 **移除嵌入层的权重衰减** 

权重衰减通常作为正则化手段应用于所有参数，但 @olmo2 [ ](https://arxiv.org/abs/2501.00656) 发现，将嵌入层排除在权重衰减之外可以提升训练稳定性。原因在于权重衰减会让嵌入向量的范数在训练中逐步减小，而层归一化的雅可比与输入范数成反比，这会导致早期层的梯度变大 [@takase2025spikemorestabilizingpretraining]。

我们测试了三种配置：标准权重衰减的基线、嵌入层不施加权重衰减的变体，以及把我们最终采用的改动组合在一起的配置（嵌入层无权重衰减 + NoPE + 文档级遮盖），以确保这些技术之间没有负面相互作用。三者的 loss 曲线与评测结果几乎一致，因此我们在 SmolLM3 训练中采用了这三项改动。

<HtmlEmbed 
  src="embeds/d3-line-chart.html" 
  config={{ 
    dataUrl: "./data/no_wd_comparison.csv",
    xColumn: "tokens",
    yColumn: "loss",
    runColumn: "run_name",
    xScaleType: "linear",
    yScaleType: "linear",
    xAxisLabel: "Consumed tokens",
    yAxisLabel: "Training Loss",
    xDomain: [0, 45e9],
    yDomain: [1, 5],
  }} 
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/no-wd_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: false,
smoothingWindow: 15
}}
/>
</Wide>



 **QKnorm** 

QK-norm [@dehghani2023scalingvisiontransformers22] 在计算注意力前对 query 与 key 向量施加层归一化。该方法可防止注意力 logits 过大，并在许多近期模型中用于提升稳定性。

不过，@rnope 发现 QK-norm 会削弱长上下文任务表现。他们的分析表明，QK-norm 会降低对相关 token（needle）的注意力权重，同时提高对无关上下文的注意力权重。他们认为这是因为归一化移除了 query-key 点积中的幅度信息，使注意力 logits 的量级更接近。基于这一原因，我们没有在 SmolLM3 中使用 QK-norm。另外，SmolLM3 作为 3B 参数的小模型，相比那些从 QK-norm 中获益更多的大模型，面临的训练不稳定风险也更低。

#### 其他核心组件

在我们已经覆盖的组件之外，还有几个架构层面的选择值得补充说明。

参数初始化方面，现代模型通常采用截断正态初始化（mean=0，std=0.02 或 std=0.006），或使用如 muP [@mup] 这样的初始化方案，例如 Cohere 的 Command A [@commandacohere]。这也是一个值得做消融实验的方向。

激活函数方面，SwiGLU 已成为现代 LLM 的事实标准（例外是 Gemma2 使用 GeGLU，以及 NVIDIA 使用 relu^2 [@nvidia2025nvidianemotronnano2; @nvidia2024nemotron4340btechnicalreport]），取代了较早的 ReLU 或 GELU。

在更宏观的层面，架构布局也会影响模型行为。虽然总参数量在很大程度上决定了语言模型的容量，但这些参数在深度与宽度上的分配同样重要。@petty2024impactdepthcompositionalgeneralization 发现，在语言建模与组合性任务上，深度更深的模型会优于同规模的更宽模型，直到收益趋于饱和。这种“深而窄”的策略在 MobileLLM 的消融中对十亿级以下的 LLM 尤其有效 [@mobilellm]，而更宽的模型往往由于更高的并行性而带来更快的推理速度。现代架构对这一权衡的取舍各不相同，详见这篇[博客文章](https://sebastianraschka.com/blog/2025/the-big-llm-architecture-comparison.html)。

到这里，我们已经覆盖了密集 Transformer 架构中最值得优化的核心部分。不过近来还出现了更针对整体模型的架构改动，主要是 MoE 与混合模型。下面我们从 MoE 开始看看它们能带来什么。

#### 走向稀疏：MoE 

*Mixture-of-Experts（MoE）* 的直觉很简单：并不是每个 token 的预测都需要整套模型，就像大脑会根据任务激活不同区域（例如视觉或运动皮层）。对于 LLM，这意味着学会了代码语法的部分，在进行翻译任务时未必需要被调用。如果路由做得好，就能在推理时只运行模型的一部分，从而节省大量计算。

从技术角度看，MoE 的目标也很清晰：在不增加每个 token “激活参数”数量的前提下，提升总参数量。简单来说，总参数量决定模型的总体学习容量，而激活参数决定训练成本与推理速度。这也是为什么如今许多前沿系统（如 DeepSeek V3、K2，以及闭源模型 Gemini、Grok 等）都在使用 MoE 架构。下面这张来自 Ling 1.5 论文 [@ling15] 的图对比了 MoE 与稠密模型的缩放规律：

<Image src={image_2931384e_bcac_80c4_ab02_f22c53e6fdee} alt="Image" />

如果你第一次接触 MoE，不用担心，它的机制并不复杂。我们从标准的稠密架构开始，看看 MoE 需要做哪些改动（图来自 [Sebastian Raschka](https://github.com/rasbt/LLMs-from-scratch/tree/main/ch04/07_moe)）：

<Wide>

<Image src={image_2931384e_bcac_8062_bc11_d1ee3706d996} alt="Image" />
</Wide>

在 MoE 中，我们把单个 MLP 替换为多个 MLP（“专家”），并在它们之前加入一个可学习的路由器。对于每个 token，路由器只选择一小部分专家执行。这就是总参数与激活参数的区别来源：模型有很多专家，但任何一个 token 实际只会使用其中几个。

设计 MoE 层会引出几个核心问题：

- 专家形状与稀疏度：是使用大量小专家，还是少量大专家？每个 token 应激活多少专家、总共需要多少专家（即稀疏度或 “top-k”）？是否需要一些通用专家始终保持激活？
- 利用率与专门化：如何选择被路由的专家，让它们被充分使用（避免闲置容量）同时又能形成专门化？在实践中这就是负载均衡问题，对训练与推理效率影响很大。

这里我们聚焦一个目标：在固定算力预算下，如何选择 MoE 配置以最小化 loss？这与纯粹的系统效率（吞吐/延迟）是不同的问题，我们稍后会再讨论。以下内容主要基于蚂蚁集团的 MoE scaling laws 分析 [@antgroup]。

我们会使用他们提出的 *Efficiency Leverage（EL）* 概念。简单来说，EL 衡量的是：要达到某个 MoE 设计的 loss，稠密模型需要多少计算量（以 FLOPs 为单位）。EL 越高，说明该 MoE 配置在单位算力下带来的 loss 改善越多。

<HtmlEmbed src="embeds/efficiency-leverage.html"/>



下面我们更细致地看看，如何通过设置 MoE 的稀疏度来提升效率杠杆。

 **稀疏度 / 激活比例** 

<Quote>

<b>TL;DR:</b> 稀疏度更高 → FLOPs 效率更好 → 极高稀疏度收益递减 → 最佳点取决于算力预算。
</Quote>

本节我们想回答哪个 MoE 设置最好。从极限来看，两端都不是理想方案：一端是始终激活所有专家，等同于稠密模型；另一端是激活参数极少（极端情况下只有一个参数被激活），显然连狭窄领域任务都难以胜任。因此我们需要寻找中间地带。在深入讨论之前，先定义两个量：***激活比例*** 及其倒数 ***稀疏度*** *：* 

$$
\text{activation ratio} \;=\; \frac{\#\text{activated experts}}{\#\text{total experts}}
$$

$$
\text{sparsity} \;=\; \frac{\#\text{total experts}}{\#\text{activated experts}} \;=\; \frac{1}{\text{activation ratio}}
$$

从算力角度看，成本只由激活参数决定。如果保持激活专家的数量（及大小）不变，同时增加专家总数，那么推理/训练 FLOPs 预算大致不变，但模型容量增加，因此只要训练足够久，模型整体会更好。

回顾近期 MoE 论文，有一些清晰的经验结论：在激活专家的数量与规模固定的情况下，增加专家总数（即降低激活比例/提高稀疏度）会带来更好的 loss，但当稀疏度很高时收益会递减。

两个例子：

- Kimi K2 图 [@kimik2]：展示了两种效应，稀疏度更高性能更好，但随着稀疏度增加收益逐渐变小。
- Ant Group 图 [@antgroup]：与 K2 结论一致，并补充了一个结果：稀疏度更高的 MoE 受益于增加算力更明显。

<Image src={Capture_decran_2025_10_20_a_13_25_47_2921384e_bcac_8087_83e5_fa7a40c1f342} alt="Image" />

<Wide>

<Image src={Capture_decran_2025_10_20_a_13_26_08_2921384e_bcac_80b5_ac36_fb73d6374208} alt="Image" />
</Wide>

下面是一些 MoE 模型的稀疏度表：

<Wide>

| Model | Total experts | Activated per token (incl. shared) | Sparsity |
| --- | --- | --- | --- |
| Mixtral-8×7B | 8 | 2 | 4.0 |
| Grok-1 | 8 | 2 | 4.0 |
| Grok-2 | 8 | 2 | 4.0 |
| OLMoE-1B-7B-0924 | 64 | 8 | 8.0 |
| gpt-oss 20b | 32 | 4 | 8 |
| Step-3 | 48 routed + 1 shared = 49 | 3 routed + 1 shared = 4 | 12.25 |
| GLM-4.5-Air | 128 routed + 1 shared = 129 | 8 routed + 1 shared = 9 | 14.3 |
| Qwen3-30B-A3B | 128 | 8 | 16.0 |
| Qwen3-235B-A22B | 128 | 8 | 16.0 |
| GLM-4.5 | 160 routed + 1 shared = 161 | 8 routed + 1 shared = 9 | 17.8 |
| DeepSeek-V2 | 160 routed + 2 shared = 162 | 6 routed + 2 shared = 8 | 20.25 |
| DeepSeek-V3 | 256 routed + 1 shared = 257 | 8 routed + 1 shared = 9 | 28.6 |
| gpt-oss 120b | 128 | 4 | 32 |
| Kimi K2 | 384 routed + 1 shared = 385 | 8 routed + 1 shared = 9 | 42.8 |
| Qwen3-Next-80B-A3B-Instruct | 512 routed + 1 shared = 513 | 10 total active + 1 shared = 11 | 46.6 |
</Wide>

近期趋势很明确：MoE 模型越来越稀疏。但最佳稀疏度仍取决于硬件与端到端效率。例如，Step-3 追求峰值效率，刻意不把稀疏度拉满，以适配特定硬件与带宽约束；而 gpt-oss-20b 因为设备内存限制而保持较低稀疏度（未激活专家仍会占用部分内存）。

 **粒度（Granularity）** 

除了稀疏度之外，我们还需要决定每个专家应该有多大。这可以用蚂蚁集团提出的“粒度”指标来刻画。不同论文的术语与公式略有差异，这里我们采用与引用图一致的定义：

$$
G = \frac{\alpha*d_{model}}{d_{expert}} \text{ with } \alpha = 2 \text{ or } 4
$$

在参数总量固定时，更高的粒度意味着更多、但更小的专家。该指标是专家维度（ $d_{expert}$ ）与模型维度（ $d_{model}$ ）之间的比值。 

在稠密模型中，一个常见经验是将 MLP 维度设为  $d_{intermediate} = 4 * d_{model}$ 。当  $\alpha = 4$  时（如 @krajewski2024scalinglawsfinegrainedmixture），可以把粒度粗略理解为 **需要多少个专家才能匹配稠密 MLP 的宽度**（ $4\, d_{\text{model}} = d_{\text{intermediate}} = G\, d_{\text{expert}}$ ）。 

这个解释只是粗略的启发式：现代 MoE 设计往往分配了远超单个稠密 MLP 的总容量，因此一一对应在实践中并不成立。蚂蚁团队选择  $\alpha = 2$ ，只是另一种归一化选择。为保持一致性，我们也采用这一约定。

<Sidenote>

由于 G 会随 $d_{\text{model}}$ 缩放，当模型宽度不同时，跨模型比较会变得复杂。
</Sidenote>

  

下面仍给出一些 MoE 发布模型的粒度数值：

| Model | $(d_\text{model})$ | $(d_\text{expert})$ | $(G = 2 d_\text{model} / d_\text{expert})$ | Year |
| --- | --- | --- | --- | --- |
| Mixtral-8×7B | 4,096 | 14,336 | 0.571 | 2023 |
| gpt-oss-120b | 2880 | 2880 | 0.5 | 2025 |
| gpt-oss-20b | 2880 | 2880 | 0.5 | 2025 |
| Grok 2 | 8,192 | 16,384 | 1.0 | 2024 |
| StepFun Step-3 | 7,168 | 5,120 | 2.8 | 2025 |
| OLMoE-1B-7B | 2,048 | 1,024 | 4.0 | 2025 |
| Qwen3-30B-A3B | 2,048 | 768 | 5.3 | 2025 |
| Qwen3-235B-A22B | 4,096 | 1,536 | 5.3 | 2025 |
| GLM-4.5-Air | 4,096 | 1,408 | 5.8 | 2025 |
| DeepSeek V2 | 5,120 | 1,536 | 6.6 | 2024 |
| GLM-4.5 | 5,120 | 1,536 | 6.6 | 2025 |
| Kimi K2 | 7,168 | 2,048 | 7.0 | 2025 |
| DeepSeek V3 | 7168 | 2048 | 7.0 | 2024 |
| Qwen3-Next-80B-A3B | 2048 | 512 | 8.0 | 2025 |

下面看看粒度如何影响模型行为（来自 [Ant Group 论文](https://arxiv.org/pdf/2507.17702)）：

<Image src={img_75ae60ff_50be_48e1_aad2_a8fc56120d3d_2921384e_bcac_80c2_984b_d81404e4bb7c} alt="Image" />

粒度看起来不是影响 EL 的主要驱动因素——它确实有帮助，尤其是高于 2 时，但并不是决定 loss 的主导变量。不过仍有一个甜点区间：粒度提高到一定程度后收益会趋于平缓。因此，粒度是一个有用的调节旋钮，近期发布模型也有趋向更高粒度的趋势，但不应单独优化它。

另一种广泛用于改进 MoE 的方法是“共享专家”。我们来看看它的作用。

 **共享专家** 

共享专家的做法是：把每个 token 都路由到一小组始终激活的专家。这些共享专家负责吸收数据中基础且重复的模式，使其他专家能更激进地专门化。实践中你不需要很多共享专家，模型设计者通常只用一个，最多两个。随着粒度增加（例如从 Qwen3 风格走向更接近 Qwen3-Next 的设定），共享专家往往更有用。从下面的图看，整体影响不算大，对 EL 的提升有限。一个简单且好用的经验法则是：只用一个共享专家，这与 DeepSeek V3、K2、Qwen3-Next 等模型的选择一致，通常能在不增加不必要复杂度的前提下最大化效率。图来自 @antgroup。

<Wide>

<Image src={Capture_decran_2025_10_21_a_11_11_38_2931384e_bcac_8008_ad8d_d5ab0c539d3a} alt="Image" />
</Wide>

共享专家就是一类始终会被部分 token 路由到的专家。那么其他专家怎么办？我们如何学习将 token 路由到不同专家，同时避免只使用少数几个专家？接下来我们讨论负载均衡，它正是为了解决这个问题。

 **负载均衡** 

负载均衡是 MoE 的关键环节。如果做得不好，其他所有设计选择都会被削弱。举个简单例子：假设我们有 4 张 GPU，把 4 个专家均匀分配到这些 GPU 上。如果路由发生坍缩，所有 token 都被送到专家 1，那么只有 1/4 的 GPU 被利用，训练与推理效率极差。同时，由于并非所有专家都被激活，模型的有效学习容量也降低了。

为了解决这个问题，我们可以在路由器上加入额外的损失项。下面是标准的基于辅助损失的负载均衡（LBL）：

$$
\mathcal{L}_{\text{Bal}} \;=\; \alpha \sum_{i=1}^{N_{r}} f_{i}\, P_{i}
$$

这个公式只包含三个因素：系数  $\alpha$  决定损失强度； $f_i$  是流量占比，即经过专家  $i$  的 token 比例； $P_i$  是概率质量，等于所有 token 经过该专家的概率之和。二者都很重要： $f_i$  反映真实的均衡情况，而  $P_i$  是平滑可导的，便于梯度传播。若实现完美均衡，则  $f_i=P_i=1/N_r$ 。但  $\alpha$  需要谨慎调节：太小无法有效引导路由，太大则会让路由均匀性压过主语言模型的 loss。

<Note title="Loss free load balancing" emoji="💡" variant="info">

也可以在没有显式损失项的情况下实现均衡。DeepSeek v3 [@deepseekv3] 在路由 softmax 的亲和度分数上加入一个简单的偏置项：如果某个专家过载，就把它的分数下调一点（常数  $\gamma$ ），让它更不容易被选中；若某专家利用不足，则把分数上调  $\gamma$ 。这个简单的自适应规则也能实现负载均衡。
</Note>



一个关键细节是路由统计的计算范围： $f_i$  和  $P_i$  是按本地 batch（每个 worker 的 mini-batch）计算，还是全局计算（跨设备汇总）？Qwen 团队的分析 [@qiu2025demonsdetailimplementingload] 表明，当本地 batch 的 token 多样性不足时，本地统计会损害专家专门化（它是路由健康状况的良好代理）以及整体模型性能。所谓专家专门化，是指某些专家在特定领域被更频繁地激活。换言之，如果本地 batch 过窄，它的路由统计会变得噪声大且偏置，从而无法形成良好均衡。这意味着在可行时应使用全局统计（或至少跨设备聚合）。值得注意的是，该论文发表时，包括 Megatron 在内的许多框架默认都使用本地统计。

下图来自 Qwen 论文，展示了 micro-batch 与 global batch 聚合的差异及其对性能与专门化的影响：

<Wide>

<Image src={Capture_decran_2025_10_21_a_15_34_27_2931384e_bcac_8066_834b_c485ae8d1fa5} alt="Image" />
</Wide>

总体来说，围绕 MoE 的架构选择做消融并不容易，因为它与许多因素存在相互作用。例如，共享专家是否有用可能取决于模型的粒度。因此，值得花些时间设计好实验组合，才能真正得到你想要的洞见。 

我们已经覆盖了 MoE 的基础要点，但仍有许多值得探索的方向。下面是一个不完全列表：

- 零计算专家、MoE 层重缩放与训练监控（LongCat-Flash 论文）。
- 正交损失负载均衡（如 ERNIE 4.5）。
- 训练中对负载均衡系数的调度。
- MoE 与架构/优化的相互作用，例如：
    - 优化器排名是否会因 MoE 改变。
    - 如何将 MuP 应用于 MoE。
    - 如何调整 MoE 的学习率（因为每个 batch 看到的 token 数量不同）。

- 开头稠密层的数量。
- 以及更多。

我们把更深入的“兔子洞”留给有兴趣的读者探索，接下来我们转向最后一个主要架构选择：混合模型！

#### 专题讨论：混合模型

最近的一个趋势是，在标准的 dense 或 MoE 架构中加入状态空间模型（SSM）或线性注意力机制 [@falconh1; @minimax01]。**这些新模型类别试图解决 Transformer 的一些基础弱点：如何高效处理超长上下文。它们介于循环模型与 Transformer 之间：循环模型能高效处理任意长度上下文并线性扩展，但可能难以充分利用上下文信息；而 Transformer 在长上下文下成本极高，却能很好地利用上下文模式。

一些研究 [@waleffe2024empiricalstudymambabasedlanguage] 试图理解例如 Mamba 模型（SSM 的一种）的弱点，发现这类模型在许多基准上表现不错，但是像在 MMLU 上表现不佳，并推测差距来自缺乏 情景学习能力（in-context learning）。这也是它们与 dense 或 MoE 模块组合、取长补短的原因，因此称为混合模型。

这些线性注意力方法的核心思想是重排计算，使注意力不再是 O(n^2d) 的成本，避免在长上下文下变得不可承受。怎么做到的？先回忆推理时的注意力公式。生成 token t 的输出为：

$$
\mathbf{o}_{t} \;=\; \sum_{j=1}^{t} \frac{\exp\!\big(\mathbf{q}_{t}^{\top}\mathbf{k}_{j}\big) \mathbf{v}_{j}}{\sum_{l=1}^{t} \exp\!\big(\mathbf{q}_{t}^{\top}\mathbf{k}_{l}\big)} \,
$$

现在去掉 softmax：

$$
o_t = \sum_{j=1}^{t} (q_t^\top k_j)\, v_j
$$

重排得到：

$$
\quad\sum_{j=1}^{t}(q_t^\top k_j)\,v_j = \Big(\sum_{j=1}^{t} v_j k_j^\top\Big) q_t.
$$

定义运行态：

$$
S_t \triangleq \sum_{j=1}^{t} k_j v_j^\top = K_{1:t}^\top V_{1:t} \in \mathbb{R}^{d\times d}
$$

并使用简单的更新：

$$
S_t = S_{t-1} + k_t v_t^\top 
$$

因此可写为：

$$
o_t = S_t q_t    = S_{t-1} q_t + v_t (k_t^\top q_t)
$$

为什么重排很重要：左边形式 $\sum_{j\le t}(q_t^\top k_j)v_j$ 表示“对每个过去的 token j 计算点积 $q_t^\top k_j$（标量），用它缩放 $v_j$，再把这 t 个向量相加”，在 t 次的工作量约为 $O(td)$。右边形式把它改写为 $\big(\sum_{j\le t} v_j k_j^\top\big) q_t$：你维护一个运行态矩阵 $S_t=\sum_{j\le t} v_j k_j^\top\in\mathbb{R}^{d\times d}$，它已经汇总了所有过去的 $(k_j,v_j)$。每来一个新 token，用一次外积 $v_t k_t^\top$ 更新它，成本 $O(d^2)$，然后输出只是一次矩阵-向量乘 $S_t q_t$（同样是 $O(d^2)$）。所以从头用左边形式生成 T 个 token 是 $O(T^2 d)$，而维护 $S_t$ 并用右边形式是 $O(T d^2)$。直观上：左边是“每一步很多小的点积-缩放-相加”；右边是“一个预先汇总的矩阵乘查询”，用对维度的依赖换掉对序列长度的依赖。这里我们聚焦推理和递推形式，但在训练中也更高效，重排只需下面的等式：

$$
\underset{n\times n}{(QK^\top)}\,V \;=\; Q\,\underset{d\times d}{(K^\top V)}
$$

所以可以看到它现在很像 RNN 的结构。这就解决问题了吗？几乎如此。在实践中，softmax 起到重要的稳定作用，朴素的线性形式如果没有归一化可能不稳定。这就引出了一个更实用的变体：Lightning Attention 或 Norm Attention！

 **Lightning and norm attention** 

这一家族出现在 Minimax01 [@minimax01] 以及更近期的 Ring-linear [@lingteam2025attentionmattersefficienthybrid] 中，建立在 Norm Attention 的思路之上 [@qin2022devillineartransformer]。关键一步很简单：**对输出做归一化**。Lightning 变体强调实现上的高速与高效，并让公式略有不同。两者公式如下：

NormAttention: 

$$
\text{RMSNorm}(Q(K^TV))
$$

LightningAttention:

$$
Q= \text{Silu(Q)}, \; K = \text{Silu(K)}, \; V = \text{Silu(V)}
$$

$$
O = \text{SRMSNorm}(Q(KV^T))
$$

根据 Minimax01 的实证结果，使用 Norm attention 的混合模型在大多数任务上与 softmax 表现相当。

<Wide>

<HtmlEmbed src="embeds/benchmark-performance.html"/>
</Wide>



有意思的是，在 Needle in a Haystack（NIAH）这类检索任务上，它能远超完整 softmax 注意力，这看起来有些反直觉，但可能说明 softmax 与线性层协同工作时存在某种增益！

<Note title="MiniMax M2" variant="info">

令人惊讶的是，最近发布的 MiniMax M2 并未采用混合或线性注意力。根据他们的 <a href="[https://x.com/zpysky1125/status/1983383094607347992](https://x.com/zpysky1125/status/1983383094607347992)" target="_blank">预训练负责人</a>所述，早期 MiniMax M1 在较小规模上用 Lightning Attention 做实验时，在当时流行的基准（MMLU、BBH、MATH）上看起来很有希望，但在更大规模上发现其在“复杂、多跳推理任务”上存在明显短板。他们还指出 RL 训练中的数值精度问题与基础设施成熟度是关键阻碍。他们的结论是：在规模化场景中做架构选择是一个多变量、计算成本很高的问题，因为它对数据分布、优化器等参数都很敏感……

不过，他们也承认“随着 GPU 计算增长放缓而数据长度持续增长，线性与稀疏注意力的优势将逐步显现”。这同时突显了架构消融的复杂性，以及研究与生产现实之间的差距。
</Note>

现在我们再看一些相关方法，以及如何用统一框架来理解它们。

 **Advanced linear attention** 

从循环模型得到的一个有用经验是，让状态偶尔“放下”过去。在实践中，这意味着为上一时刻的状态引入门控 $\mathbf{G}_t$：

$$
\mathbf{S}_t \;=\; \mathbf{G}_t \odot \mathbf{S}_{t-1} \;+\; \mathbf{v}_t \mathbf{k}_t^{\top}
$$

几乎所有近期的线性注意力方法都有这个门控组件，只是对 $\mathbf{G}_t$ 的实现不同。来自 [这篇论文](arxiv.org/abs/2312.06635) 的表格列出了不同的门控变体及其对应架构：

<Wide>

<Reference caption="Gated linear attention formulation of recent models, which vary in their parameterization of  $\mathbf{G}_t$ . The bias terms are omitted.">

| Model | Parameterization | Learnable parameters |
| --- | --- | --- |
| Mamba [@mamba] | $\mathbf{G}_t = \exp(-(\mathbf{1}^\top \boldsymbol{\alpha}_t) \odot \exp(\mathbf{A})), \quad \boldsymbol{\alpha}t = \text{softplus}(\mathbf{x}t \mathbf{W}{\alpha_1} \mathbf{W}{\alpha_2})$ | $\mathbf{A} \in \mathbb{R}^{d_k \times d_v}, \quad \mathbf{W}{\alpha_1} \in \mathbb{R}^{d \times \frac{d}{16}}, \quad \mathbf{W}{\alpha_2} \in \mathbb{R}^{\frac{d}{16} \times d_v}$ |
| Mamba-2 [@mamba2] | $\mathbf{G}_t = \gamma_t \mathbf{1}^\top \mathbf{1}, \quad \gamma_t = \exp(-\text{softplus}(\mathbf{x}t \mathbf{W}{\gamma})\exp(a))$ | $\mathbf{W}_{\gamma} \in \mathbb{R}^{d \times 1}, \quad a \in \mathbb{R}$ |
| mLSTM [@beck2025tiledflashlinearattention,; @peng2021randomfeatureattention] | $\mathbf{G}_t = \gamma_t \mathbf{1}^\top \mathbf{1}, \quad \gamma_t = \sigma(\mathbf{x}t \mathbf{W}{\gamma})$ | $\mathbf{W}_{\gamma} \in \mathbb{R}^{d \times 1}$ |
| Gated Retention [@sun2024cacheoncedecoderdecoderarchitectures] | $\mathbf{G}_t = \gamma_t \mathbf{1}^\top \mathbf{1}, \quad \gamma_t = \sigma(\mathbf{x}t \mathbf{W}{\gamma})^{\frac{1}{\tau}}$ | $\mathbf{W}_{\gamma} \in \mathbb{R}^{d \times 1}$ |
| DFW (Mao, 2022; Pramanik et al., 2023) [@mao2022finetuningpretrainedtransformersdecaying; ] | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \boldsymbol{\beta}_t, \quad \boldsymbol{\alpha}_t = \sigma(\mathbf{x}t \mathbf{W}{\alpha}), \quad \boldsymbol{\beta}_t = \sigma(\mathbf{x}t \mathbf{W}{\beta})$ | $\mathbf{W}{\alpha} \in \mathbb{R}^{d \times d_k}, \quad \mathbf{W}{\beta} \in \mathbb{R}^{d \times d_v}$ |
| GateLoop [@katsch2024gateloopfullydatacontrolledlinear] | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}_t = \sigma(\mathbf{x}t \mathbf{W}{\alpha_1})\exp(\mathbf{x}t \mathbf{W}{\alpha_2} \mathrm{i})$ | $\mathbf{W}{\alpha_1} \in \mathbb{R}^{d \times d_k}, \quad \mathbf{W}{\alpha_2} \in \mathbb{R}^{d \times d_k}$ |
| HGRN-2 [@qin2024hgrn2gatedlinearrnns] | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}_t = \gamma + (1-\gamma)\sigma(\mathbf{x}t \mathbf{W}{\alpha})$ | $\mathbf{W}_{\alpha} \in \mathbb{R}^{d \times d_k}, \quad \gamma \in (0,1)^{d_k}$ |
| RWKV-6 [@peng2024eaglefinchrwkvmatrixvalued] | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}_t = \exp(-\exp(\mathbf{x}t \mathbf{W}{\alpha}))$ | $\mathbf{W}_{\alpha} \in \mathbb{R}^{d \times d_k}$ |
| Gated Linear Attention (GLA) | $\mathbf{G}_t = \boldsymbol{\alpha}_t^\top \mathbf{1}, \quad \boldsymbol{\alpha}t = \sigma(\mathbf{x}t \mathbf{W}{\alpha_1} \mathbf{W}{\alpha_2})^{\frac{1}{\tau}}$ | $\mathbf{W}{\alpha_1} \in \mathbb{R}^{d \times 16}, \quad \mathbf{W}{\alpha_2} \in \mathbb{R}^{16 \times d_k}$ |
</Reference>

</Wide>



其中一个值得注意的变体是 Mamba-2 [@mamba2]。它被用于许多混合模型，例如 Nemotron-H [@nemotronh]、Falcon H1 [@falconh1] 和 Granite-4.0-h [@granite4]。 

不过这仍处于早期阶段，扩展到大型混合模型时有很多细微差别需要考虑。尽管前景可期，MiniMax 在 [M2](https://x.com/zpysky1125/status/1983383094607347992) 的经验表明，小规模收益并不一定能转化为大规模生产系统的收益，尤其在复杂推理任务、RL 训练稳定性以及基础设施成熟度方面。即便如此，混合模型的发展很快，依然是前沿训练的稳健选择。Qwen3-Next（带门控的 DeltaNet 更新）[@qwen3next] 报告称在长上下文推理更快、训练更快、在常见基准上更强。我们也期待 Kimi 的下一代模型，很可能使用其新的 ["Kimi Delta Attention](https://github.com/fla-org/flash-linear-attention/pull/621)["](https://github.com/fla-org/flash-linear-attention/pull/621)。此外也要提到稀疏注意力，它通过选择 block 或 query 计算注意力来解决线性注意力同样的长上下文问题。例子包括 Native Sparse Attention [@nsa]、DeepSeek Sparse Attention [@dsa] 和 InfLLM v2 [@minicpm4]。

在转向 tokenizer 之前，我们会用一个小的决策树来收束架构选择，判断是训练 dense、MoE 还是混合模型。

####  **To MoE or not MoE: 选择基本架构** 

我们已经看过 dense、MoE 和混合模型，所以你可能自然会好奇：该选哪一个？你的架构选择通常取决于模型的部署场景、团队的经验，以及时间窗口。下面我们简要概览各架构的优缺点，并给出一个简单的指引流程，帮助你做出合适选择。

 **Dense transformers**  是最基础的标准 decoder-only transformer，每个 token 都会激活所有参数。数学推导可参考 [The Annotated Transformers](https://nlp.seas.harvard.edu/2018/04/03/attention.html)，直观理解可参考 [The Illustrated Transformers](https://jalammar.github.io/illustrated-transformer/)。

优点：生态支持广、理解成熟、训练稳定、单位参数性能好。

缺点：计算量随规模线性增长，70B 模型的成本约为 3B 的 ~23×。

<Note variant="success">

这通常是内存受限场景或新手训练 LLM 的默认选择。
</Note>

 **Mixture of Experts (MoE)**  将 Transformer 的前馈层替换为多个“专家”。对每个 token，门控网络只路由到少数专家，因此可以用一小部分计算获得大模型的容量。比如 [Kimi K2](https://huggingface.co/moonshotai/Kimi-K2-Instruct) 的总参数为 1T，但每个 token 仅激活 32B。代价是必须把所有专家都加载到内存中。可视化理解可参考 [这篇博客](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mixture-of-experts)。

优点：训练与推理的单位计算性能更好。

缺点：内存占用高（必须加载所有专家）；训练比 dense 更复杂；框架支持在改善但不如 dense 成熟；分布式训练在专家放置、负载均衡、以及 all-to-all 通信上非常棘手。

<Note variant="success">

当你不受内存限制，且希望单位计算性能最大化时使用。
</Note>

 **Hybrid models**  将 Transformer 与 Mamba 等状态空间模型（SSM）结合，在部分操作上提供线性复杂度，从而规避注意力的二次增长。([Mathy blog](https://srush.github.io/annotated-mamba/hard.html) | [Visual guide](https://newsletter.maartengrootendorst.com/p/a-visual-guide-to-mamba-and-state))

优点：可能更擅长长上下文；对超长序列更高效。

缺点：成熟度不如 dense 与 MoE，经过验证的训练配方更少；框架支持有限。

<Note variant="success">

当你希望扩展到超大上下文，同时降低标准 Transformer 推理开销时使用。
</Note>

<Sidenote>

我们也看到一些团队在探索文本扩散模型，但这类模型（以及其他实验性替代方案）仍处于非常早期阶段，因此本文不作展开。
</Sidenote>

总结一下，先问清楚模型会部署到哪里，再结合团队经验和训练周期，评估你能承担多少探索成本：

<HtmlEmbed frameless src="embeds/model-architecture-decision-flowchart.html"/>



以 SmolLM3 为例，我们要做的是面向端侧部署的高质量小模型，时间大约只有 3 个月，且团队过去主要训练 dense 模型。于是我们排除了 MoE（内存限制）与混合模型（探索新架构的时间不足；而 dense 也能达到我们目标的 128k 最大上下文），最终选择了 Llama 风格的 dense 模型。

既然我们已经了解了模型架构的内部机制，接下来看看 tokenizer，它是数据与模型之间的桥梁。

#### 分词器

虽然分词器很少比架构创新更耀眼，但它可能是任何语言模型中最被低估的组件之一。把它想象成在人类语言与模型的数学世界之间的翻译器，翻译质量对最终效果有很大影响。那么我们应如何为自己的需求构建或选择合适的分词器？

 **分词器基础** 

分词器的核心任务是将原始文本切分并映射为模型可处理的数字序列（token）。在深入技术细节前，先回答几个会影响分词器设计的基本问题：

-  **我们要支持哪些语言？**  如果打算做多语言模型，但分词器只基于英语训练，那么遇到非英语文本时会被切割成更多 token，直接影响性能、训练成本与推理速度。
-  **哪些领域是重点？**  数学与代码等领域对数字与符号的表示要特别谨慎。
-  **我们是否知道目标数据的混合比例？**  若从零训练分词器，应尽量用接近最终训练混合的样本来训练分词器。

明确这些后，再看主要设计抉择：

<Sidenote>

如果想深入了解分词基础，Andrej Karpathy 的 ["Let's build the GPT Tokenizer"](https://www.youtube.com/watch?v=zduSFxRajkE) 是非常棒的动手教程。你也可以参考这个[资料](https://github.com/huggingface/evaluation-guidebook/blob/main/contents/general-knowledge/tokenization.md)，里面介绍了分词器并汇总了一些外部资源。
</Sidenote>

 **词表大小** 

词表相当于模型认识的“词典”——所有最小可处理单元（词、子词或符号）。

更大的词表能更高效压缩文本（每句话生成的 token 更少），但代价是嵌入矩阵变大：词表大小 V 与隐藏维度 h 决定了输入嵌入参数 V×h（输出层同理）。对小模型来说，嵌入会占据明显比例；模型越大，这部分占比越小。 

合适的词表大小取决于覆盖需求和模型规模：英语模型常见在 ~50k；多语言模型常需 100k+。许多最新 SOTA 模型（如 Llama3）采用 128k+ 的词表以提升多语种效率。对小模型可用嵌入共享（embedding sharing）来减轻大词表带来的参数增长。
@dagan2024gettingtokenizerpretrainingdomain 分析了词表大小对压缩率、推理与内存的影响。他们发现更大的词表带来的压缩收益呈指数递减，说明存在一个最优规模。就推理而言，大模型更能从大词表中受益，因为压缩带来的前向计算节省超过了 softmax 中额外嵌入 token 的成本。就内存而言，最优词表大小与序列长度和 batch size 相关：更长的上下文和更大的 batch 往往能从更大词表中获益，因为 token 更少会带来更小的 KV cache。

<Sidenote>

把词表大小设为 128 的倍数（如 50,304 而非 50,000）可以优化吞吐。现代 GPU 在矩阵维度可被更高的 2 次幂整除时性能更好，因为这样能确保内存对齐并降低非对齐访问的开销。更多细节见这篇[博客](https://www.thonking.ai/p/what-shapes-do-matrix-multiplications)。
</Sidenote>

 **分词算法** 

BPE (Byte-Pair Encoding) [@sennrich2016neuralmachinetranslationrare] 依然是最常用的方案，WordPiece 或 SentencePiece 也常见。研究方向上有尝试直接以字节或字符为单位、免去分词器的方案，但目前主流仍以子词算法为主。

在了解了决定分词器的关键参数后，我们要做一个实际决策：使用现成的分词器，还是从零训练？答案取决于覆盖度：现有分词器在我们期望的词表大小下，是否能很好覆盖我们的语言与领域。

下图对比了 GPT-2 的英文分词器 [@gpt2] 与 Gemma 3 的多语分词器 [@gemma3] 在同一条英语与阿拉伯语句子上的切分方式。

<Wide>

<HtmlEmbed frameless src="/embeds/tokenization-comparison.html" />
</Wide>



两者在英语上的效果相近，但在阿拉伯语上差异非常明显：GPT2 会把文本切成一百多个碎片，而 Gemma3 因为使用了多语训练数据和更大、更包容的词表，产生的 token 要少得多。

但要衡量分词器的质量，不能只靠“看起来还不错”的几个例子——就像我们不会在没有消融实验的情况下凭直觉改架构一样。我们需要明确的指标来评估分词器的质量。

 **衡量分词器质量** 

评估分词器效果时，我们可以使用 FineWeb2 [@fineweb2] 中的两项关键指标。

 **Fertility：** 

它衡量编码一个“词”平均需要多少个 token。Fertility 越低，压缩率越高，训练与推理也更快。可以这样理解：如果一个分词器在大多数词上多用一到两个 token，而另一个用得更少，那么后者明显更高效。

衡量 fertility 的标准方法是计算 **words-to-tokens ratio**（词级 fertility），也就是平均每个词需要多少 token。之所以围绕“词”来定义，是因为当有合适的分词工具时（如 [Spacy](https://spacy.io/) 与 [Stanza](https://stanfordnlp.github.io/stanza) [@fineweb2]），它能提供有意义的跨语言比较。

在单一语言内比较分词器时，也可以用字符或字节代替词，得到字符/字节到 token 的比率 [@dagan2024gettingtokenizerpretrainingdomain]。但这些指标在跨语言比较时有限制。字节数会因为脚本不同而偏斜（例如 UTF-8 中汉字占 3 个字节，而拉丁字符只占 1–2 个字节）。同样，字符数量无法体现不同语言词长差异，例如中文词通常比德语复合词短得多。

 **Proportion of continued words（续分词比例）：** 

该指标衡量有多少比例的词被切成多个片段。比例越低越好，因为被拆分的词更少，分词更高效。

下面实现这些指标：

```python
import numpy as np

def compute_tokenizer_metrics(tokenizer, word_tokenizer, text):
    """
    Computes fertility and proportion of continued words.
    
    Returns:
        tuple: (fertility, proportion_continued_words)
            - fertility: average tokens per word (lower is better)
            - proportion_continued_words: percentage of words split into 2+ tokens (lower is better)

    """
    words = word_tokenizer.word_tokenize(text)
    tokens = tokenizer.batch_encode_plus(words, add_special_tokens=False)
    tokens_per_word = np.array(list(map(len, tokens["input_ids"])))
    
    fertility = np.mean(tokens_per_word).item()
    proportion_continued_words = (tokens_per_word >= 2).sum() / len(tokens_per_word)
    
    return fertility, proportion_continued_words
```
不过在代码、数学等专业领域，仅看 fertility 还不够，我们还要关注分词器对领域特定模式的处理能力。多数现代分词器会把数字拆成单个数字（例如把 "123" 拆成 ["1", "2", "3"]）[@palm; @deepseekv2]。这看似反直觉，但其实有助于模型学习算术规律。如果把 "342792" 编成一个不可再分的 token，模型就得记住它与每个数字 token 做加减乘时的结果；而拆分后，模型能学到数字级的运算规律。一些分词器如 Llama3 [@llama3] 会把 1 到 999 编成独立 token，其余数字由这些 token 组合而成。

<Sidenote>

如需深入了解分词对算术能力的影响，可参考 <a href="https://huggingface.co/spaces/huggingface/number-tokenization-blog" target="_blank">From Digits to Decisions: How Tokenization Impacts Arithmetic in LLMs</a>，该文比较了不同分词方案在数学任务上的表现。
</Sidenote>

因此我们可以在目标领域上测 fertility，从而评估分词器的优势与短板。下表比较了不同分词器在多种语言与领域上的 fertility。

 **评估分词器** 

为了比较不同语言上的分词器表现，我们采用 [FineWeb2](https://arxiv.org/abs/2506.20920) 的分析设置，以 Wikipedia 文章作为评估语料。每种语言抽样 100 篇文章，既保证样本有代表性，又控制计算成本。

首先安装依赖，并定义要比较的分词器与语言：

```bash
pip install transformers datasets sentencepiece 'datatrove[multilingual]'
## we need datatrove to load word tokenizers
```
```python
tokenizers = [
    ("Llama3", "meta-llama/Llama-3.2-1B"),
    ("Gemma3", "google/gemma-3-1b-pt"),
    ("Mistral (S)", "mistralai/Mistral-Small-24B-Instruct-2501"),
    ("Qwen3", "Qwen/Qwen3-4B")
]

languages = [
    ("English", "eng_Latn", "en"),
    ("Chinese", "cmn_Hani", "zh"),
    ("French", "fra_Latn", "fr"),
    ("Arabic", "arb_Arab", "ar"),
]
```
接着加载 Wikipedia 样本，我们使用 streaming 来避免下载完整数据集：

```python
from datasets import load_dataset

wikis = {}
for lang_name, lang_code, short_lang_code in languages:
	wiki_ds = load_dataset("wikimedia/wikipedia", f"20231101.{short_lang_code}", streaming=True, split="train")
	wiki_ds = wiki_ds.shuffle(seed=42, buffer_size=10_000)
	# Sample 100 articles per language
  ds_iter = iter(wiki_ds)
  wikis[lang_code] = "\n".join([next(ds_iter)["text"] for _ in range(100)])
```
数据准备好后，就可以在每种语言上评估各分词器。对每个组合，我们从 [datatrove](https://github.com/huggingface/datatrove) 加载相应的词级分词器，并计算两项指标：

```python
from transformers import AutoTokenizer
from datatrove.utils.word_tokenizers import load_word_tokenizer
import pandas as pd

results = []
	
for tokenizer_name, tokenizer_path in tokenizers:
    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)
    
    for lang_name, lang_code, short_lang_code in languages:
        word_tokenizer = load_word_tokenizer(lang_code)
        
        # Compute metrics on Wikipedia
        fertility, pcw = compute_tokenizer_metrics(tokenizer, word_tokenizer, wikis[lang_code])
        
        results.append({
            "tokenizer": tokenizer_name,
            "language": lang_name,
            "fertility": fertility,
            "pcw": pcw
        })

df = pd.DataFrame(results)
print(df)
```
```bash
      tokenizer    language  fertility       pcw
0        Llama3     English   1.481715  0.322058
1        Llama3     Chinese   1.601615  0.425918
2        Llama3      French   1.728040  0.482036
3        Llama3     Spanish   1.721480  0.463431
4        Llama3  Portuguese   1.865398  0.491938
5        Llama3     Italian   1.811955  0.541326
6        Llama3      Arabic   2.349994  0.718284
7        Gemma3     English   1.412533  0.260423
8        Gemma3     Chinese   1.470705  0.330617
9        Gemma3      French   1.562824  0.399101
10       Gemma3     Spanish   1.586070  0.407092
11       Gemma3  Portuguese   1.905458  0.460791
12       Gemma3     Italian   1.696459  0.484186
13       Gemma3      Arabic   2.253702  0.700607
14  Mistral (S)     English   1.590875  0.367867
15  Mistral (S)     Chinese   1.782379  0.471219
16  Mistral (S)      French   1.686307  0.465154
17  Mistral (S)     Spanish   1.702656  0.456864
18  Mistral (S)  Portuguese   2.013821  0.496445
19  Mistral (S)     Italian   1.816314  0.534061
20  Mistral (S)      Arabic   2.148934  0.659853
21        Qwen3     English   1.543511  0.328073
22        Qwen3     Chinese   1.454369  0.307489
23        Qwen3      French   1.749418  0.477866
24        Qwen3     Spanish   1.757938  0.468954
25        Qwen3  Portuguese   2.064296  0.500651
26        Qwen3     Italian   1.883456  0.549402
27        Qwen3      Arabic   2.255253  0.660318
```
结果显示，不同优先目标会带来不同的赢家与取舍：

<Wide>

<HtmlEmbed frameless src="embeds/tokenizer-comparison.html"/>
</Wide>



Gemma3 分词器在多种语言上都达到了较低的 fertility 和拆分率，尤其是在英语、法语和西班牙语上，这与其分词器训练数据以及超大的 262k 词表有关，约为 Llama3 128k 的两倍。Qwen3 分词器在中文上表现突出，但在英语、法语和西班牙语上落后于 Llama3。Mistral Small 的分词器 [@mistralsmall] 在阿拉伯语上最好，但在英语和中文上不如其他分词器。

 **在现有分词器与自训分词器之间做选择** 

目前有不少强力分词器可选。许多新模型会以 GPT4 的分词器 [@gpt4] 为基础，再增加多语种 token。从上表可见，Llama 3 的分词器在多语文本与代码上表现均衡，而 Qwen 2.5 在中文与部分低资源语言上更突出。

-  **何时用现成分词器：** 如果我们的目标用例与上面这些分词器（Llama、Qwen、Gemma）的语言或领域覆盖相匹配，那么它们是经过实战验证的稳妥选择。SmolLM3 训练中我们选了 Llama3 的分词器：它在目标语言（英语、法语、西班牙语、葡萄牙语、意大利语）上表现竞争力强，同时词表规模适中，适合小模型。对于更大的模型，嵌入在总参数中占比更小，Gemma3 的效率收益会更有吸引力。
-  **何时自训分词器：** 如果你面向低资源语言，或者数据混合比例差异很大，就很可能需要自训分词器以保证覆盖度。此时应尽量用接近最终训练混合的数据来训练分词器。但这会带来“鸡生蛋、蛋生鸡”的问题：我们需要分词器来做数据消融以找到混合比例。解决方法是先用近似数据训练一个分词器做消融，最终开跑前再重训分词器，并验证下游性能提升且 fertility 仍然良好。

分词器的选择看似细节，却会影响模型性能的方方面面。所以别怕在这一步投入时间，把它打磨到位。

#### SmolLM3

在考察完架构版图并完成系统性消融之后，我们来看这些结论如何在 SmolLM3 这样的模型上落地。

SmolLM 系列的目标，是不断拓展小模型的可能性边界。SmolLM2 交付了 135M、360M 和 1.7B 三个能力扎实的模型，全部面向端侧高效运行。到了 SmolLM3，我们希望在仍适配手机的前提下提升性能，并补齐 SmolLM2 的短板：多语言、超长上下文处理，以及更强的推理能力。综合权衡后，我们选择 3B 参数作为最佳平衡点。

因为是在放大一套已验证的配方，我们自然选择了稠密 Transformer。MoE 当时还未在 nanotron 中实现，而我们也具备训练强小型稠密模型的经验与基础设施。更关键的是，端侧部署受限于内存：即使 MoE 只有少数专家被激活，仍需把所有专家加载进内存，因此会受到限制。对我们的端侧目标来说，稠密模型更现实。

 **Ablations:** 我们以 SmolLM2 1.7B 的架构为基础，先用 Qwen2.5-3B 的布局在 100B tokens 上训练了一个 3B 的消融模型。这样就有了一个稳固基线，便于逐项验证修改。每一项架构改动都必须满足其一：降低损失与提升英文基准的下游性能；或在不损失质量的前提下，带来可量化收益（如推理速度）。

在正式开跑最终版本前，我们测试了以下内容：

 **Tokenizer:**  在动架构之前，首先要选分词器。我们找到了覆盖目标语言与领域的一组可用分词器。基于 fertility 分析，Llama3.2 的分词器在我们的 6 种目标语言之间给出了最佳折中，同时词表维持在 128k：足够支撑多语效率，又不会让 3B 模型的嵌入参数过度膨胀。

 **Grouped Query Attention (GQA)** : 我们再次验证了先前结论：4 组的 GQA 能达到多头注意力的性能，这次是在 3B 规模、100B tokens 的设置下。KV cache 的效率提升非常可观，尤其在端侧内存紧张的情况下。

 **NoPE for long context** : 我们通过每 4 层移除一次 RoPE 来实现 NoPE。3B 的消融结果印证了上文发现：NoPE 能提升长上下文处理能力，同时不牺牲短上下文性能。 

 **Intra-document attention masking** : 训练时禁止跨文档注意力，有助于在超长序列训练中提升速度与稳定性，而下游性能并未受到影响。

 **Model layout optimization** : 我们对比了文献中近期 3B 模型的布局，有的偏深、有的偏宽。在我们的训练配置下测试了 Qwen2.5-3B（3.1B）、Llama3.2-3B（3.2B）和 Falcon3-H1-3B（3.1B）三种布局。结果很有意思：尽管 Qwen2.5-3B 参数更少，但三者在损失与下游性能上几乎一致。不过，Qwen2.5-3B 更深的架构与研究中“网络更深有利于泛化”的结论一致 [@petty2024impactdepthcompositionalgeneralization]。因此我们选择更深的布局，押注其在训练推进时更有优势。

 **Stability improvements** : 我们保留了 SmolLM2 的 tied embeddings，同时引入了受 OLMo2 启发的新技巧：对 embedding 取消 weight decay。消融结果表明这不会影响性能，同时降低 embedding 范数，有助于避免训练发散。

系统性消融的好处在于：我们可以确信每一项改动都被验证过，从而放心地将它们组合在一起。

<Note title="Combining changes in ablations" emoji="💡" variant="info">

实践中我们会逐步验证：一旦某个特性被确认有效，它就成为后续测试的基线。测试顺序也很关键：先从最稳妥的特性开始（tie embeddings → GQA → document masking → NoPE → remove weight decay）。
</Note>

#### 行动准则

<Quote>

TL;DR：用例决定选择。
</Quote>

 **让部署目标主导架构决策。**  在评估新的架构创新时，先考虑模型将在哪里、以何种方式运行。

 **在创新与务实之间找到平衡。**  关键架构进展不能忽视——当 GQA 和更优替代方案已存在时，仍使用多头注意力就是糟糕的技术选择。保持对最新研究的了解，并采纳那些在规模上有清晰、可验证收益的技术。但也要克制追逐每一篇只承诺微小增益的新论文的冲动（除非你资源充足或目标本就是架构研究）。

 **系统化胜过直觉。**  任何架构变更都要验证，不管它在纸面上多么诱人。然后逐项测试改动，再去组合，才能弄清各自影响。

 **规模效应真实存在——尽可能在目标规模上重做消融实验。**  不要假设小规模消融结论能在目标模型规模上完全成立。有算力就尽量复核。

 **在真实领域验证分词器效率。**  针对目标语言与领域的 fertility 指标，比追随最新模型用什么分词器更重要。50k 的英文分词器不足以胜任严肃的多语任务，但如果语言覆盖不多，也没必要上 256k 的词表。

既然模型架构已经确定，现在是时候着手选择将驱动学习过程的优化器和超参数了。

### Optimiser and training hyperparameters

The pieces are coming into place. We've run our ablations, settled on the architecture, and chosen a tokenizer. But before we can actually launch the training, there are still some crucial missing pieces: which optimizer should we use? What learning rate and batch size? How should we schedule the learning rate over training?

The tempting approach here is to just borrow values from another strong model in the literature. After all, if it worked for big labs, it should work for us, right? And for many cases that will work just fine if we're taking values from a similar architecture and model size.

However, we risk leaving performance on the table by not tuning these values for our specific setup. Literature hyperparameters were optimized for specific data and constraints, and sometimes those constraints aren't even about performance. Maybe that learning rate was picked early in development and never revisited. Even when model authors do thorough hyperparameter sweeps, those optimal values were found for their exact combination of architecture, data, and training regime, not ours. Literature values are always a good starting point, but it's a good idea to explore if we can find better values in the neighbourhood.

In this chapter, we'll explore the latest optimizers (and see if trusty old AdamW [@kingma2014adam] still stands the [test of time](https://blog.iclr.cc/2025/04/14/announcing-the-test-of-time-award-winners-from-iclr-2015/) 🎉), dive into learning rate schedules that go beyond the standard cosine decay and figure out how to tune the learning rate and batch size given a model and data size. 

Let's start with the the optimizer wars.

#### Optimizers: AdamW and beyond

The optimizer is at the heart of the whole LLM training operation. It decides for every parameter what the actual update step will be based on the past updates, the current weights and the gradients derived from the loss. At the same time it is also a [memory and compute hungry beast](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=profiling_the_memory_usage) and thus can impact how many GPUs you need and how fast your training is.

<Sidenote>

If you're uncertain what an optimizer is and what it's useful for, check Ruder's [blog on Gradient Descent and optimizers](https://www.ruder.io/optimizing-gradient-descent/), which notably compares cool optimizers
</Sidenote>

We didn't spare any efforts to summarize the current landscape of optimizers used for LLM pretraining:

| Model | Optimizer |
| --- | --- |
| Kimi K2, GLM 4.5 | Muon |
| Everyone else | AdamW |

So, you might wonder why everyone is using AdamW?

The person writing this part of the blog post thinks it's because "people are lazy" (hi, it's [Elie](https://x.com/eliebakouch)), but others might more realistically say that AdamW has been working well/better at different scales for a long time, and it's always a bit scary to change such a core component especially if it's hard (ie expensive) to test how well it does in very long trainings.

Moreover, comparing optimizers fairly is harder than it looks. Scale changes the dynamics in ways that can be hard to simulate in small ablations, so hyperparameter tuning is complex. You could say:  *"it's ok, I've tuned my AdamW for weeks, I can just reuse the same hyperparameters to compare!"*  and we wish so much this would be true. But unfortunately, for each optimizer, you need to do proper hyperparameter search (1D? 2D? 3D?), which makes optimizer research hard and costly.

<Sidenote>

Often, the baseline is not tuned very well, so new optimizers are compared against a weak AdamW settings. A recent study [@wen2025fantasticpretrainingoptimizers] shows how much that alone skews reported gains.
</Sidenote>

So let's start with the classic and the foundation of Durk Kingma's scary [Google scholar ](https://scholar.google.com/citations?user=yyIoQu4AAAAJ&hl=en)domination: AdamW.

 **AdamW** 

Adam (Adaptive Momentum Estimation) is a first order optimization technique. It means that in addition to looking at the gradients alone, we also consider how much the weights changed in the previous steps. This makes the learning rate for each parameter adapt based on the momentum. 

The careful reader might wonder: hey there, aren't you missing a W? Indeed! The reason we specifically add the W (=weight decay) is the following. In standard SGD we can simply add a  $\lambda \theta^2$  (where  $\theta$  are the weights) to the loss to apply L2 regularization. However, if we do the same with Adam, the adaptive learning rate will also affect the L2 regularization. This means the regularization strength becomes dependent on gradient magnitudes, weakening its effect. This is not what we want and that's why AdamW applies it decoupled from the main optimization loop to fix this.

Interestingly, across the last few years the AdamW hyperparameters have barely moved:

- β₁ = 0.9, β₂ = 0.95
- grad norm clipping = 1.0
- weight decay = 0.1 (Llama-3-405B drops this to 0.01)

The same triplet is almost reused from Llama 1,2,3 to DeepSeek-V1,2,3 671B, no change. Was Durk Kingma right all along or can we do better? 

 **Muon in one line** 

Adam is a first-order methods, as it is only uses the gradients. Muon is a second-order optimizer that acts on the  *matrix*  view of a parameter tensor.

$$

\begin{aligned}
G_t &= \nabla_{\theta}\mathcal{L}_t(\theta_{t-1}) \\
B_t &= \mu\, B_{t-1} + G_t \\
O_t &= \mathrm{NewtonSchulz5}(B_t) \ \approx\ U V^\top \quad \text{if } B_t = U\Sigma V^\top \text{ (SVD)} \\
\theta_t &= \theta_{t-1} - \eta\, O_t
\end{aligned}
$$

Looking at these equations you might wonder why is this a second order method, I only see gradients and no higher oder terms. The second order optimization actually happens inside the Newton Schulz step, but we won't go into further details here. There are already high-quality blogs that explain Muon in depth, so here we'll just list the three key ideas of Muon:

<Sidenote>

To learn more about Muon, we recommend checking out this [blog](https://kellerjordan.github.io/posts/muon/) by Keller Jordan,  this [one](https://jeremybernste.in/writing/deriving-muon) by Jeremy Bernstein and this [video](https://www.youtube.com/watch?v=bO5nvE289ec) by Jia-Bin Huang which is a great starting point.
</Sidenote>

1.  **Matrix-wise geometry vs. parameter-wise updates:** AdamW preconditions  *per parameter*  (diagonal second moment). Muon treats each weight  **matrix**  as a single object and updates along  $G=UV^{\top}$ , which captures row/column subspace structure.
1.  **Isotropic steps via orthogonalization:** Decomposing  $G=U\Sigma V^{\top}$   with singular value decomposition (SVD) separates magnitude ( $\Sigma$ ) from directions (the left/right subspaces  $U,V$  ). Replacing  $G$  by  $UV^{\top}$  discards singular values and makes the step  *isotropic*  in the active subspaces. It's a bit counterintuitive at first—since throwing away  $\Sigma$  looks like losing information—but it reduces axis-aligned bias and encourages exploration of directions that would otherwise be suppressed by very small singular values. There's still an open question on whether this kind of exploration bakes different capabilities into the model that aren't obvious if you only look at the loss.
1.  **Empirical tolerance to larger batch sizes:** In practice, Muon often tolerates higher batch sizes. We'll talk about this more in depth in the batch size section, but this might be a key point of Muon adoption!

For years, the community mostly settled on AdamW and the optimizer recipes of frontier labs are often kept secret (Qwen doesn't talk about theirs, for instance), but recently Muon has seen uptake in high-profile releases (e.g., Kimi K2, GLM-4.5). Hopefully we'll see more open and robust recipes to use 

There is a wild zoo of optimizers and the only thing researchers are more creative at than combining all possible momentums and derivates is coming up with names for them: Shampoo, SOAP, PSGD, CASPR, DION, Sophia, Lion… even AdamW has its own variants like NAdamW, StableAdamW, etc. Diving into all these optimizers would be worth its own blog, but we'll keep that for another time. In the meantime, we recommend this amazing paper by the stanford/marin team [@wen2025fantasticpretrainingoptimizers] who benchmarked many different optimizer to show how important hyperparemeter tuning is when doing comparisons.

Hand in hand with almost every optimizer is the question on how strongly we should update the weights determined by the learning rate which typically appears as a scalar value in the optimizer equations. Let's have a look how this seemingly simple topic still has many facets to it.

#### Learning Rate

The learning rate is one of the most important hyperparameters we'll have to set. At each training step, it controls how much we adjust our model weights based on the computed gradients. Choosing a learning rate too low and our training gets painfully slow and we can get trapped in a bad local minima. The loss curves will look flat, and we'll burn through our compute budget without making meaningful progress. On the other hand if we set the learning rate too high we cause the optimizer to take massive steps that overshoot optimal solutions and never converge or, the unimaginable happens, the loss diverges and the loss shoots to the moon.

But the best learning rate isn't even constant since the learning dynamics change during training. High learning rates work early when we're far from good solutions, but cause instability near convergence. This is where learning rate schedules come in: warmup from zero to avoid early chaos, then decay to settle into a good minimum. These patterns (warmup + cosine decay, for example) have been validated for neural networks training for many years.

<Note title="Warmup steps" emoji="💡" variant="info">

Most modern LLMs use a fixed number of warmup steps (for example 2000) regardless of model size and length of training, as shown in [Table 1](#llms-landscape-pretrain). We've found that for long trainings, increasing the number of warmup steps doens't have an impact on performance, but for very short trainings people usually use 1% to 5% of training steps.
</Note>

Let's look at the common schedules, then discuss how to pick the peak value.

 **Learning Rate Schedules: Beyond Cosine Decay** 

It has been known for years that changing the learning rate helps convergence [@smith2018superconvergencefasttrainingneural] and the cosine decay [@loshchilov2017sgdrstochasticgradientdescent] was the go-to schedule for training LLMs: start at a peak learning rate after warmup, then smoothly decrease following a cosine curve. It's simple and works well. But its main disadvantage is inflexibility; we need to know our total training steps upfront, as the cosine cycle length must match your total training duration. This becomes a problem in common scenarios: your model hasn't plateaued yet or you get access to more compute and want to train longer, or you're running scaling laws and need to train the same model on different token counts.  Cosine decay forces you to restart from scratch.

<Sidenote>

Interestingly, this inflexibility skewed early scaling laws research [@kaplan2020scalinglawsneurallanguage], because they used a fixed cosine schedule length when training models on varying token counts, underestimating the impact of data size. The Chinchilla study [@hoffmann2022trainingcomputeoptimallargelanguage] corrected this and matched the schedule length to each model's actual training duration.
</Sidenote>

Many teams now use schedules where you don't need to start decaying immediately after warmup. This is the case for  **Warmup-Stable-Decay (** WSD) [@hu2024minicpmunveilingpotentialsmall] and  **Multi-Step**  [@deepseekai2024deepseekllmscalingopensource] variants shown in the plot below. You maintain a constant high learning rate for most of training, and either sharply decay in the final phase (typically the last 10-20% of tokens) for WSD, or do discrete drops (steps) to decrease the learning rate, for example after 80% of training and then after 90% as it was done in [DeepSeek LLM](https://arxiv.org/abs/2401.02954)'s Multi-Step schedule. 

<HtmlEmbed 
  src="embeds/d3-line-chart.html" 
  config={{ 
    dataUrl: "./data/learning_rate_schedules.csv",
    xColumn: "steps",
    yColumn: "learning_rate",
    runColumn: "run_name",
    xScaleType: "linear",
    yScaleType: "linear",
    xAxisLabel: "Step",
    yAxisLabel: "Learning Rate",
    title: "Learning Rate Schedules"
  }} 
/>



These schedules offer practical advantages over cosine decay. We can extend training mid-run without restarting, whether we want to train longer than initially planned, are decay early to get a more measure of training progress and we can run scaling law experiments across different token counts with one main training run. Moreover, studies show that both WSD and Multi-Step match cosine decay [@wsdhagele; @deepseekai2024deepseekllmscalingopensource] while being more practical for real-world training scenarios. 

<Sidenote>

recently GLM 4.5 mentions that WSD perform worse on general benchmarks (SimpleQA, MMLU), but they don't provide any results.
</Sidenote>

But you probably noticed that these schedules introduce new hyperparameters compared to cosine: How long should the decay phase last in WSD? And how long should each step be in the Multi-Step variant?

- For WSD:   The required cooldown duration to match cosine performance decreases with longer training runs, and it is recommended to allocate 10-20% of total tokens to the decay phase [@wsdhagele]. We will confirm this setup matches cosine in our ablations below.
- For Multi-Step: DeepSeek LLM's ablations found that while their baseline 80/10/10 split (stable until 80%, first step from 80-90%, second step from 90-100%) matches cosine, tweaking these proportions can even outperform it, for instance when using 70/15/15 and 60/20/20 splits.

But we can get even more creative with these schedules. Let's look at the schedules used in each family of the DeepSeek models:

<HtmlEmbed 
  src="embeds/d3-line-chart.html" 
  config={{ 
    dataUrl: "./data/deepseek_schedules.csv",
    xColumn: "steps",
    yColumn: "learning_rate",
    runColumn: "run_name",
    xScaleType: "linear",
    yScaleType: "linear",
    xAxisLabel: "Step",
    yAxisLabel: "Learning Rate (×10⁻⁴)",
    title: "DeepSeek Multi-Step Learning Rate Schedules Comparison"
  }} 
/>



DeepSeek LLM used the baseline Multi-Step schedule (80/10/10). [DeepSeek V2](https://arxiv.org/abs/2405.04434) adjusted the proportions to 60/30/10, giving more time to the first decay step. [DeepSeek V3](https://arxiv.org/abs/2412.19437) took the most creative approach:  instead of maintaining a constant learning rate followed by two sharp steps, they transition from the constant phase with a cosine decay (from 67% to 97% of training), then apply a brief constant phase before the final sharp step.

<Note title="DeepSeek Schedules Change" variant="info">

DeepSeek-V2 and V3's technical reports don't include ablations on these schedule changes. For your setup, start with simple WSD or Multi-Step schedules, then consider tuning the parameters through ablations.
</Note>

Let's stop our survey of exotic learning rate schedules here and burn some GPU hours to determine what works in practice!

 **Ablation - WSD matches Cosine** 

Now it's time for an ablation! Let's test whether WSD actually matches cosine's performance in practice. We won't show Multi-Step ablations here but we recommend DeepSeek LLM's ablations where they showed that Multi-Step matches cosine with different phase splits. In this section, we'll compare cosine decay against WSD with two decay windows: 10% and 20%.

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/wsd_loss.csv",
xDomain: [0, 46e9],
yDomain: [2.1, 2.7],
smoothing: true,
title: "WSD Loss"
}}
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/wsd_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: false,
smoothingWindow: 5
}}
/>
</Wide>



The evaluation results show similar final performance across all three configurations. Looking at the loss and evaluation curves (specifically HellaSwag), we see an interesting pattern: cosine achieves better loss and evaluation scores during the stable phase (before WSD's decay begins). However, once WSD enters its decay phase, there's an almost linear improvement in both loss and downstream metrics allowing WSD to catch up to cosine by the end of training.

This confirms that WSD's 10-20% decay window is sufficient to match cosine's final performance while maintaining the flexibility to extend training mid-run. We opted for WSD with 10% decay for SmolLM3.

<Note title="Comparing models trained with different schedulers mid-run" emoji="⚠️" variant="danger">

If you're comparing intermediate checkpoints between cosine and WSD during the stable phase, make sure to apply a decay to the WSD checkpoint for a fair comparison.
</Note>

Now that we have a good overview of popular learning rate schedules, the next question is: what should the peak learning rate actually be?

 **Finding The Optimal Learning Rate** 

How do we pick the right learning rates for our specific learning rate scheduler and training setups? 

We could run learning rate sweeps on short ablations like we did for architecture choices. But optimal learning rate depends on training duration: a learning rate that converges fastest in a short ablation might not be the best one for the full run. And we can't afford to run expensive multi-week trainings multiple times just to test different learning rates. 

Let's first look at simple sweeps we can quickly run that help us rule out learning rates that are much too high or low and then we'll discuss scaling laws for hyperparameters.

 **Ablation - LR sweeps** 

To illustrate the impact of different learning rates, let's look at a sweep on our 1B ablation model trained on 45B tokens. We train the same model, under the same setup with 4 different learning rates: 1e-4, 5e-4, 5e-3, 5e-2. The results clearly show the dangers at both extremes:

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/lr_loss.csv",
xDomain: [0, 45e9],
yDomain: [2.1, 10],
smoothing: true,
title: "Learning Rate Loss"
}}
/>

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/lr_evals.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: true,
smoothingWindow: 15
}}
/>
</Wide>



LR 5e-2 diverges almost immediately, the loss spikes early and never recovers, making the model unusable. LR 1e-4 is too conservative, while it trains stably, it converges much more slowly than the other learning rates. The middle ground of 5e-4 and 5e-3 show better convergence and comparable performance. But running sweeps for every model size gets expensive quickly, and more importantly, it doesn't account for the planned number of training tokens as we previously stated. This is where scaling laws become invaluable.

For SmolLM3, we trained 3B models on 100B tokens with AdamW using the WSD schedule, comparing several learning rates. We found that 2e-4 converged much faster than 1e-4 in both loss and downstream performance, while 3e-4 was only slightly better than 2e-4. The marginal gains from 3e-4 came with increased risk of instability during long training runs, so we chose 2e-4 as our sweet spot.

These sweeps help us rule out learning rates that are clearly too high (divergence) or too low (slow convergence), but running sweeps for every model size gets expensive quickly, and more importantly, it doesn't account for the planned number of training tokens as we previously stated. This is where scaling laws become invaluable.

But before we dive into scaling laws for  hyperparameters, let's discuss the other critical hyperparameter that interacts with learning rate: batch size.

#### Batch size

The batch size is the number of samples processed before updating model weights. It directly impacts both training efficiency and final model performance. Increasing the batch size improves throughput if your hardware and training stack scale well across devices. But beyond a certain point, larger batches start to hurt data efficiency: the model needs more total tokens to reach the same loss. The breakpoint where this happens is known as the  **critical batch size** [@mccandlish2018empiricalmodellargebatchtraining].

<Sidenote>

Throughput is the number of tokens processed per second during training.
</Sidenote>

-  **Increasing the batch size while staying below critical:**  after increasing the batch size and retuning the learning rate, you reach the same loss with the same number of tokens as the smaller batch size run, no data is wasted.
-  **Increasing the batch size while staying above critical:** larger batches start to sacrifice data efficiency; reaching the same loss now requires more total tokens (and thus more money), even if wall-clock time drops because more chips are busy.

Let's try to give some intuition about why we need to retune the learning rate, and how to compute an estimation of what the critical batch size should be. 

When batch size grows, each mini-batch gradient is a better estimate of the true gradient, so you can safely take a larger step (i.e., increase the learning rate) and reach a target loss in fewer updates. The question is how to scale it.

Averaging over B samples

- Batch gradient:  $\tilde{g}_{B} \;=\; \frac{1}{B}\sum_{i=1}^{B} \tilde{g}^{(i)}$ 
- Mean stays the same:  $\mathbb{E}\!\left[\tilde{g}_{B}\right] \;=\; g$ 
- But covariance shrinks:  $\mathrm{Cov}\!\left(\tilde{g}_{B}\right) \;=\; \frac{\Sigma}{B}$ 

The SGD parameter update is:

-  $\Delta w \;=\; -\,\eta \,\tilde{g}_{B}$ 

The variance of this update is proportional to:

-  $\mathrm{Var}(\Delta w) \;\propto\; \eta^{2}\,\frac{\Sigma}{B}$ 

so to keep the update variance roughly constant, if you scale the batch size by k, you want to scale the learning rate by  $\sqrt k$ . So let's say you have you have computed your optimal batch size and learning rate and you've find that increasing to the critical batch size is possible and increasing the throughput, you'll need to adapt the optimal learning rate as well.

$$
B_{\text{critical}} \;\rightarrow\; kB_{\text{optimal}} \quad\Rightarrow\quad \eta_{\text{critical}} \;\rightarrow\; \sqrt{k}\eta_{\text{optimal}}
$$

<Sidenote>

See the (amazing) Jianlin Su's series for more math on this: [https://kexue.fm/archives/11260](https://kexue.fm/archives/11260)
</Sidenote>

A useful rule of thumb for optimizers like AdamW or Muon is  ***square-root***   ***LR scaling***  as batch size grows, but this also depends on the optimizer. For instance using AdamW there are interactions with  `beta1` / `beta2`  that can introduce very different behavior. A pragmatic alternative is to branch training for a brief window: keep one run at the original batch, start a second with the larger batch and a rescaled LR, and only adopt the larger batch if the two loss curves align after the rescale  [@merrill2025criticalbatchsizerevisited]. In the paper, they re-warm up the learning rate and reset the optimizer state when switching the batch size. They also set a tolerance and a time window to decide whether the losses "match", both knobs are chosen empirically. They found that the  $B_{simple}$  estimate - which is also noisy - is underestimating the "actual" critical batch size. This gives you a quick, low-risk check that the new batch/LR pair preserves training dynamics.

The critical batch size isn't fixed, it grows as training progresses. Early in training, the model is making big gradient step, so  $\lVert g\rVert^2$  is big which means  $B_\text{simple}$  is small, hence the model have a smaller critical batch size. Later, as the model updates stabilizes and larger batches become more effective. This is why some large-scale trainings don't keep the batch size constant and use what we can batch size warmup. For example, DeepSeek-V3 begins with a 12.6 M batch for the first ~469 B tokens, then increases to 62.9M for the remainder of training. A batch-size warmup schedule like this serves the same purpose as a learning-rate warmup: it keeps the model on the efficient frontier as the gradient noise scale grows, maintaining stable and efficient optimization throughout.

Another interesting approach is treating the loss as a proxy for the critical batch size. Minimax01 used this and in the last stage they trained with a 128M batch size! This is a bit different because they don't increase the learning rate, so their batch-size schedule acts like a learning-rate decay schedule.

<Note title="Tuning batch size and learning rate" variant="info">

In practice, here's how you can choose the batch size and learning rate:
- You first pick the batch size and learning rate you consider optimal, either from scaling laws (see later!) or from literature. 
- Then, you can tune the batch size to see if you can improve the training throughput. 

The key insight is that there's often a range between your starting batch size 
and the critical batch size where you can increase it to improve hardware 
utilization without sacrificing data efficiency, but you must retune the learning 
rate accordingly. 
If the throughput gain isn't significant, or if testing a larger batch size 
(with rescaled learning rate) shows worse data efficiency, stick with the initial values.
</Note>

As mentioned in the note above, one way to pick your starting points for the batch size and learning rate is through scaling laws. Let's see how these scaling laws work and how they predict both hyperparameters as a function of your compute budget.

####  **Scaling laws for hyperparameters** 

The optimal learning rate and batch size aren't just about model architecture and size, they also depends on our compute budget, which combines both the number of model parameters and the number of training tokens. In practice, both of these factors interact to determine how aggressive or conservative our updates should be. This is where scaling laws come in.

Scaling laws establish empirical relationships describing how model performance evolves as we increase training scale, whether that's through larger models or more training data (see the "Scaling laws" section at the end of this chapter for the full history). But scaling laws can also help us predict how to adjust key hyperparameters like the learning rate and batch size as we scale up training, as it was done in recent work by [DeepSeek](https://arxiv.org/abs/2401.02954) and [Qwen2.5](https://arxiv.org/abs/2412.15115). This gives us principled defaults rather than relying entirely on hyperparameter sweeps.

To apply scaling laws in this context, we need a way to quantify training scale. The standard metric is the compute budget, denoted C and measured in FLOPs, which can be approximated as:

 $C\approx 6×N×D$  

N is the number of model parameters (e.g., 1B = 1e9), D is the number of training tokens. This is often measured in FLOPs (floating-point operations), a hardware-agnostic way of quantifying how much actual computation is being done. But if FLOPs feel too abstract, just think of it this way: training a 1B parameter model on 100B tokens consumes about 2× fewer FLOPs than training a 2B model on 100B tokens, or a 1B model on 200B tokens.

The constant 6 comes from empirical estimates of how many floating-point operations are required to train a Transformer, roughly 6 FLOPs per parameter per token.

<Sidenote>

If you want a more precise measure taking into account MoE layers and Hybrid layers you can checkout the [num_floating_point_operations](https://github.com/NVIDIA/Megatron-LM/blob/f34fa11af6f5dc65f5342f2a785c3227446cebfd/megatron/training/training.py#L158) function in Megatron-LM.
</Sidenote>

Now, how does this relate to learning rate? We can derive scaling laws that predict optimal learning rates and batch sizes as functions of total compute budget (C). They help answer questions like:

- How should the learning rate change as I scale from 1B to 7B parameters?
- If I double my training data, should I adjust the learning rate?

Let's see how this works be walking through the approach DeepSeek used: First, we choose our learning rate schedule, ideally WSD for its flexibility. Then, we train models across a range of compute budgets (e.g., 1e17, 5e17, 1e18, 5e18, 1e19, 2e19 FLOPs) with different combinations of batch sizes and learning rates. In simpler terms: we train different model sizes for different numbers of tokens, testing different hyperparameter settings. This is where the WSD schedule shines, we can extend the same training run to different token counts without restarting. 

For each setup, we perform sweeps over learning rate and batch size and identify the configurations that result in near-optimal performance, typically defined as being within a small margin (e.g., 0.25%) of the best validation loss (computed on an independent validation set, with a similar distribution to the training set). Each near-optimal configuration gives us a data point — a tuple of (compute budget C, optimal learning rate η) or (C, optimal batch size B). When plotted on a log-log scale, these relationships typically follow power-law behavior, appearing as approximately straight lines (as shown in the figure above). By fitting these data points, we can extract scaling laws that describe how optimal hyperparameters evolve with compute.

An important finding from this process is that for a fixed model size and compute budget, performance remains stable across a wide range of hyperparameters. This means there's a broad sweet spot rather than a narrow optimum. We don't need to find the perfect value, just a value that's close enough, which makes the whole process much more practical.

Here you can see the results of the scaling laws DeepSeek derived, where each dot represents a near optimal setting:



<Image src={image_2471384e_bcac_8059_84a4_d4ce5ae3847c} alt="Image" />

The core intuition behind these results is that as training becomes larger and longer, we want  **more stable updates**  (hence, smaller learning rates) and  **more efficient gradient estimation**  (hence, larger batch sizes).

These scaling laws give us starting points for the learning rate and batch size. But the objective is not "optimal samples per gradient" but "lower loss reachable within our time and number of GPUd constraint" while still extracting the full signal from every token. 

In practice, you may be able to increase the batch size beyond the predicted optimal batch size to significantly improve throughput without meaningfully hurting data efficiency, up to the critical batch size we discussed earlier.



#### SmolLM3

So what did we end up using for SmolLM3? At the time of ablations before launching SmolLM3, we compared AdamW, AdEMAMix, and Muon on a 1B model trained on 100B tokens. Muon could outperform AdamW when properly tuned but was sensitive to learning rate and prone to divergence. AdeMaMix was less sensitive and achieved similar loss to Muon. AdamW was the most stable but reached a higher final loss than the tuned alternatives.

However, when we scaled up to 3B, we encountered more frequent divergence with Muon and AdeMaMix. This may have been due to a parallelism bug we discovered after finishing the ablations (see The Training Marathon chapter), though we haven't confirmed this. We decided to use AdamW (beta1: 0.9, beta2: 0.95) with weight decay 0.1 and gradient clipping 1. After all a very vanilla setting.

For the learning rate schedule, we chose WSD. We had used it successfully in SmolLM2, and it proved to be one of our best decisions for ease of use and flexibility regarding total training duration plus the ability to run mid-training decay experiments. We ran learning rate sweeps and settled on 2e-4. For the global batch size, we tested values from 2M to 4M tokens but found minimal impact on the loss or downstream performance, so we chose 2.36M tokens - the size that gave us the best throughput.

#### Rules of engagement

<Quote>

<b>TL;DR:</b> Balance exploration and execution, done is better than perfect.
</Quote>

We've talked a lot about the "what" (optimizer, learning rate, batch size) but just as important is the  **how** . How do we decide what's worth experimenting with? How do we structure our time? When do we stop exploring and just train? 

 **Allocate your time wisely between exploration and execution.**  Spending weeks perfecting a minor improvement from a new method is less valuable than investing that same compute in better data curation or more thorough architecture ablations. From our experience, and though it might disappoint architecture enthusiasts, the biggest performance gains usually come from data curation.

 **When in doubt, choose flexibility and stability over peak performance.**  If two methods perform equally well, pick the one that offers more flexibility or that has better implementation maturity and stability. A learning rate schedule like WSD that lets us extend training or run mid-training experiments is more valuable than a rigid schedule that might converge slightly better.

 **Know when to stop optimizing and start training.**  There's always one more hyperparameter to tune or one more optimizer to try. Set a deadline for exploration and stick to it - the model we actually finish training will always beat the perfect model we never start.

<Reference caption="One more ablation won't hurt (Spoiler: It did). Credits to <a href='[https://x.com/sea_snell/status/1905163154596012238](https://x.com/sea_snell/status/1905163154596012238)'>

sea_snell</a>">

<Image src={Capture_decran_2025_10_27_a_22_10_05_2991384e_bcac_802a_a6e6_dab0f9f410ec} alt="Image" />
</Reference>

Perfect is the enemy of good, especially when we're working with finite compute budgets and deadlines.

### Scaling laws: how many parameters, how much data?

In the early days of deep learning, before language models (and the clusters they were trained on) were "large", training runs were often not heavily constrained by compute. When training a model, you'd just pick the largest model and batch size that fit on your hardware and then train until the model started overfitting or you ran out of data. However, even in these early days there was a sense that scale was helpful — for example, [Hestness et al.](https://arxiv.org/abs/1712.00409) provided a comprehensive set of results in 2017 showing that training larger models for longer produced predictable gains. 

In the era of large language models, we are  *always*  compute-constrained. Why? These early notions of scalability were formalized by [Kaplan et al.'s work on Scaling Laws for Neural Language Models](https://arxiv.org/abs/2001.08361), where it was shown that language model performance is remarkably predictable across many orders of magnitude of scale. This set off an explosion in the size and training duration of language models, because it provided a way to  *accurately predict* how much performance would improve from increasing scale. Consequently, the race to build better language models became a race to train larger models on larger amounts of data with ever-growing compute budgets, and the development of language models quickly became compute-constrained.

When faced with compute constraints, the most important question is whether to train a larger model or to train on more data. Surprisingly, Kaplan et al.'s scaling laws suggested that it was advantageous to allocate much more compute towards model scale than previous best practices — motivating, for example, training the gargantuan (175B parameters) GPT-3 model on a relatively modest token budget (300B tokens). On reexamination, [Hoffman et al.](https://arxiv.org/abs/2203.15556) found a methodological issue with Kaplan et al.'s approach, ultimately re-deriving scaling laws that suggested allocating much more compute to training duration which indicated, for example, that compute-optimal training of the 175B-parameter GPT-3 should have consumed 3.7T tokens!

This shifted the field from "make models bigger" to "train them longer and better." However, most modern trainings still don't strictly follow the Chinchilla laws, because they have an shortcoming: They aim to predict the model size and  *training*  duration that achieves the best performance given a certain compute budget, but they fail to account for the fact that larger models are more expensive  *after*  training. Put another way, we might actually prefer to use a given compute budget train a smaller model for longer — even if this isn't "compute-optimal" — because this will make inference costs cheaper ([Sardana et al.](https://arxiv.org/abs/2401.00448), [de Vries](https://www.harmdevries.com/post/model-size-vs-compute-overhead/)). This could be the case if we expect that a model will be see a lot of inference usage (for example, because it's being released openly 🤗). Recently, this practice of "overtraining" models beyond the training duration suggested by scaling laws has become standard practice, and is the approach we took when developing SmolLM3.

While scaling laws provide a suggestion for the model size and training duration given a particular compute budget, choosing to overtrain means you have to decide these factors yourself. For SmolLM3, we started by picking a target model size of 3 billion parameters. Based on recent models of a similar scale like Qwen3 4B, Gemma 3 4B, and Llama 3.2 3B, we considered 3B to be large enough to have meaningful capabilities (such as reasoning and tool calling), but small enough to enable super fast inference and efficient local usage. To pick a training duration, we first noted that recent models have been  *extremely*  overtrained — for example, the aforementioned Qwen3 series is claimed to have been trained for 36T tokens! As a result, training duration is often dictated by the amount of compute available. We secured 384 H100s roughly a month, which provided a budget for training on 11 trillion tokens (assuming an MFU of ~30%).

<Note title="Scaling laws" variant="info">

Despite these deviations, scaling laws remain practically valuable.
They provide baselines for experimental design,
people often use Chinchilla-optimal setups to get signal on ablations,
and they help predict whether a model size can reach a target performance.
As de Vries notes in this <a href="[https://www.harmdevries.com/post/model-size-vs-compute-overhead/](https://www.harmdevries.com/post/model-size-vs-compute-overhead/)" target="_blank">blog</a>,
by scaling down model size you can hit a critical model size: the minimal capacity required to reach a given loss,
below which you start getting diminishing return.
</Note>

Now that we're settled on our model architecture, training setup, model size, and training duration, we need to prepare two critical components: the data mixture that will teach our model, and the infrastructure that will train it reliably. With SmolLM3's architecture set at 3B parameters, we needed to curate a data mixture that would deliver strong multilingual, math and code performance, and set up infrastructure robust enough for 11 trillion tokens of training. Getting these fundamentals right is essential, even the best architectural choices won't save us from poor data curation or unstable training systems.

## The art of data curation

Picture this: you've spent weeks perfecting your architecture, tuning hyperparameters, and setting up the most robust training infrastructure. Your model converges beautifully, and then... it can't write coherent code, struggles with basic math, and maybe even switches languages mid-sentence. What went wrong?
The answer usually lies in the data. While we obsess over fancy architectural innovations and hyperparameter sweeps, data curation often determines whether our model becomes genuinely useful or just another expensive experiment. It's the difference between training on random web crawls versus carefully curated, high-quality datasets that actually teach the skills we want our model to learn.

If model architecture defines  *how*  your model learns, then data defines  *what*  it learns, and no amount of compute or optimizer tuning can compensate for training on the wrong content. Moreover, getting the training data right isn't just about having good datasets. It's about assembling the right  **mixture** : balancing conflicting objectives (like strong English vs. robust multilinguality) and tuning data proportions to align with our performance goals. This process is less about finding a universal best mix and more about asking the right questions and devising concrete plans to answer them:

- What do we want our model to be good at?
- Which datasets are best for each domain and how do we mix them?
- Do we have enough high-quality data for our target training scale?

This section is about navigating these questions using a mix of principled methods, ablation experiments, and a little bit of alchemy, to turn a pile of great datasets into a great training mixture.

### What's a good data mixture and why it matters most

We expect a lot from our language models, they should be able to help us write code, give us advice, answer questions about pretty much anything, complete tasks using tools, and more. Plentiful pre-training data sources like the web don't cover the full range of knowledge and capabilities needed for these tasks. As a result, recent models additionally rely on more specialized pre-training datasets that target specific domains like math and coding. We have done a lot of past work on curating datasets, but for SmolLM3 we primarily made use of preexisting datasets. To learn more about dataset curation, check out our reports on building [FineWeb and FineWeb-Edu](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1), [FineWeb2](https://arxiv.org/abs/2506.20920), [Stack-Edu, and FineMath](https://arxiv.org/abs/2502.02737).

####  **The unintuitive nature of data mixtures** 

If you're new to training language models, finding a good data mixture might seem straightforward: identify your target capabilities, gather high-quality datasets for each domain, and combine them. The reality is more complex, since some domains might compete with each other for your training budget. When focusing on some particular capability like coding, it can be tempting to upweight task-relevant data like source code. However, upweighting one source implicitly downweights all of the other sources, which can harm the language model's capabilities in other settings. Training on a collection of different sources therefore involves striking some kind of balance between downstream capabilities.

Additionally, across all of these sources and domains, there's often a subset of "high-quality" data that is especially helpful at improving the language model's capabilities. Why not just throw out all the lower quality data and train on the highest quality data only? For SmolLM3's large training budget of 11T tokens, doing such extreme filtering would result in repeating data many times. Prior work has shown that this kind of repetition can be harmful [@muennighoff2025scalingdataconstrainedlanguagemodels], so we should ideally be able to make use of higher and lower quality while still maximizing model performance.

To balance data across sources and make use of high-quality data, we need to carefully design the  *mixture* : the relative proportion of training documents from each source. Since a language model's performance on some particular task or domain depends heavily on the amount of data it saw that is relevant to that task, tuning the mixing weights provides a direct way of balancing the model's capabilities across domains. Because these trade-offs are model-dependent and difficult to predict, ablations are essential.

But the mixture doesn't have to stay fixed throughout training. By adjusting the mixture as training progresses, what we call multi-stage training **** or curriculum, we can make better use of both high-quality and lower-quality data.

####  **The evolution of training curricula** 

In the early days of large language model training, the standard approach was to fix a single data mixture for the entire training run. Models like GPT3 and early versions of Llama trained on a static mixture from start to finish. More recently, the field has shifted toward  **multi-stage training**  [@smollm2] where the data mixture changes over the course of training. The main motivation is that a language model's final behavior is strongly influenced by data seen toward the end of training [@chen2025scalinglawspredictingdownstream]. This insight enables a practical strategy: upweighting more plentiful sources early in training and mixing in smaller, higher quality sources towards the end.

A common question is: how do you decide when to change the mixture? While there's no universal rule, but we typically follow these principles:

1.  **Performance-driven interventions** : Monitor evaluation metrics on key benchmarks and adapt dataset mixtures to address specific capability bottlenecks. For example, if math performance plateaus while other capabilities continue improving, that's a signal to introduce higher-quality math data.
1.  **Reserve high-quality data for late stages** : small high quality math and code datasets are most impactful when introduced during the annealing phase (final stage with learning rate decay).

Now that we've established why mixtures matter and how curricula work, let's discuss how to tune both.

### Ablation setup: how to systematically test data recipes

When testing data mixtures, our approach is similar to how we run architecture ablations, with one difference: we try to run them at the target model scale. Small and large models have different capacities, for example a very small model might struggle to handle many languages, while a larger one can absorb them without sacrificing performance elsewhere. Therefore running data ablations at too small a scale risks drawing the wrong conclusions about the optimal mix.

For SmolLM3, we ran our main data ablations directly on the 3B model, using shorter training runs of 50B and 100B tokens. We also used another type of ablation setup: ******  **annealing experiments** . Instead of training from scratch with different mixtures, we took an intermediate checkpoint from the main run (for example at 7T tokens) and continued training with modified data compositions. This approach, allows us to test data mixture changes for doing multi-stage training (i.e changing the training mixture mid-training), and was used in recent work such as SmolLM2, Llama3 and Olmo2. For evaluation, we expanded our benchmark suite to include multilingual tasks alongside our standard English evaluations, ensuring we could properly assess the trade-offs between different language ratios.

<Wide>

<HtmlEmbed src="embeds/ablation-study.html"/>
</Wide>



Recent work has proposed automated approaches for finding optimal data proportions, including:

-  **DoReMi**  [@xie2023doremioptimizingdatamixtures]: Uses a small proxy model to learn domain weights that minimize validation loss
-  **Rho Loss**  [@mindermann2022prioritizedtrainingpointslearnable]: Selects individual training points based on a holdout loss, prioritizing samples that are learnable, task-relevant, and not yet learned by the model
-  **RegMix**  [@liu2025regmixdatamixtureregression]: Determines optimal data mixture proportions through regularized regression that balances performance across multiple evaluation objectives and data domains

We experimented with DoReMi and Rho Loss in past projects, but found they tend to converge toward distributions that roughly mirror the natural distribution of dataset sizes, essentially suggesting to use more of what we have more of. While theoretically appealing, they didn't outperform careful manual ablations in our setting. Recent SOTA models still rely on manual mixture tuning through systematic ablations and annealing experiments, which is the approach we adopted for SmolLM3.

### SmolLM3: Curating the data mixture (web, multilingual, math, code)

For SmolLM3, we wanted a model that can handle English and multiple other languages, and excel in math and code. These domains — web text, multilingual content, code and math — are common in most LLMs, but the process we'll describe here applies equally if you're training for a low-resource language or a specific domain such as finance or healthcare. The method is the same: identify good candidate datasets, run ablations, and design a mixture that balances all the target domains.

We won't cover how to build high-quality datasets here, since we've already detailed that extensively in earlier work (FineWeb, FineWeb2, FineMath and Stack-Edu). Instead, this section focuses on how we  *combine*  those datasets into an effective pretraining mixture.

####  **Building on Proven Foundations** 

When it comes to pretraining data, the good news is that we rarely have to start from scratch. The open-source community has already built strong datasets for most common domains. Sometimes we will need to create something new — as we did with the Fine series (FineWeb, FineMath, etc.) — but more often, the challenge is in selecting and combining existing sources rather than reinventing them.

That was our situation with SmolLM3. SmolLM2 had already established a strong recipe at 1.7B parameters for English web data, and identified the best math and code datasets we had access to. Our goal was to scale that success to 3B while adding the certain capabilities: robust multilinguality, stronger math reasoning, and better code generation.

####  **English Web Data: The Foundation Layer** 

Web text forms the backbone of any general-purpose LLM, but quality matters as much as quantity.

From SmolLM3, we knew that FineWeb-Edu and DCLM were the strongest open English web datasets at the time of training. Together, they gave us 5.1T tokens of high-quality English web data. The question was: what's the optimal mixing ratio? FineWeb-Edu helps on educational and  STEM benchmarks, while DCLM improves commonsense reasoning.

Following the SmolLM2 methodology, we ran a sweep on our 3B model over 100B tokens, testing ratios of 20/80, 40/60, 50/50, 60/40, and 80/20 (FineWeb-Edu/DCLM).  Mixing them (around 60/40 or 50/50) gave the best trade-offs. We rerun the same ablations as the [SmolLM2 paper](https://arxiv.org/abs/2502.02737v1) on our 3B model trained on 100B tokens and found the same conclusion. 

Using 60/40 or 50/50  provided the best balance across benchmarks, matching our SmolLM2 findings. We used 50/50 ratio ratio for Stage 1.

We also added other datasets like [Pes2o](https://huggingface.co/datasets/allenai/dolmino-mix-1124/tree/main/data/pes2o), [Wikipedia & Wikibooks](https://huggingface.co/datasets/allenai/dolmino-mix-1124/tree/main/data/wiki) and [StackExchange](https://huggingface.co/datasets/HuggingFaceTB/stackexchange_2025_md), these datasets didn't have any impact on the performance but we included them to improve diversity.

####  **Multilingual Web Data** 

For multilingual capability, we targeted 5 other languages: French, Spanish, German, Italian, and Portuguese. We selected them from FineWeb2-HQ, which gave us 628B tokens total. We also included 10 other languages at smaller ratios, such as Chinese, Arabic, and Russian, not to target state of the art performance for them, but to allow people to easily do continual pretraining of SmolLM3 on them. We used FineWeb2 for the languages not supported in FineWeb2-HQ.

The key question was: how much of our web data should be non-English? We know that more data a model sees in a language or domain, the better it gets at that language or domain. The trade-off comes from our fixed compute budget: increasing data for one language means reducing data for the other languages including English.

Through ablations on the 3B model, we found that 12% multilingual content in the web mix struck the right balance, improving multilingual performance without degrading English benchmarks. This fit SmolLM3's expected usage, where English would remain the primary language. It's also worth noting that with only 628B tokens of non-English data versus 5.1T English tokens, going much higher would require doing more repetition of the multilingual data.

####  **Code Data** 

Our code sources for Stage 1 are extracted from[ The Stack v2 and StarCoder2 ](https://arxiv.org/abs/2402.19173)training corpus: 

- [The Stack v2](https://huggingface.co/datasets/bigcode/the-stack-v2) (16 languages) as our basis, filtered as StarCoder2Data.
- StarCoder2 GitHub pull requests for real-world code review reasoning.
- Jupyter and [Kaggle notebooks](https://huggingface.co/datasets/HuggingFaceTB/issues-kaggle-notebooks) for executable, step-by-step workflows.
- [GitHub issues](https://huggingface.co/datasets/HuggingFaceTB/issues-kaggle-notebooks) and [StackExchange](https://huggingface.co/datasets/HuggingFaceTB/stackexchange_2025_md) threads for contextual discussions around code.

@aryabumi2024codecodeexploringimpact highlight that code improves language models' performance beyond coding, for example on natural language reasoning and world knowledge and recommend using 25% code in the training mixture. Motivated by this, we started our ablations with 25% code in the mixture.  However, we observed significant degradation on English benchmarks (HellaSwag, ARC-C, MMLU). Reducing to 10% code, we didn't see improvements on our English benchmark suite compared to 0% code, but we included it anyway since code was a very important capability to have in the model.

We delayed adding Stack-Edu — our educationally filtered subset of StarCoder2Data — until later stages, following the principle of staging high-quality data for maximum late-training impact.

####  **Math Data** 

Math followed a similar philosophy to code. Early on, we used the larger, more general sets FineMath3+ and InfiWebMath3+ and later we upsampled FineMath4+ and InfiWebMath4+, and introduced new high quality datasets:

- MegaMath [@zhou2025megamathpushinglimitsopen]
- Instruction and reasoning datasets like OpenMathInstruct [@toshniwal2024openmathinstruct118millionmath] and OpenMathReasoning [@moshkov2025aimo2winningsolutionbuilding]

We use 3% of math in Stage 1 equally split between FineMath3+ and InfiWebMath3+. With only 54B tokens available and an estimated 8T to 9T token Stage 1, using more than 3% math would require more than 5 epochs on the dataset. 

####  **Finding the right mixture for new stages** 

While we ran ablations from scratch to determine the stage 1 mixture, to test new datasets for new stages (in our case two new stages) we used annealing ablations: we took a checkpoint at around 7T tokens (late in stage 1) and ran a 50B token annealing experiments with the following setup:

-  **40% baseline mixture** : The exact stage 1 mixture we'd been training on
-  **60% new dataset** : The candidate dataset we wanted to evaluate

For example, to test whether MegaMath would improve our math performance, we ran 40% Stage 1 mixture (maintaining the 75/12/10/3 domain split) and 60% MegaMath.

The can find the composition of the 3 stages in the following section. 

With our data carefully curated and our mixture validated through ablations, we're ready to embark on the actual training journey. The chapter that follows is the story of SmolLM3's month-long training run: the preparation, the unexpected challenges, and the lessons learned along the way.

## The training marathon

You've made it this far, congrats! The real fun is about to begin.

At this point, we have everything in place: a validated architecture, a finalized data mixture, and tuned hyperparameters. The only thing left is setting up the infrastructure and hitting "train".

For SmolLM3, we trained on 384 H100 GPUs (48 nodes) for nearly a month, processing 11 trillion tokens. This section walks you through what actually happens during a long training run: the pre-flight checks, the inevitable surprises, and how we kept things stable. You'll see firsthand why both solid ablation practices and reliable infrastructure matter. We cover the technical infrastructure details of GPU hardware, storage systems, and optimizing throughputs in the [final chapter](#infrastructure---the-unsung-hero).

Our team has been through this many times: from StarCoder and StarCoder2, to SmolLM, SmolLM2, and now SmolLM3. Every single run is different. Even if you've trained a dozen models, each new run finds a fresh way to surprise you. This section is about stacking the odds in your favour so you're ready for those surprises.

### Pre-flight checklist: what to verify before hitting "train"

Before hitting "train", we go through a checklist to ensure everything works end-to-end:

 **Infrastructure readiness:**  

- If your cluster supports Slurm reservations, use them. For SmolLM3, we had a fixed 48-node reservation for the entire run. That meant no queueing delays, consistent throughput, and the ability to track node health over time. 
- Stress-test GPUs before launch (we use [GPU Fryer](https://github.com/huggingface/gpu-fryer) and [DCGM Diagnostics](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html)) to catch throttling or performance degradation. For SmolLM3, we found two GPUs throttling and replaced them before starting the run.
- Avoid storage bloat: our system uploads each checkpoint to S3, then deletes the local copy right after saving the next one, so we never store more than one on the fast local GPU SSDs.

 **Evaluation setup:** Evaluations are deceptively time-consuming. Even with everything implemented, running them manually, logging results, and making plots can eat up hours each time. So try to automate them completely, and ensure they are running and logging correctly before the run starts. For SmolLM3, every saved checkpoint automatically triggered an evaluation job on the cluster that got logged to Wandb and [Trackio](https://github.com/gradio-app/trackio).

 **Checkpoint & auto-resume system:**  Verify that checkpoints are saved correctly and that the training job can resume from the latest one without manual intervention. On Slurm, we use  `--requeue`  option so a failed job gets automatically relaunched, resuming from the most recent checkpoint.

 **Metrics logging:**  Confirm that you're logging all the metrics you care about: evaluation scores, throughput (tokens/sec), training loss, gradient norm, node health (GPU utilization, temperature, memory usage), and any custom debug metrics specific to your run.

 **Training configuration sanity check:** Double-check your training config, launch scripts, and Slurm submission commands.

<Note title="Infrastructure deep-dive" variant="info">

For detailed guidance on GPU testing, storage benchmarking, monitoring setup, and building resilient training systems, see the [Infrastructure chapter](#infrastructure---the-unsung-hero).
</Note>

### Scaling surprises 

After running extensive ablations for SmolLM3, we were ready for the full-scale run. Our 3B ablations on 100B tokens looked promising. The architectural changes compared to SmolLM2 (detailed in [Architecture Choices](#architecture-choices): GQA, NoPE, document masking, tokenizer) either improved or maintained performance, and we found a good data mixture that balances English, multilingual, code, and math performance (see [The art of data curation](#smollm3-curating-the-data-mixture-web-multilingual-math-code). We optimized our configuration for around 30% MFU on 384 GPUS (48 nodes).

<Sidenote>

We ran some ablations on 48 nodes to validate the throughput before launching the run. You can find more details about them in the Infrastructure chapter.
</Sidenote>

We were ready for the big one: 11T tokens. That's when reality started throwing curveballs.

####  **Mystery #1 – The vanishing throughput** 

Within hours of launch, throughput plummeted. It was a big jump with repeated sharp drops.

<Note title="Why throughput matters" emoji="📍" variant="info">

Throughput measures how many tokens per second our system processes during training. It directly impacts our training time, a 50% drop in throughput means our month-long run becomes a two-month run. In the Infrastructure chapter, we'll show how we optimized throughput for SmolLM3 before starting the run.
</Note>

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/throughput_weka_drops.csv",
xColumn: "step",
yColumn: "throughput",
runColumn: "run_name",
smoothing: false,
xAxisLabel: "Training Step",
yAxisLabel: "Tokens/sec/GPU",
title: "Throughput Weka Drops"
}}
/>



This didn't happen in any ablation run, so what changed? Three things:

1. Hardware state can change over time. GPUs that worked fine in ablations might fail and network connections might degrade under sustained load.
1. The size of the training datasets. We now used the full ~24 TB training dataset instead of the smaller subsets from ablations, though the data sources themselves were the same.
1. The number of training steps. We set the real step count for 11T tokens instead of the short 100B-token ablation horizon.

Everything else remained exactly the same as in the throughput ablations: number of nodes, dataloader configuration, model layout, and parallelism setup... 

Intuitively, neither dataset size nor step count should cause throughput drops, so we naturally suspected hardware issues first. We checked our node monitoring metrics, which showed that the big throughput jump correlated with spikes in disk read latency. That pointed us straight at our data storage.

<Note title="Storage options in our cluster" emoji="📍" variant="info">

Our cluster has three storage tiers for training data:

-  **FSx** : Network-attached storage which uses [Weka](https://www.weka.io/), a "keep-hot" caching model that stores frequently accessed files locally and evicts inactive "cold" files to S3 as capacity fills up.
-  **Scratch (Local NVMe RAID)** : Fast local storage on each node (8×3.5TB NVMe drives in RAID), which is faster than FSx but limited to local node access.
-  **S3** : Remote object storage for cold data and backups.

You can find more details in the [Infrastructure chapter](#infrastructure---the-unsung-hero).
</Note>

For SmolLM3's 24TB dataset, we initially stored the data in FSx (Weka). With 24TB of training data, on top of storage already used by several other teams, we were pushing Weka's storage to the limit. So it started evicting dataset shards mid-training, which meant we had to fetch them back, creating stalls, which explained the big throughput jump. Worse: there was no way to pin our dataset folders as hot for the full training.

 **Fix #1 – Changing data storage** 

We didn't find a way to pin our dataset folders as hot for the full training in Weka, so we tried to change the storage method. Streaming directly from S3 was slow, so we decided to store the data in each node in its local storage  `/scratch` .

This came with a catch: If a node died and was replaced, the new replacement GPUs had no data. Downloading 24TB from S3 with  `s5cmd`  took 3h. We cut that to 1h30 by copying from another healthy node using  `fpsync`  instead of going through S3. This was faster given all the nodes were in the same datacenter.

Still, 1h30 of downtime per node failure, and the need for manually copying the data to the new node immediately, was painful. The hack that finally made it bearable: reserve a spare node in our Slurm reservation with the dataset preloaded. If a node died, we swapped it instantly with the spare node, so zero recovery delay. While idle, the spare ran evals or dev jobs, so it wasn't wasted.

This fixed Mystery #1… or so we thought.

####  **Mystery #2 – The persisting throughput drops** 

Even after moving to scratch, the individual throughput drops kept happening although we didn't find any anomaly in the hardware monitoring metrics. The chart below compares the throughput we got, after fixing the storage issue in orange, to the throughput we were getting during the ablations in blue. As you can see, the drops became much sharper.

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/throughput_drops_comparison.csv",
xColumn: "step",
yColumn: "throughput",
runColumn: "run_name",
smoothing: false,
xAxisLabel: "Training Step",
yAxisLabel: "Tokens/sec/GPU",
title: "Throughput Drops Comparison"
}}
/>



Still suspecting hardware, we decided to test on fewer nodes. With 384 GPUs, there's a high chance something could be failing. Surprisingly, we could reproduce the exact same throughput drops on a single node, no matter which specific node we tested. This ruled out hardware issues.

Remember the three things that changed from our ablations? We had already addressed the data storage issue by moving to local node storage. Hardware was now eliminated. That left only one variable: the step count. We tested this by rolling back to smaller step counts (from 3M to 32k) and the thoughput drops became smaller! Larger step counts produced sharper, more frequent drops.

To test this, we ran identical configurations with only the training steps changed from 32k to 3.2M. You can see the [exact configs we used](https://huggingface.co/datasets/HuggingFaceTB/ablations-training-configs/tree/main/throughput_debugging):

```diff
## Short run (32k steps)
- "lr_decay_starting_step": 2560000
- "lr_decay_steps": 640000
- "train_steps": 3200000

## Long run (3.2M steps)
+ "lr_decay_starting_step": 26000
+ "lr_decay_steps": 6000
+ "train_steps": 32000

```
The results shown in the figure below were clear: shorter runs had small throughput drops, while longer step counts produced sharper, more frequent drops. So the issue was not the hardware, but a software bottleneck, likely in the dataloader! Given that most other training components process each batch identically regardless of step count.

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/throughput_debug_1node_full.csv",
xColumn: "step",
yColumn: "throughput",
runColumn: "run_name",
smoothing: false,
xAxisLabel: "Training Step",
yAxisLabel: "Tokens/sec/GPU",
title: "Throughput Debug 1 Node"
}}
/>



That's when we realized we'd never actually done large-scale pretraining with nanotron's dataloader. SmolLM2 had been trained with steady throughput using a Megatron-LM derived dataloader ([TokenizedBytes](https://github.com/huggingface/nanotron/blob/7bc9923285a03069ebffe994379a311aceaea546/src/nanotron/data/tokenized_bytes.py#L80)) through an internal wrapper around nanotron. For SmolLM3, we switched to nanotron's built-in dataloader ( `nanosets` ).

After deep diving into its implementation, we found that it was naively building one giant index that grew with each training step. For very large steps, this caused a higher shared memory which triggered throughput drops.

 **Fix #2 – Bring in TokenizedBytes dataloader** 

To confirm that the dataloader was indeed the culprit, we launched the same configuration with our internal SmolLM2 framework using  `TokenizedBytes`  dataloader. No drops. Even on 48 nodes using the same datasets.

Fastest path forward: copy this dataloader into nanotron. The drops were gone and the throughput back to target.

We were ready to relaunch… until the next curveball.

####  **Mystery #3 – The noisy loss** 

With the new dataloader, we didn't have throughput drops but the loss curve looked more noisy.

 `nanosets`  had been producing smoother loss, and the difference rang a bell from an old debugging war: a few years ago, we'd found a shuffling bug in our pretraining code where documents were shuffled, but sequences inside a batch were not, leading to small spikes. 

Checking our new dataloader confirmed it: it was reading sequences sequentially from each document. That's fine for short files, but with domains like code, a single long low-quality file can fill an entire batch and cause loss spikes.

 **Fix #3 – Shuffle at the sequence level** 

We had two options:

1. Change the dataloader to do random access (risk: higher memory usage).
1. Pre-shuffle tokenized sequences offline.

With the time pressure to start the run and our cluster reservation running, we went with option #2 as the safer, faster fix. Tokenized data was already on each node, so reshuffling locally was cheap (~1 h). We also generated shuffled sequences for each epoch with different seeds to avoid repeating shuffling patterns across epochs.

<Note title="Know when to patch vs. fix" variant="danger">

When facing urgent deadlines, it might be faster to adopt a proven solution or quick workaround than to debug your own broken implementation. Earlier, we plugged in TokenizedBytes dataloader rather than fixing nanosets' index implementation. Here, we chose offline pre-shuffling over dataloader changes. But know when to take shortcuts, or you'll end up with a patchwork system that's hard to maintain or optimize.
</Note>

####  **Launch, Take Two** 

By now we had:

-  **Stable throughput**  (scratch storage + spare node strategy)
-  **No step-count-induced drops**  ( `TokenizedBytes`  dataloader)
-  **Clean, sequence-level shuffling**  (offline pre-shuffle per epoch)

We relaunched. This time, everything held. The loss curve was smooth, throughput was consistent, and we could finally focus on training instead of firefighting.

 **Mystery #4 – Unsatisfactory performance** 

After fixing the throughput and dataloader issues, we launched the run again and trained smoothly for the first two days. Throughput was stable, loss curves looked as expected, and nothing in the logs suggested any problems. At around the 1T token mark, however, the evaluations revealed something unexpected.

As part of our monitoring, we evaluate intermediate checkpoints and compare them to historical runs. For instance, we had the [intermediate checkpoints ](https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-intermediate-checkpoints)from SmolLM2 (1.7B) trained on a similar recipe, so we could track how both models progressed at the same stages of training. The results were puzzling: despite having more parameters and a better data mixture, the 3B model was performing worse than the 1.7B at the same training point. Loss was still decreasing, and benchmark scores were improving, but the improvement rate was clearly below expectations.

Given that we had thoroughly tested every architecture and data change introduced in SmolLM3 compared to SmolLM2, we validated the training framework and there were only a few remaining untested differences between the two training setups. The most obvious was tensor parallelism. SmolLM2 could fit on a single GPU and was trained without TP, while SmolLM3 required TP=2 to fit in memory. We didn't suspect it or think of testing it before, since TP was used in the 3B ablations and their results made sense.

 **Fix #4 - The final fix** 

To test the TP bug hypothesis, we trained a 1.7B model with the exact same setup as SmolLM3 — same architecture changes (document masking, NoPE), same data mixture, same hyperparameters — both with and without TP. The difference was immediate: the TP version consistently had a higher loss and lower downstream performance than the non-TP version. That confirmed we were looking at a TP-related bug.

We then examined the TP implementation in detail, comparing weights from TP and non-TP runs. The problem turned out to be subtle but significant: we were using identical random seeds across all TP ranks, when each rank should have been initialised with a different seed. This caused correlated weight initialisation across shards, which affected convergence. The effect was not catastrophic — the model still trained and improved — but it introduced enough inefficiency to explain the gap we observed at scale.
Below is the bug fix:

```diff
diff --git a/src/nanotron/trainer.py b/src/nanotron/trainer.py
index 1234567..abcdefg 100644
--- a/src/nanotron/trainer.py
+++ b/src/nanotron/trainer.py
@@ -185,7 +185,10 @@ class DistributedTrainer:
     ):
         # Set random states
-        set_random_seed(self.config.general.seed)
+        # Set different random seed for each TP rank to ensure diversity
+        tp_rank = dist.get_rank(self.parallel_context.tp_pg)
+        set_random_seed(self.config.general.seed + tp_rank)

+
```

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/tp_debug_fix_loss.csv",
xDomain: [0, 9e9],
yDomain: [2.7, 3.7],
smoothing: true,
title: "TP Debug Fix Loss"
}}
/>



Once we fixed the seeds so that each TP rank used a different seed, we repeated the ablations experiments and confirmed that TP and non-TP runs now matched in both loss curves and downstream performance. To make sure there were no other hidden issues, we ran additional sanity checks: a SmolLM2-style (architecture and data wise) run at 3B parameters, and a separate SmolLM3 run at 3B parameters, comparing both to SmolLM2's checkpoints. The results now aligned with expectations: the 1.7B SmolLM2 performed worse than the 3B SmolLM2 variant, which in turn was below SmolLM3's 3B performance.

<Wide>

<HtmlEmbed
src="embeds/d3-six-line-charts.html"
config={{
dataUrl: "./data/evals_tp_bug_fix_200B.csv",
charts: [
{ title: "HellaSwag", metric: "hellaswag" },
{ title: "MMLU", metric: "mmlu" },
{ title: "ARC", metric: "arc" },
{ title: "PIQA", metric: "piqa" },
{ title: "OpenBookQA", metric: "openbookqa" },
{ title: "WinoGrande", metric: "winogrande" }
],
smoothing: true,
smoothingWindow: 15
}}
/>
</Wide>



This debugging process reinforced one of the core principles we outlined earlier in this blog: 

"The real value of a solid ablation setup goes beyond just building a good model. When things inevitably go wrong during our main training run (and they will, no matter how much we prepare), we want to be confident in every decision we made and quickly identify which components weren't properly tested and could be causing the issues. This preparation saves debugging time and keeps our sanity intact. There's nothing worse than staring at a mysterious training failure with no idea where the bug could be hiding."

Because every other component in our training had been validated, we could pinpoint TP as the only plausible cause and fix the bug within a single day of detecting the performance gap.

With that, we had resolved the last in a series of unexpected issues that had surfaced since launch. Third time's a charm, from that point on, the remaining month of training was relatively uneventful, just the steady work of turning trillions of tokens into a finished model, interrupted by occasional restarts due to node failures.

### Staying the course

As the previous section showed, scaling from ablations to full pretraining wasn't just "plug and play." It brought unexpected challenges, but we successfully identified and resolved each issue. This section covers the essential monitoring setup and considerations for large-scale training runs. We'll address critical questions: when should you restart training after encountering problems? How do you handle issues that surface deep into a run? Which metrics truly matter? Should you maintain a fixed data mixture throughout training?

#### Training monitoring: beyond loss curves

The reason we caught the tensor-parallelism bug was not the loss curve, which looked fine, but the fact that downstream evaluations were lagging behind expectations. Additionally, having evaluations from SmolLM2's intermediate checkpoints was critical: they gave us a sanity check that the 3B model wasn't on the right track early. So if you're training large models, start running downstream evaluations early, and if you're comparing to an open-source model, ask whether the authors can provide intermediate checkpoints, those can be invaluable as reference points.

On the infrastructure side, the most important metric is throughput, measured in tokens per second. For SmolLM3, we expected stable throughput between 13,500–14,000 tokens/sec across the run, and any sustained deviation was a red flag. But throughput alone is not enough: you also need continuous hardware health monitoring to anticipate and detect hardware failures. Some of the key metrics we tracked included: GPU temperatures, memory usage and compute utilisation. We log them into Grafana dashboards and set up real-time Slack alerts for hardware anomalies.

#### Fix and restart vs fix on the fly

Given that we restarted our run after 1T tokens, an important question arises: do you always need to restart when something goes wrong? The answer depends on the severity and root cause of the issue.

In our case, the TP seeding bug meant we were starting on the wrong foot, half our weights weren't properly initialised. The model was showing performance similar to SmolLM2 and plateauing at similar points, meaning we'd likely end up with a model that performed the same but cost almost twice as much to train. Restarting made sense. However, many issues can be course-corrected mid-run to avoid wasting compute. The most common issue involves  *loss spikes* , those sudden jumps in training loss that can either signal minor hiccups or divergence.

As [Stas Bekman](https://media.istockphoto.com/id/486869012/fr/photo/ch%C3%A8vre-est-%C3%A0-nous.jpg?s=612x612&w=0&k=20&c=F26PCPZiy1P3FLZS23GWhKcQ8Buqfx8StHYoX85hq-s%3D) nicely puts it in the [Machine Learning Engineering Open Book ](https://github.com/stas00/ml-engineering/blob/master/training/instabilities/training-loss-patterns.md)"Training loss plots are similar to heartbeat patterns—there's the good, the bad, and the you-should-worry ones."

<iframe 
  src="https://tfrere-loss-experiment.hf.space?project=spike-loss-comparison&metrics=loss&sidebar=hidden&navbar=hidden&smoothing=0&" 
  style="width:100%; height:350px; border:0;"
  title="Trackio Dashboard - Spike Loss Comparison"
  loading="lazy">
</iframe>


Loss spikes fall into two categories:

- Recoverable spikes: These can recover either fast (immediately after the spike) or slow (requiring several more training steps to return to the pre-spike trajectory). You can usually continue training through these. If recovery is very slow, you can try rewinding to a previous checkpoint to skip problematic batches.
- Non-recoverable spikes: The model either diverges or plateaus at worse performance than before the spike. These require more significant intervention than simply rewinding to a previous checkpoint.

While we don't fully understand training instabilities, we know they become more frequent at scale. Common culprits, assuming a conservative architecture and optimizer, include:

- High learning rates: These cause instability early in training and can be fixed by reducing the learning rate.
- Bad data: Usually the main cause of recoverable spikes, though recovery may be slow. This can happen deep into training when the model encounters low-quality data. 
- Data-parameter state interactions: PaLM [@palm] observed that spikes often result from specific combinations of data batches and model parameter states, rather than "bad data" alone. Training on the same problematic batches from a different checkpoint didn't reproduce the spikes.
- Poor initialisation: Recent work by OLMo2 [@olmo2] showed that switching from scaled initialisation to a simple normal distribution (mean=0, std=0.02) improved stability.
- Precision issues: While no one trains with FP16 anymore, [BLOOM](https://arxiv.org/abs/2211.05100) found it highly unstable compared to BF16.

 **Before spikes happen, build in stability:** 

Small models with conservative learning rates and good data rarely spike, but larger models require proactive stability measures. As more teams have trained at scale, we've accumulated a toolkit of techniques that help prevent training instability:

Data filtering and shuffling: By this point in the blog, you've noticed how often we circle back to data. Making sure your data is clean and well-shuffled can prevent spikes. For instance, OLMo2 found that removing documents with repeated n-grams (32+ repetitions of 1-13 token spans) significantly reduced spike frequency.

Training modifications: Z-loss regularisation keeps output logits from growing too large without affecting performance. And excluding embeddings from weight decay also helps.

Architectural changes: QKNorm (normalising query and key projections before attention) has proven effective. OLMo2 and other teams found it helps with stability, and interestingly, [Marin team](https://wandb.ai/marin-community/marin/reports/Marin-32B-Work-In-Progress--VmlldzoxMzM1Mzk1NQ) found that it can even be applied mid-run to fix divergence issues. 

 **When spikes happen anyway - damage control:** 

Even with these precautions, spikes can still occur. Here are some options for fixing them:

-  **Skip problematic batches** : Rewind to before the spike and skip the problematic batches. This is the most common fix for spikes. The Falcon team [@almazrouei2023falconseriesopenlanguage] skipped 1B tokens to resolve their spikes, while the PaLM team [@palm] found that skipping 200-500 batches around the spike location prevented recurrence.
-  **Tighten gradient clipping** : Reduce the gradient norm threshold temporarily
-  **Apply architectural fixes**  such as QKnorm, as done in Marin.

We've walked through the scaling challenges, from throughput drops to the TP bug, the monitoring practices to catch problems early, and strategies for preventing and fixing loss spikes. Let's finish this chapter by discussing how multi-stage training can enhance your model's final performance.

### Mid-training

Modern LLM pretraining typically involves multiple stages with different data mixtures, often followed by a final phase to extend context length. For example, Qwen3 [@qwen3] uses a three-stage approach: a general stage on 30T tokens at 4k context, a reasoning stage with 5T higher-quality tokens emphasising STEM and coding, and finally a long context stage on hundreds of billions of tokens at 32k context length. SmolLM3 follows a similar philosophy, with planned interventions to introduce higher-quality datasets and extend context, alongside reactive adjustments based on performance monitoring.

As we explained in the data curation section, the data mixture doesn't have to stay fixed throughout training. Multi-stage training allows us to strategically shift dataset proportions as training progresses. Some interventions are planned from the start: for SmolLM3, we knew we'd introduce higher-quality FineMath4+ and Stack-Edu in Stage 2, then add curated Q&A and reasoning data during the final decay phase. Other interventions are reactive, driven by monitoring performance during training. For example, in SmolLM2, when we found math and code performance lagging behind our targets, we curated entirely new datasets (FineMath and Stack-Edu) and introduced them mid-training. This flexibility—whether following a planned curriculum or adapting to emerging gaps—is what allows us to maximize the value of our compute budget.

#### Stage 2 and stage 3 mixtures

The chart below show our 3 training stages and the progression of our web/code/math ratios during training. The SmolLM3 training configs for each stage are available [here](https://github.com/huggingface/smollm/tree/main/text/pretraining/smollm3) with exact data weights. For more details on the rationale and composition behind each stage, refer to the data curation section.

<Wide>

<HtmlEmbed src="/embeds/data-mixture-stages.html" />
</Wide>



 **Stage 1: Base training (8T tokens, 4k context)** 
The foundation stage uses our core pretraining mixture: web data (FineWeb-Edu, DCLM, FineWeb2, FineWeb2-HQ), code from The Stack v2 and StarCoder2, and math from FineMath3+ and InfiWebMath3+. All training happens at 4k context length.

 **Stage 2: High-quality injection (2T tokens, 4k context)** 
We introduce higher-quality filtered datasets: Stack-Edu for code, FineMath4+ and InfiWebMath4+ for math, and MegaMath for advanced mathematical reasoning (we add the Qwen Q&A data, synthetic rewrites, and text-code interleaved blocks).

 **Stage 3: LR decay with reasoning & Q&A data (1.1T tokens, 4k context)** 
During the learning rate decay phase, we further upsample high-quality code and math datasets while introducing instruction and reasoning data like OpenMathReasoning, OpenCodeReasoning and OpenMathInstruct. The Q&A samples are simply concatenated and separated by new lines.

#### Long context extension: From 4k to 128k tokens

Context length determines how much text your model can process, it's crucial for tasks like analysing long documents, maintaining coherent multi-turn conversations, or processing entire codebases. SmolLM3 started training at 4k tokens, but we needed to scale to 128k for real-world applications.

 **Why extend context mid-training?** 

Training on long contexts from the start is computationally expensive since attention mechanisms scale quadratically with sequence length. Moreover, research shows that extending context with a few dozen to a hundred billion tokens toward the end of training, or during continual pretraining, is enough to reach good long context performance [@prolong].

 **Sequential scaling: 4k→32k→64k** 

We didn't jump straight to 128k. Instead, we gradually extended context in stages, giving the model time to adapt at each length before pushing further. We ran two long context stages: first from 4k to 32k, then from 32k to 64k (the 128k capability comes from inference-time extrapolation, not training). We found that starting a fresh learning rate schedule for each stage over 50B tokens worked better than extending context during the last 100B tokens of the main decay phase. At each stage, we ran ablations to find a good long context data mix and RoPE theta value, and evaluated on the Ruler benchmark.

<Note title="Long context evals on base model" emoji="💡" variant="info">

During the long context ablations, we found [HELMET](https://arxiv.org/abs/2410.02694) benchmark to be very noisy on base models (the same training with different seeds gives variable results). <a href="https://arxiv.org/abs/2410.02660">Gao et al.</a> recommend doing SFT on top to reduce variance on the benchmarks' tasks. 
Instead we opted for RULER, which we found to give more reliable signal at the base model level.
</Note>

During this phase, it's common to upsample long context documents such as lengthy web pages and books to improve long context performance [@prolong; qwen3]. We ran several ablations upsampling books, articles, and even synthetically generated documents for tasks like retrieval and fill-in-the-middle, following Qwen2.5-1M's approach [@qwen1Million] with FineWeb-Edu and Python-Edu. Surprisingly, we didn't observe any improvement over just using the baseline mixture from Stage 3, which was already competitive with other state-of-the-art models like Llama 3.2 3B and Qwen2.5 3B on Ruler. We hypothesise this is because the baseline mixture naturally includes long documents from web data and code (estimated at 10% of tokens), and that using NoPE helped.

<Sidenote>

For more insights into long context extension, we recommend reading the paper <a href="https://arxiv.org/abs/2410.02660">How to Train Long-Context Language Models (Effectively)</a>
</Sidenote>

 **RoPE ABF (RoPE with Adjusted Base Frequency):**  When going from 4k to 32k, we increased RoPE theta (base frequency) to 2M, and to go from 32k to 64k, we increased it to 5M. We found that using larger values like 10M slightly improves RULER score but hurts some short context task like GSM8k, so we kept 5M which didn't impact short context. 
During this context extension phase, we also used the opportunity to further upsampled math, code, and reasoning Q&A data, and we added few hundred thousand samples in ChatML format.

<Sidenote>

We also experimented with sliding window attention (window sizes of 4k, 8k, and 16k) during the 4k→32k extension, but found it performed worse on RULER compared to full attention.
</Sidenote>

 **YARN extrapolation: Reaching 128k** 
Even after training on 64k contexts, we wanted SmolLM3 to handle 128k at inference. Rather than training on 128k sequences (prohibitively expensive), we used YARN (Yet Another RoPE extensioN method) [@yarn], which allows the model to extrapolate beyond its training length. In theory, YARN allows a four-fold increase in sequence length. We found that using the 64k checkpoint gave better performance at 128k than using the 32k checkpoint, confirming the benefit of training closer to the target inference length. However, pushing to 256k (four-fold from 64k) showed degraded Ruler performance, so we recommend using the model up to 128k. 

And with that, we've walked through the full pretraining journey for SmolLM3, from planning and ablations to the final training run, with all the behind-the-scenes challenges along the way.

### Wrapping up pretraining

We've covered a lot of ground. From the training compass that helped us decide why and what to train, through strategic planning, systematic ablations that validated every architectural choice, to the actual training marathon where surprises emerged at scale (throughput mysteriously collapsing, dataloader bottlenecks, and a subtle tensor parallelism bug that forced a restart at 1T tokens).

The messy reality behind those polished technical reports is now visible:  **training LLMs is as much about disciplined experimentation and rapid debugging as it is about architectural innovations and data curation.**  Planning identifies what's worth testing. Ablations validate each decision. Monitoring catches problems early. And when things inevitably break, systematic derisking tells you exactly where to look.

For SmolLM3 specifically, this process delivered what we set out to build: a 3B model trained on 11T tokens that's competitive on math, code, multilingual understanding, and long-context tasks, in the Pareto frontier of Qwen3 models. 

<Wide>

<HtmlEmbed src="embeds/base-model-performance.html" caption="The win rate of of base models evaluation on: HellaSwag, ARC, Winogrande, CommonsenseQA, MMLU-CF, MMLU Pro CF, PIQA, OpenBookQA, GSM8K, MATH, HumanEval+, MBPP+" />
</Wide>

With our base model checkpoint saved, training complete, and GPUs finally cooling down, we might be tempted to call it done. After all, we have a model that predicts text well, achieves strong benchmark scores, and demonstrates the capabilities we targeted.

Not quite. Because what people want today are assistants and coding agents, not raw next-token predictors.

This is where post-training comes in. And just like pretraining, the reality is messier than the papers suggest.

## Beyond base models — post-training in 2025

<Quote source="Lewis Tunstall, optimistic LLM expert.">

Once the pre-training finishes we should have an SFT baseline within a day
</Quote>

<HtmlEmbed src="embeds/post-training-adventure.html" caption="Choose your own (post-training) adventure." />



Pre-training gave us SmolLM3's raw ability, but before the GPUs are cooled down we enter the next frontier of model capabilities:  *post-training* . This includes supervised fine-tuning, reinforcement learning, model merging, and more — all designed to bridge the gap between "a model that predicts text" to "a model people can actually use". If pre-training is about brute-forcing knowledge into weights, post-training is about sculpting that raw capability into something reliable and steerable. And just like pre-training, the polished post-training papers don't capture the late-night surprises: GPU meltdowns, finicky data mixtures, or the way a seemingly minor chat template decision can ripple through downstream benchmarks. In this section, we'll show how we navigated the messy world of post-training to turn SmolLM3 from a strong base model into a state-of-the-art hybrid reasoner.

<Note title="What is a hybrid reasoning model?" emoji="📍" variant="info">

A hybrid reasoning model operates in two distinct modes: one for concise, direct responses and another for extended step-by-step reasoning. Typically, the operating mode is set by the user in the system message. Following Qwen3, we make this explicit with lightweight commands: "/think" invokes extended reasoning, while "/no_think" enforces concise answers. This way, the user controls whether the model prioritises depth or speed.
</Note>

### Post-training compass: why → what → how

Just like pre-training, post-training benefits from a clear compass to avoid wasted research and engineering cycles. Here's how to frame it:

1.  **Why post-train?** The three motivations for training that we outlined in the [pretraining compass](#training-compass-why--what--how)—research, production, and strategic open-source—apply equally to post-training. For example, you might be exploring whether RL can unlock new reasoning capabilities in an existing model (research), or you might need to distill a large model into a smaller one for latency reasons (production), or you may have identified a gap where no strong open model exists for a specific use case (strategic open-source). The distinction is that post-training builds on existing capabilities rather than creating them from scratch. However, before reaching for your GPUs, ask yourself:
    - Do you really need to post-train? Many open-weight models now rival proprietary ones on a wide range of tasks. Some can even be run locally with quantisation and modest compute. If you want a generalist assistant, an off-the-shelf model from the [Hugging Face Hub](https://huggingface.co/models) may already meet your needs.
    - Do you have access to high-quality, domain specific data? Post-training makes most sense when you are targeting a specific task or domain where a generalist model underperforms. With the right data, you can tune the model to produce more accurate outputs for the applications you care most about.
    - Can you measure success? Without clear evaluation criteria, you won't know if post-training really helps.

1.  **What should post-training achieve?**  This depends on your priorities:
    - Do you want a crisp instruction-follower that rarely drifts off-topic?
    - A versatile assistant that can switch tones and roles on demand?
    - A reasoning engine that can tackle math, code, or agentic problems?
    - A model that can converse in multiple languages?

1.  **How will you get there?**  That's where recipes matter. We'll cover:
    -  **Supervised fine-tuning (SFT)**  to instil core capabilities.
    -  **Preference optimisation (PO)**  to directly learn from human or AI preferences.
    -  **Reinforcement learning (RL)**  to refine reliability and reasoning beyond supervised data.
    -  **Data curation**  to strike the right balance between diversity and quality.
    -  **Evaluation**  to track progress and catch regressions early.

This compass keeps the chaos of post-training grounded. The  *why*  gives direction, the  *what*  sets priorities, and the  *how* turns ambitions into a practical training loop. 

Let's walk through how we answered these questions for SmolLM3:

-  **Why?** For us, the "why" was straightforward as we had a base model that needed post-training before release. At the same time, hybrid reasoning models like Qwen3 were becoming increasingly popular, yet open recipes showing how to train them were scarce. SmolLM3 gave us an opportunity to address both: prepare a model for real-world use and contribute a fully open recipe to sit on the Pareto front alongside Qwen3's 1.7B and 4B models.
-  **What?**  We set out to train a hybrid reasoning model that was tailored to SmolLM3's strengths, chiefly that reasoning quality should hold up across languages other than English. And since real-world use increasingly involves tool calling and long-context workflows, those became core requirements in our post-training recipe.
-  **How?** That's the rest of this chapter 😀.

Just like with pre-training, we start with the fundamentals: evals and baselines, because every big model starts with a small ablation. But there's a key difference in how we ablate. In pre-training, "small" usually means smaller models and datasets. In post-training, "small" means smaller datasets and  *simpler*   *algorithms* . We almost never use a different base model for ablations because behaviour is too model-dependent, and runs are short enough to iterate on the target model directly.

<Sidenote>

The main exception to this rule is when you're using an off-the-shelf base model from the Hugging Face Hub. In this case, ablating base models can make sense, since there's a world of difference between one model trained on 1T tokens and another on 10T, even if they have the same size.
</Sidenote>

Let's start with the topic many model trainers avoid until too late into a project: evals.

### First things first: evals before everything else

The very first step in post-training — just like in pre-training — is to decide on the right set of evals. Since most LLMs today are used as assistants, we've found that aiming for a model that "works well" is a better goal than chasing abstract benchmarks of "intelligence" like [ARC-AGI](https://arcprize.org/arc-agi). So what does a good assistant need to do? At minimum, it should be able to:

- Handle ambiguous instructions
- Plan step-by-step
- Write code
- Call tools when appropriate

These behaviours draw on a mix of reasoning, long-context handling, and skills with math, code, and tool use. Models as small as or even smaller than 3B parameters can work well as assistants, though performance usually falls off steeply below 1B.

<Sidenote>

It remains an interesting, yet open question whether tiny models can use tool calling to offset their limited capacity and thus act as viable assistants. See the models from [LiquidAI](https://huggingface.co/LiquidAI/LFM2-1.2B-Tool) for recent work in this direction.
</Sidenote>

At Hugging Face, we use a layered eval suite, echoing the pre-training principles (monotonicity, low noise, above-random signal, ranking consistency) that we detailed in the [ablations section](#every-big-model-starts-with-a-small-ablation) for pretraining. 

<Note title="Keep your evals current" emoji="📍" variant="info">

The list of evals to consider is continuously evolving as models improve and the ones discussed below reflect our focus in mid 2025. See the <a href="https://github.com/huggingface/evaluation-guidebook/blob/main/yearly_dives/2025-evaluations-for-useful-models.md">[Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook/blob/main/yearly_dives/2025-evaluations-for-useful-models.md)</a> for a comprehensive overview of post-training evals.
</Note>

Here are the many ways one can evaluate a post trained model: 

1.  **Capability evals** 

This class of evals target fundamental skills, like reasoning and competitive math and coding. 
    -  **Knowledge.**  We currently use GPQA Diamond [@gpqa] as the main eval for scientific knowledge. This benchmark consists of graduate-level, multiple-choice questions.  For small models, it's far from saturated and gives better signal than MMLU and friends, while being much faster to run. Another good test of factuality is SimpleQA [[@simpleqa](https://huggingface.co/papers/2411.04368)], although small models tend to struggle significantly on this benchmark due to their limited knowledge.
    -  **Math.**  To measure mathematical ability, most models today are evaluated on the latest version of AIME (currently the 2025 version). MATH-500 [[@openaiprm](https://arxiv.org/abs/2305.20050)] remains a useful sanity test for small models, but is largely saturated by reasoning models. For a more comprehensive set of math evals, we recommend those from [MathArena](https://matharena.ai/).
    -  **Code.**  We use the latest version of [LiveCodeBench](https://livecodebench.github.io/leaderboard.html) to track coding competency. Although targeted towards competitive programming problems, we've found that improvements on LiveCodeBench do translate into better coding models, albeit limited to Python. [SWE-bench Verified](https://openai.com/index/introducing-swe-bench-verified/) is a more sophisticated measure of coding skill, but tends to be too hard for small models and thus is not one we usually consider.
    -  **Multilinguality.** Unfortunately, there are not many options when it comes to testing the multilingual capabilities of models. We currently rely on Global MMLU [[@globalmmlu](https://arxiv.org/abs/2412.03304)] to target the main languages our models should perform well in, with MGSM [[@mgsm](/2421384ebcac800cb22cdf0bb34c69f7)] included as a test of multilingual mathematical ability.

1.  **Integrated task evals** 

These evals test things that are close to what we'll ship: multi-turn reasoning, long-context use, and tool calls in semi-realistic settings.
    -  **Long context.**  The most commonly used test for long-context retrieval is the Needle in a Haystack (NIAH) [[@niah](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)], where a random fact ("needle") is placed in somewhere within a long document ("haystack") and the model has to retrieve it. However, this benchmark is too superficial to discriminate long-context understanding, so the community has developed more comprehensive evals like RULER [[@ruler](https://arxiv.org/abs/2404.06654)] and HELMET [[@helmet](https://arxiv.org/abs/2410.02694)]. More recently, OpenAI have released the [MRCR](https://huggingface.co/datasets/openai/mrcr) and [GraphWalks](https://huggingface.co/datasets/openai/graphwalks) benchmarks which extend the difficulty of long-context evals.

<Sidenote>

See also this <a href="https://nrehiew.github.io/blog/long_context/">excellent [blog post](https://nrehiew.github.io/blog/long_context/)</a> on the limitations of long-context evals and how to design realistic ones.
</Sidenote>

    -  **Instruction following.**  IFEval [[@ifeval](https://arxiv.org/abs/2311.07911)] is currently the most popular eval to measure instruction following, and uses automatic scoring against "verifiable instructions". IFBench [[@ifbench](https://arxiv.org/abs/2507.02833)] is a new extension from Ai2 which includes a more diverse set of constraints than IFEval and mitigates some benchmaxxing that has occurred in recent model releases. For multi-turn instruction following, we recommend Multi-IF [[@multiif](https://arxiv.org/abs/2410.15553)] or MultiChallenge [[@multichallenge](https://arxiv.org/abs/2501.17399)]. 
    -  **Alignment.** Measuring how well models align to user intent is typically done through human annotators or by public leaderboards like [LMArena](https://lmarena.ai/). This is because qualities such as free-form generation, style, or overall helpfulness are difficult to measure quantitatively with automated metrics. However, in all cases it is very expensive to run these evaluations which is why the community has resorted to using LLMs as a proxy for human preferences. The most popular benchmarks of this flavour include AlpacaEval [[@alpacaeval](https://arxiv.org/abs/2404.04475)], ArenaHard [[@arenahard](https://arxiv.org/abs/2406.11939)] and MixEval [[@mixeval](https://arxiv.org/abs/2406.06565)], with the latter having the strongest correlation with human Elo ratings on LMArena.
    -  **Tool calling.**  [BFCL](https://gorilla.cs.berkeley.edu/leaderboard.html) provides a comprehensive test of tool calling, albeit one that is often saturated quite quickly. TAU-Bench [[@taubench](https://arxiv.org/abs/2506.07982)] provides a test of a model's ability to use tools and resolve user problems in simulated customer service settings and has also become a popular benchmark to report on.

1.  **Overfitting-prevention evals** 

To test whether our models are overfitting to a specific skill, we include some robustness or adaptability evals in our set, like GSMPlus [[@gsmplus](https://arxiv.org/abs/2402.19255)], which perturbs problems from GSM8k [[@gsm8k](https://arxiv.org/abs/2110.14168)] to test whether models can still solve problems of similar difficulty.  

1.  **Internal evals** 

Although public benchmarks can provide some useful signal during model development, they are no substitute for implementing your own internal evals to target specific capabilities, or asking internal experts to interact with your model.

<Sidenote>

This is especially true if you are building an AI product. See Hamel Husain's wonderful <a href="https://hamel.dev/blog/posts/evals/">[blog post](https://decodingml.substack.com/?utm_source=navbar&utm_medium=web)</a> for specific advice on this topic.
</Sidenote>

For example, for SmolLM3 we needed a benchmark to evaluate whether the model was capable of  *multi-turn*   *reasoning* , so we implemented a variant of Multi-IF to measure this. 

1.  **Vibe evaluations and arenas** 

Similarly, we have found that "vibe testing" intermediate checkpoints (aka interacting with your model) is essential for uncovering subtle quirks in model behaviour that are not captured by eval scores. As we discuss later, vibe testing uncovered a bug in our data processing code where all system messages were deleted from the corpus! 
This is also something that can be done at scale to measure human preference, like on the popular [LMArena](https://lmarena.ai/). However, crowdsourced human evaluation tends to be brittle (favouring sycophancy and flowery speech over actual usefulness), so it's important to see it as a low signal feedback. 

<Note title="Decontaminate your training data" emoji="☝️" variant="danger">

One risk with relying on public benchmarks is that the models can easily be overfit to them, especially when synthetic data is used to generate prompts and responses that are similar to the target benchmarks. For this reason, it is essential to decontaminate your training data against the evals you will use to guide model development. You can do this with N-gram matching using scripts like those in <a href="https://github.com/huggingface/open-r1/blob/main/scripts/decontaminate.py">Open-R1.</a>
</Note>

For SmolLM3 specifically, we wanted a hybrid reasoning model that could reliably follow instructions and reason well in popular domains like mathematics and code. We also wanted to ensure we preserved the base model's capabilities of multilinguality and long-context retrieval.

This led us to the following set of evals:

| Benchmark | Category | Number of prompts | Metric |
| --- | --- | --- | --- |
| AIME25 | Competitive mathematics | 30 | avg@64 |
| LiveCodeBench (v4 for validation, v5 for final release) | Competitive programming | 100 (268) | avg@16 |
| GPQA Diamond | Graduate-level reasoning | 198 | avg@8 |
| IFEval | Instruction following | 541 | accuracy |
| MixEval Hard | Alignment | 1000 | accuracy |
| BFCL v3 | Tool use | 4441 | mixed |
| Global MMLU (lite for validation) | Multilingual Q&A | 590,000 (6,400) | accuracy |
| GSMPlus (mini for validation) | Robustness | 10,000 (2,400) | accuracy |
| RULER | Long context | 6,500 | accuracy |

Let's look at a few example questions from each to get a concrete sense of what these evaluations actually test:

<iframe
  src="https://huggingface.co/datasets/HuggingFaceTB/post-training-benchmarks-viewer/embed/viewer/aime25/test"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>


Browse through the examples above to see the types of questions in each benchmark. Notice how the diversity of domains ensures we're testing different aspects of model capability throughout our ablations.

For the 3B model scale we were working with, we felt these evals would give us actionable signal, run faster than training itself, and give us confidence that improvements are real and not just noise from sampling. We also tracked our pre-training evals (see the [ablation section](#every-big-model-starts-with-a-small-ablation) for a full list) to make sure we weren't regressing too much on the base model performance.

<Note title="Prioritise your evals" emoji="☝️" variant="danger">

The story above suggests that we got together as a team, converged on the set of evals, and had them ready to go before training any models. The reality was far messier: we had a tight deadline and rushed ahead with model training before many of the above evals were implemented (e.g. RULER was not available until a few days before the model release 🙈). In hindsight, this was a  **mistake**  and we should have discussed with the pre-training team which core evals should be preserved across post-training and also prioritised implementing them long before the base model was finished training. In other words, prioritise your evals before all else!
</Note>

#### Rules of engagement

Let's summarise this section with a few hard-earned lessons we've acquired from evaluating thousands of models:

- Use  **small subsets to accelerate evals**  during model development. For example, LiveCodeBench v4 is highly correlated with v5, but runs in half the time. Alternatively, use methods like those from tinyBenchmarks [[@tinybenchmarks](https://arxiv.org/abs/2402.14992)] which seek to find the smallest subset of prompts that reliably match the full evaluation.
- For  **reasoning models** , strip the chain-of-thought from the  **scored**  output. This eliminates false positives and also directly impacts benchmarks like IFEval which penalise responses that violate constraints like "write a poem in under 50 words".
- If an eval uses  **LLM judges, pin the judge and version**  for apples-to-apples comparisons over time. Even better, use an open weight model so that the eval is reproducible even if a provider deprecates the judge model.
- Be wary of  **contamination**  in the base models. For example, most models released before AIME 2025 performed much worse than AIME 2024, suggesting some benchmaxxing was at play.
- If possible, treat anything used during ablations as  **validation** , not  **test.** This means keeping a set of held-out benchmarks for the final model reports, similar to the Tulu3 evaluation framework [[@tulu3](https://arxiv.org/abs/2411.15124)].
- Always include a small set of  **"vibe evals"**  on your own data and tasks to catch overfitting to public suites.
- For evals with a small number of problems (typically less than ~2k), sample  `k`  times and report the  `avg@k`  accuracy. This is important to mitigate noise which can lead to incorrect decisions during development.
- When implementing a new eval, make sure you  **can replicate the published result of a few models**  (within some error). Failing to do this will waste a lot of time later on if you need to fix the implementation and re-evaluate many checkpoints.
- When in doubt, always go back to the evaluation data, and notably inspect what you are prompting your models with 

With the evals at hand, it's time to train some models! Before doing that, we first need to pick a post-training framework.

### Tools of the trade

Behind every post-training recipe lies a toolbox of frameworks and libraries that enable large-scale experimentation. Each frameworks brings its own set of supported algorithms, fine-tuning methods, and scalability features. The table below summarises the main areas of support, from supervised fine-tuning (SFT) to preference optimisation (PO) and reinforcement learning (RL):

| Framework | SFT | PO | RL | Multi-modal | FullFT | LoRA | Distributed |
| --- | --- | --- | --- | --- | --- | --- | --- |
| [ **TRL** ](https://github.com/huggingface/trl) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [ **Axolotl** ](https://github.com/axolotl-ai-cloud/axolotl) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [ **OpenInstruct** ](https://github.com/allenai/open-instruct) | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ |
| [ **Unsloth** ](https://github.com/unslothai/unsloth) | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [ **vERL** ](https://github.com/volcengine/verl) | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ✅ |
| [ **Prime RL** ](https://github.com/PrimeIntellect-ai/prime-rl) | ✅ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ |
| [ **PipelineRL** ](https://github.com/ServiceNow/PipelineRL) | ❌ | ❌ | ✅ | ❌ | ✅ | ✅ | ✅ |
| [ **ART** ](https://github.com/OpenPipe/ART/tree/main) | ❌ | ❌ | ✅ | ❌ | ❌ | ✅ | ❌ |
| [ **TorchForge** ](https://github.com/meta-pytorch/torchforge) | ✅ | ❌ | ✅ | ❌ | ✅ | ❌ | ✅ |
| [ **NemoRL** ](https://github.com/NVIDIA-NeMo/RL) | ✅ | ✅ | ✅ | ❌ | ✅ | ❌ | ✅ |
| [ **OpenRLHF** ](https://github.com/OpenRLHF/OpenRLHF) | ✅ | ✅ | ✅ | ❌ | ✅ | ✅ | ✅ |

Here  *FullFT*  refers to  **full fine-tuning** , where all model parameters are updated during training.  *LoRA*  stands for  **Low-Rank Adaptation** , a parameter-efficient approach that updates only small low-rank matrices while keeping the base model frozen.  *Multi-modal*  refers to whether support for training on modalities beyond text (e.g. images) is supported and  *Distributed*  indicates whether training models on more than one GPU is possible.

At Hugging Face, we develop and maintain TRL, so it's our framework of choice and the one we used to post-train SmolLM3. 

<Note title="Fork your frameworks" emoji="📍" variant="info">

Given the fast-moving pace of the field, we've found it quite effective to run our experiments on an internal fork of TRL. This allows us to add new features very quickly, which are later upstreamed to the main library. If you're comfortable working with your framework's internals, adopting a similar workflow can be a powerful approach for rapid iteration.
</Note>

#### Why bother with frameworks at all?

There is a class of researchers that love to bemoan the use of training frameworks and instead argue that you should implement everything from scratch, all the time. The implicit claim here is that "real" understanding only comes from re-implementing every RL algorithm, manually coding every distributed training primitive, or hacking together a one-off eval harness.

But this position ignores the reality of modern research and production. Take RL for example. Algorithms like PPO and GRPO are notoriously tricky to implement correctly [[@ndetailsrlhf](https://arxiv.org/abs/2403.17031)], and tiny mistakes in normalisation or KL penalties can lead to days of wasted compute and effort.

Similarly, although it's tempting to write a single-file implementation of some algorithm, can that same script scale from 1B to 100B+ parameters?

Frameworks exist precisely because the basics are already well-understood and endlessly reinventing them is a poor use of time. That's not to say there's no value in low-level tinkering. Implementing PPO from scratch once is an excellent learning exercise. Writing a toy transformer without a framework teaches you how attention really works. But in most cases, just pick a framework you like and hack it for your purposes.

With that rant out of the way, let's take a look at where we often start our training runs.

### Why (almost) every post-training pipeline starts with SFT

If you spend any time on X these days, you'd think reinforcement learning (RL) is the only game in town. Every day brings new acronyms, [algorithmic tweaks](https://x.com/agarwl_/status/1981518825007853891), and heated debates [[@chu2025](https://huggingface.co/papers/2501.17161); [@yue2025](https://huggingface.co/papers/2504.13837)] about whether RL can elicit new capabilities or not.

<Sidenote>

As we'll see later in this chapter, RL really does work, but comes with practical tradeoffs we discuss below.
</Sidenote>

RL isn't new of course. OpenAI and other labs relied heavily on RL from human feedback (RLHF) [[@rlhf](https://huggingface.co/blog/rlhf)] to align their early models, but it wasn't until the release of DeepSeek-R1 [[@deepseekr1](https://huggingface.co/papers/2501.12948)] that RL-based post-training really caught on in the open-source ecosystem.

But one thing hasn't changed: almost every effective post-training pipeline still begins with supervised fine-tuning (SFT). The reasons are straightforward:

-  **It's cheap:**  SFT requires modest compute compared to RL. You can usually get meaningful gains without needing to burn a bonfire of silicon, and in fraction of the time required for RL.
-  **It's stable:**  unlike RL, which is notoriously sensitive to reward design and hyperparameters, SFT "just works."
-  **It's the right baseline:** a good SFT checkpoint usually gives most of the gains you're after, and it makes later methods like DPO or RLHF far more effective.

In practice, this means that SFT isn't just the first step because it's easy; it's the step that consistently improves performance before anything more complex should be attempted. This is especially true when you are working with base models. With a few exceptions, base models are too unrefined to benefit from advanced post-training methods.

<Note title="What about DeepSeek R1-Zero?" emoji="📍" variant="info">

At the frontier, the usual reasons for starting with SFT don't always apply. There's no stronger model to distill from and human annotations are too noisy for complex behaviors like long chain-of-thought. That's why DeepSeek skipped SFT and went straight to RL with R1-Zero; to *discover* reasoning behaviors that couldn't be taught with standard supervision.  

If you're in that regime, starting with RL can make sense. But if you're operating there ... you probably aren't reading this blog post anyway 😀.
</Note>

So, if SFT is where most pipelines begin, the next question is:  *what*  should you fine-tune? That starts with choosing the right base model.

#### Picking a base model

When choosing a base model for post-training, a few practical dimensions matter most:

-  **Model size:**  although smol models have dramatically improved over time, it is still the case today that larger models generalise better, and often with fewer samples. Pick a model size that is representative of how you plan to use or deploy the model after training. On the [Hugging Face Hub](https://huggingface.co/models?pipeline_tag=text-generation&num_parameters=min%3A0%2Cmax%3A32B&sort=trending), you can filter models by modality and size to find suitable candidates:

<Image src={Screenshot_2025_10_24_at_09_37_24_2961384e_bcac_8055_9e8e_ffbd3a1aa368} alt="Image" />

-  **Architecture (MoE vs dense):**  MoE models activate a subset of parameters per token and offer higher capacity per unit of compute. They're great for large-scale serving, but trickier to fine-tune in our experience. By contrast, dense models are simpler to train and often outperform MoEs at smaller scales.
-  **Post-training track record:**  benchmarks are useful, but it's even better if the base model has already spawned a collection of strong post-trained models that resonate with the community. This provides a proxy for whether the model trains well.

<Sidenote>

The <a href="https://www.reddit.com/r/LocalLLaMA/">LocalLLaMa subreddit</a> is a great place to understand the broad vibes of new models. <a href="https://artificialanalysis.ai">Artificial Analysis</a> and <a href="https://lmarena.ai">LMArena</a> also provide independent evaluation of new models, although these platforms are sometimes [benchmaxxed by model providers.](https://huggingface.co/papers/2504.20879)
</Sidenote>

In our experience, the base models from Qwen, Mistral, and DeepSeek are the most amenable to post-training, with Qwen being a clear favourite since each model series typically covers a large parameter range (e.g. Qwen3 models range in size from 0.6B to 235B!). This feature makes scaling far more straightforward.

Once you've chosen a base model that matches your deployment needs, the next step is to establish a simple, fast SFT baseline to probe its core skills.

#### Training simple baselines

For SFT, a good baseline should be fast to train, focused on the model's core skills, and simple to extend with more data when a particular capability isn't up to scratch. Choosing which datasets to use for an initial baseline involves some taste and familiarity with those that are likely to be of high quality. In general, avoid over-indexing on public datasets which report high scores on academic benchmarks and instead focus on those which have been used to train great models like [OpenHermes](https://huggingface.co/datasets/teknium/OpenHermes-2.5). For example, in the development of SmolLM1, we initially ran SFT on [WebInstruct](https://huggingface.co/datasets/TIGER-Lab/WebInstructFull), which is a great dataset on paper. However, during our vibe tests, we discovered it was too science focused because the model would respond with equations to simple greetings like "How are you?".

<Sidenote>

The use of vibe testing to uncover quirks in the training data is a recurrent theme in this chapter — don't underestimate the power of just chatting with your model!
</Sidenote>

This led us to create the [Everyday Conversations](/2421384ebcac800cb22cdf0bb34c69f7) dataset which turned out to be crucial for instilling basic chat capabilities in small models. 

For SmolLM3, we set out to train a hybrid reasoning model and initially picked a small set of datasets to target reasoning, instruction following, and steerabilty. The table below shows the statistics of each dataset:[^f1]

<Wide>

<Reference id="sft-datasets" caption="Data mixture for hybrid reasoning baselines">

| Dataset | Reasoning mode | # examples | % of examples | # tokens (M) | % of tokens | Avg. # tokens per example | Avg. # tokens in context | Avg. # tokens in response | Avg. # turns |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| Everyday Conversations | /no_think | 2,260 | 2.3 | 0.6 | 0.8 | 260.2 | 222.3 | 94.0 | 7.8 |
| SystemChats 30k | /no_think | 33,997 | 35.2 | 21.5 | 28.2 | 631.9 | 422.8 | 267.7 | 6.3 |
| Tulu 3 SFT Personas IF | /no_think | 29,970 | 31.0 | 13.3 | 17.5 | 444.5 | 119.8 | 380.7 | 2 |
| Everyday Conversations (Qwen3-32B) | /think | 2,057 | 2.1 | 3.1 | 4.1 | 1,522.4 | 376.8 | 1,385.6 | 4 |
| SystemChats 30k (Qwen3-32B) | /think | 27,436 | 28.4 | 29.4 | 38.6 | 1070.8 | 84.6 | 1,042.7 | 2 |
| s1k-1.1 | /think | 835 | 0.9 | 8.2 | 10.8 | 8,859.3 | 370.9 | 9,728.5 | 2 |
| Total | - | 96,555 | 100.0 | 76.1 | 100.0 | 2,131.5 | 266.2 | 2,149.9 | 4.0 |
</Reference>

</Wide>

As we learned throughout the development of SmolLM3, training hybrid reasoning models is trickier than standard SFT because you can't just mix datasets together; you need to  *pair*  data across modes. Each example has to clearly indicate whether the model should engage in extended reasoning or give a concise answer, and ideally you want parallel examples that teach it when to switch modes. Another thing to note from the above table is that you should balance your data mixture in terms of  *tokens*  not  *examples* : for instance, the s1k-1.1 dataset is ~1% of the total examples but accounts for ~11% of the total tokens due to the long reasoning responses.

This gave us basic coverage across the skills we cared about most, but also introduced a new challenge: each dataset had to be formatted differently, depending on whether it should enable extended thinking or not. To unify these formats, we needed a consistent chat template.

#### Picking a good chat template

When it comes to choosing or designing a chat template, there isn't a one-size-fits-all answer. In practice, we've found there are a few questions worth asking up front:

-  **Can users customise the system role?**  If users should be able to define their own system prompts (e.g. "act like a pirate"), the template needs to handle that cleanly.
-  **Does the model need tools?**  If your model needs to call APIs, the template needs to accommodate structured outputs for tool calls and responses.
-  **Is it a reasoning model?** Reasoning models use templates like  `<think> ... </think>`  to separate the model's "thoughts" from its final answer. Some models discard the reasoning tokens across turns in a conversation, and the chat template needs to handle that logic. 
-  **Will it work with inference engines?**  Inference engines like vLLM and SGLang have dedicated parsers for reasoning and tools[^f2]. Compatibility with these parsers saves a lot of pain later, especially in complex agent benchmarks where consistent tool calls are essential.[^f3]

The table below shows a few popular chat templates and how they compare across the key considerations:

<Wide>

| **Chat template** | **System role customisation** | **Tools** | **Reasoning** | **Inference  compatibility** | **Notes** |
| --- | --- | --- | --- | --- | --- |
| ChatML | ✅ | ✅ | ❌ | ✅ | Simple and good for most use cases. |
| Qwen3 | ✅ | ✅ | <u>✅</u> | ✅ | Hybrid reasoning template |
| DeepSeek-R1 | ❌ | ❌ | ✅ | ✅ | Prefills reasoning content with  `<think>` . |
| Llama 3 | ✅ | ✅ | ❌ | ✅ | Has built-in tools like a Python code interpreter. |
| Gemma 3 | ✅ | ❌ | ❌ | ❌ | System role customisation defined at the first user turn. |
| Command A Reasoning | ✅ | ✅ | ✅ | ❌ | Multiple chat templates per model. |
| GPT-OSS | ✅ | ✅ | ✅ | ✅ | Based on the [Harmony response format](https://cookbook.openai.com/articles/openai-harmony). Complex, yet versatile. |
</Wide>

In most cases, we've found that ChatML or Qwen's chat templates are an excellent place to start. For SmolLM3, we needed a template for hybrid reasoning and found that Qwen3 was one of the few templates that struck a good balance across the dimensions we cared about. However, it had one quirk that we weren't entirely happy with: the reasoning content is  *discarded*  for all but the final turn in a conversation. As shown in the figure below, this is similar to how [OpenAI's reasoning models work](https://platform.openai.com/docs/guides/reasoning/how-reasoning-works):

<Wide>

```mermaid

flowchart LR
    subgraph Turn1 ["Turn 1"]
        T1_Input["**INPUT**"]
        T1_Output["**OUTPUT**"]
    end

    subgraph Turn2 ["Turn 2"]
        T2_Input["**INPUT**"]
        T2_Output["**OUTPUT**"]
    end

    subgraph Turn3 ["Turn 3"]
        T3_Input["**INPUT**"]
        T3_Output1["**OUTPUT**"]
        Reasoning["**REASONING**"]
        T3_Output2_Top["**OUTPUT**"]
        TruncatedOutput["**TRUNCATED OUTPUT**"]
    end

    T1_Input --> T2_Input
    T1_Output --> T2_Input

    T2_Input --> T3_Input
    T2_Output --> T3_Input

    T3_Input --> T3_Output1
    T3_Output1 --> Reasoning
    Reasoning --> T3_Output2_Top

    T3_Output2_Top -->|CONTEXT WINDOW ✂️| TruncatedOutput

    classDef input fill:#e8f5e8,stroke:#4caf50,stroke-width:2px
    classDef output fill:#e3f2fd,stroke:#2196f3,stroke-width:2px
    classDef reasoning fill:#f5f5f5,stroke:#999,stroke-width:1px
    classDef truncated fill:#ffebee,stroke:#f44336,stroke-width:3px
    classDef subgraphStyle fill:#f8f9fa,stroke:#dee2e6,stroke-width:1px
    classDef linkLabel fill:#f8f9fa,stroke:#dee2e6,stroke-width:1px

    class T1_Input,T2_Input,T3_Input input
    class T1_Output,T2_Output,T3_Output1,T3_Output2_Top output
    class Reasoning reasoning
    class TruncatedOutput truncated
    class Turn1,Turn2,Turn3 subgraphStyle
    linkStyle 4 stroke:#333,stroke-width:2px,fill:#f8f9fa

```
</Wide>

Although this makes sense for  *inference*  (to avoid blowing up the context), we concluded that for  *training*  it is important to  *retain the reasoning tokens across all turns*  in order to condition the model appropriately.

Instead, we decided to craft our own chat template with the following features:

- A structured system prompt, like [Llama 3's](https://huggingface.co/spaces/huggingfacejs/chat-template-playground?modelId=meta-llama%2FLlama-3.1-8B-Instruct) and those [jailbroken from proprietary models](https://github.com/elder-plinius/CL4R1T4S). We also wanted to offer the flexibility to override the system prompt entirely.
- Support for [code agents](https://huggingface.co/learn/agents-course/en/unit2/smolagents/code_agents), which execute arbitrary Python code instead of making JSON tool calls.
- Explicit control of the reasoning mode via the system message.

To iterate on the design of the chat template, we used the [Chat Template Playground](https://huggingface.co/spaces/huggingfacejs/chat-template-playground). This handy application was developed by our colleagues at Hugging Face and makes it easy to preview how messages are rendered and debug formatting issues. Here's an embedded version of the playground so you can try it out directly:

<Wide>

<iframe
	src="https://huggingfacejs-chat-template-playground.hf.space?modelId=HuggingFaceTB%2FSmolLM3-3B&example=hello-world"
	frameborder="0"
	width="100%"
	height="650"
></iframe>
</Wide>



Select different examples from the drop-down to see how the chat template works for multi-turn dialogues, reasoning, or tool-use. You can even change the JSON input manually to enable different behaviour. For example, see what happens if you provide  `enable_thinking: false`  or append  `/no_think`  to the system message.

Once you've settled on some initial datasets and a chat template, it's time to train some baselines!

#### Baby baselines

Before we dive into optimisation and squeezing every point of performance, we need to establish some "baby baselines". These baselines aren't about reaching state-of-the-art (yet), but aim to validate that the chat template does what you want and that the initial set of hyperparameters produce stable training. Only after we have this foundation do we start heavily tuning hyperparameters and training mixtures.

When it comes to training SFT baselines, here's the main things to consider:

- Will you use full fine-tuning (FullFT) or parameter efficient methods like LoRA or QLoRA? As described in the wonderful [blog post](https://thinkingmachines.ai/blog/lora/) by Thinking Machines, LoRA can match FullFT under certain conditions (usually determined by the size of the dataset).
- What type of parallelism do you need? For small models or those trained with LoRA, you can usually get by with data parallel. For larger models you will need FSDP2 or DeepSpeed ZeRO-3 to shared the model weights and optimiser states. For models trained with long context, use methods like [context parallelism](https://huggingface.co/docs/trl/v0.23.0/en/reducing_memory_usage#context-parallelism).
- Use kernels like FlashAttention and Liger if your hardware supports them. Many of these kernels are hosted on the [Hugging Face Hub](https://huggingface.co/models?other=kernel) and can be set via a [simple argument](https://huggingface.co/docs/trl/kernels_hub) in TRL to dramatically lower the VRAM usage.
- Mask the loss to [train only on assistant tokens](https://huggingface.co/docs/trl/sft_trainer#train-on-assistant-messages-only).  As we discuss below, this can be achieved by wrapping the assistant turns of your chat template with a special  `{% generation %}`  keyword.
- Tune the learning rate; aside from the data, this is the most important factor that determines whether your model is "meh" vs "great".
- [Pack the training samples](https://huggingface.co/docs/trl/v0.23.0/en/reducing_memory_usage#packing) and tune the sequence length to match the distribution of your data. This will dramatically speed up training.  TRL has a handy [application](https://huggingface.co/docs/trl/v0.23.0/en/reducing_memory_usage#how-to-choose-the-maxlength-value) to do this for you.

Let's look at how some of these choices panned out for SmolLM3. For our first baseline experiments, we wanted a simple sanity check: does the chat template actually elicit hybrid reasoning? To test this, we compared three data mixtures from our [table](#sft-datasets):

-  **Instruct:**  train on the non-reasoning examples.
-  **Thinking:** train on the reasoning examples.
-  **Hybrid:**  train on all examples.

For each mixture, we ran SFT on [SmolLM3-3B-Base](https://huggingface.co/HuggingFaceTB/SmolLM3-3B-Base) using FullFT with a learning rate of 1e-5, an effective batch size of 128, and trained for 1 epoch.

<Sidenote>

We have found that for most models and datasets, this choice of hyperparameters works well as a baseline.
</Sidenote>

Since these are small datasets, we did not use packing, capping sequences at 8,192 tokens for the Instruct subset and 32,768 tokens for the rest. On one node of 8 x H100s, these experiments were quick to run, taking between 30-90 minutes depending on the subset. The figures below compare the performance of each subset for the corresponding reasoning mode:

<HtmlEmbed src="embeds/d3-sft-baby-baseline.html" />



These results quickly showed us that hybrid models exhibit a type of "split brain", where the data mixture for one reasoning mode has little effect on the other. This is evident by most evals having similar scores between the Instruct, Thinking and Hybrid subsets, with LiveCodeBench v4 and IFEval being the exception where hybrid data boosts the overall performance. 

####  **Vibe-test your baselines** 

Although the evals looked OK, when we tried getting the hybrid model to act in different personas (e.g. like a pirate), it consistently ignored anything we placed in the system message. After a bit of digging, we found the reason was due to the way we had formatted the data: 

<Image src={Screenshot_2025_09_26_at_22_36_40_27a1384e_bcac_8063_94e0_f1c689e7d9b9} alt="Image" />

What happened is that in the design of our chat template, we exposed a  `custom_instructions`  argument to store the system prompts. For example, here's how we set a persona in a dialogue:

```python
from transformers import AutoTokenizer

tok = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM3-3B")

messages = [
    {
        "content": "I'm trying to set up my iPhone, can you help?",
        "role": "user",
    },
    {
        "content": "Of course, even as a vampire, technology can be a bit of a challenge sometimes [TRUNCATED]",
        "role": "assistant",
    },
]
chat_template_kwargs = {
    "custom_instructions": "You are a vampire technologist",
    "enable_thinking": False,
}
rendered_input = tok.apply_chat_template(
    messages, tokenize=False, **chat_template_kwargs
)
print(rendered_input)
## <|im_start|>system
### Metadata

## Knowledge Cutoff Date: June 2025
## Today Date: 28 October 2025
## Reasoning Mode: /no_think

### Custom Instructions

## You are a vampire technologist

## <|im_start|>user
## I'm trying to set up my iPhone, can you help?<|im_end|>
## <|im_start|>assistant
## <think>

## </think>
## Of course, even as a vampire, technology can be a bit of a challenge sometimes # [TRUNCATED]<|im_end|>
```
The issue was that our data samples looked like this:

```python
{
    "messages": [
        {
            "content": "I'm trying to set up my iPhone, can you help?",
            "role": "user",
        },
        {
            "content": "Of course, even as a vampire, technology can be a bit of a challenge sometimes [TRUNCATED]",
            "role": "assistant",
        },
    ],
    "chat_template_kwargs": {
        "custom_instructions": None,
        "enable_thinking": False,
        "python_tools": None,
        "xml_tools": None,
    },
}
```
A bug in our processing code had set  `custom_instructions`  to  `None` , which effectively removed the system message from  *every single training sample*  🙈! So instead of getting a nice persona for these training samples, we ended up with the SmolLM3 default system prompt:

```python
chat_template_kwargs = {"custom_instructions": None, "enable_thinking": False}
rendered_input = tok.apply_chat_template(messages, tokenize=False, **chat_template_kwargs)
print(rendered_input)
## <|im_start|>system
#### Metadata

## Knowledge Cutoff Date: June 2025
## Today Date: 28 October 2025
## Reasoning Mode: /no_think

#### Custom Instructions

## You are a helpful AI assistant named SmolLM, trained by Hugging Face.

## <|im_start|>user
## I'm trying to set up my iPhone, can you help?<|im_end|>
## <|im_start|>assistant
## <think>

## </think>
## Of course, even as a vampire, technology can be a bit of a challenge sometimes [TRUNCATED]<|im_end|>
```
This was especially problematic for the SystemChats subset, where all the personas are defined via  `custom_instructions`  and thus the model had a tendency to randomly switch character mid-conversation. This brings us to the following rule:

<Note title="Rule" emoji="☝️" variant="info">

Always vibe-test your models, even if the evals look fine. More often than not, you will uncover subtle bugs in your training data.
</Note>

Fixing this bug had no impact on the evals, but finally we were confident the chat template and dataset formatting was working. Once your setup is stable and your data pipeline checks out, the next step is to focus on developing specific capabilities.

#### Targeting specific capabilities

During the development of [Open-R1](https://github.com/huggingface/open-r1), we noticed that training a base model entirely on single-turn reasoning data would fail to generalise to multi-turn. This is not a surprise; absent such examples, the model is being tested outside its training distribution. 

To measure this quantitatively for SmolLM3, we took inspiration from Qwen3, who developed an internal eval called  *ThinkFollow* , which randomly inserts  `/think`  or  `/no_think`  tags to test whether the model can consistently switch reasoning modes. In our implementation, we took the prompts from Multi-IF and then checked if the model generated empty or non-empty think blocks enclosed in the  `<think>`  and  `</think>`  tags. As expected, the results from our hybrid baseline showed the model failing abysmally to enable the reasoning mode beyond the first turn:

<HtmlEmbed src="embeds/d3-sft-think-follow-0.html" />



To fix this capability, we constructed a new dataset called IFThink. Based on the Multi-IF pipeline, we used single-turn instructions from [Tulu 3's instruction-following subset](https://huggingface.co/datasets/allenai/tulu-3-sft-personas-instruction-following) and expanded them into multi-turn exchanges using Qwen3-32B to both generate both verifiable instructions and reasoning traces. The method is illustrated below:

<Sidenote>

We considered filtering out conflicting instructions, but the initial results were strong enough to skip this step.
</Sidenote>

```mermaid
flowchart TD
    %% Inputs
    IFEval["Tülu3 IF Dataset"]
    InstructionTypes["Set of instruction types"]
    
    %% English Multi-Turn Generation
    SingleTurn["Single turn prompt"]
    LLM1["Generate instructions with Qwen3-32B"]
    
    subgraph Turns ["Multi-turn prompts"]
        Turn1["Prompt @ turn 1"]
        Turn2["Prompt @ turn 2"]
        Turn3["Prompt @ turn 3"]
    end
    
    MultiTurn["Generate reasoning traces with Qwen3-32B"]
    
    IFThink["IFThink"]
    
    %% Connections
    IFEval --> SingleTurn
    IFEval --> InstructionTypes
    SingleTurn --> Turn1
    InstructionTypes --> LLM1
    LLM1 --> Turn2
    LLM1 --> Turn3
    
    Turn1 --> MultiTurn
    Turn2 --> MultiTurn
    Turn3 --> MultiTurn
    
    MultiTurn --> IFThink
    
    %% Styling
    classDef question fill:#ffd0c5
    classDef decision fill:#f9f9f9
    classDef success fill:#d1f2eb
    classDef danger fill:#fef3c7
    classDef category fill:#fef3c7
    
    class IFEval,InstructionTypes question
    class SingleTurn,LLM1,MultiTurn decision
    class Turn1,Turn2,Turn3 decision
    class IFThink success
```
Including this data in our baseline mix produced a dramatic improvement:

<HtmlEmbed src="embeds/d3-sft-think-follow-1.html" />



After fixing the multi-turn reasoning issue with IFThink, our baseline finally behaved as intended; it could stay consistent across turns, follow instructions, and use the chat template correctly. With that foundation in place, we turned back to the basics: tuning the training setup itself. 

#### Which hyperparameters actually matter?

In SFT, there are only a few hyperparameters that actually matter. Learning rate, batch size, and packing determine almost everything about how efficiently your model trains and how well it generalises. In our baby baselines, we picked reasonable defaults just to validate the data and chat template. Now that the setup was stable, we revisited these choices to see how much impact they have on our baseline. 

 **Masking user turns** 

One subtle design choice for the chat template is whether to mask the user turns during training. In most chat-style datasets, each training example consists of alternating user and assistant messages (possibly with interleaved tool calls). If we train the model to predict all tokens, it effectively learns to autocomplete user queries, rather than focusing on producing high-quality assistant responses. 

As shown in the figure below, masking user turns prevents this by ensuring the model's loss is only computed on assistant outputs, not user messages:

<HtmlEmbed src="/embeds/input-labels-training.html" />



In TRL, masking is applied for chat templates that can return the assistant tokens mask. In practice, this involves including a  `{% generation %}`  keyword in the template as follows:

```javascript
{%- for message in messages -%}
  {%- if message.role == "user" -%}
    {{ "<|im_start|>" + message.role + "\n" + message.content + "<|im_end|>\n" }}
  {%- elif message.role == "assistant" -%}
{% generation %}
{{ "<|im_start|>assistant" + "\n" + message.content + "<|im_end|>\n" }}
{% endgeneration %}
  {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
  {{ "<|im_start|>assistant\n" }}
{%- endif %}
```
Then, when  `apply_chat_template()`  is used with  `return_assistant_tokens_mask=True`  the chat template will indicate which parts of the dialogue should be masked. Here's a simple example, which shows how the assistant tokens are given ID 1, while the user tokens are masked with ID 0:

```python
chat_template = '''
{%- for message in messages -%}
  {%- if message.role == "user" -%}
    {{ "<|im_start|>" + message.role + "\n" + message.content + "<|im_end|>\n" }}
  {%- elif message.role == "assistant" %}
    {% generation %}
    {{ "<|im_start|>assistant" + "\n" + message.content + "<|im_end|>\n" }}
    {% endgeneration %}
  {%- endif %}
{%- endfor %}
{%- if add_generation_prompt %}
  {{ "<|im_start|>assistant\n" }}
{%- endif %}
'''
rendered_input = tok.apply_chat_template(messages, chat_template=chat_template, return_assistant_tokens_mask=True, return_dict=True)
print(rendered_input)
## {'input_ids': [128011, 882, 198, 40, 2846, 4560, 311, 743, 709, 856, 12443, 11, 649, 499, 1520, 30, 128012, 198, 257, 128011, 78191, 198, 2173, 3388, 11, 1524, 439, 264, 51587, 11, 5557, 649, 387, 264, 2766, 315, 264, 8815, 7170, 510, 2434, 12921, 9182, 60, 128012, 271], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'assistant_masks': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```
In practice, masking doesn't have a huge impact on downstream evals and usually provides a few points improvement in most cases. With SmolLM3, we found it had the most impact on IFEval, likely because the model is less inclined to restate the prompt and follow the various constraints more closely. A comparison of how user masking affected each eval and reasoning mode is shown below: 

<HtmlEmbed src="/embeds/d3-sft-mask-turns.html" />



 **To pack or not to pack?** 

As we Sequence packing is one of the training details that makes a huge difference to training efficiency. In SFT, most datasets contain samples of variable length, which means each batch contains a large number of padding tokens that waste compute and slow convergence. 

Packing solves this by concatenating multiple sequences together until a desired maximum token length is achieved. There are various ways to perform the concatenation, with TRL adopting a "best-fit decreasing" strategy [[@bfd](https://huggingface.co/papers/2404.10830)], where the ordering of sequences to pack is determined by their length. As shown below, this strategy minimises truncation of documents across batch boundaries, while also reducing the amount of padding tokens:

<Wide>

<HtmlEmbed frameless src="/embeds/data-packing.html" />
</Wide>



<Note title="Packing in post-training vs pre-training" variant="info">

In pre-training, this isn't really a question. When training on trillions of tokens, packing is essential to avoid wasting significant amounts of compute on padding. Pretraining frameworks like Megatron-LM and Nanotron implement packing by default. Post-training is different. Because the runs are shorter, the trade-offs change.
</Note>

To get a sense of how efficient packing is for training, below we compare the runtimes between packing and no-packing over one epoch of our baseline dataset:

<Sidenote>

The reason the runtime flattens out after an effective batch size of 32 is because this is the largest size possible without invoking gradient accumulation.
</Sidenote>

<HtmlEmbed src="/embeds/d3-sft-packing-runtime.html" />



Depending on the batch size, we see that packing improves throughput by a factor of 3-5x! So, should you  *always*  use packing? To some extent, the answer depends on how large your dataset is, because packing reduces the number of optimisation steps per epoch by fitting more tokens into each step. You can see this in the following figure, where we plot the average number of non-padding tokens per batch:

<HtmlEmbed src="/embeds/d3-sft-packing-tokens.html" />



With packing, the number of tokens per batch scales linearly with the batch size and compared to training without packing, can include up to 33x more tokens per optimisation step! However, packing can slightly alter the training dynamics: while you process more data overall, you take fewer gradient updates which can influence final performance, especially on small datasets where each sample matters more. For example, if we compare packing versus no-packing at the same effective batch size of 128, we see that some evals like IFEval take a significant performance hit of nearly 10 percentage points:

<HtmlEmbed src="/embeds/d3-sft-packing-vs-no-packing.html" />



More generally, we see that once the effective batch size is large than 32, there is an average drop in performance for this particular model and dataset:

<HtmlEmbed src="/embeds/d3-sft-packing-vs-bs.html" />



In practice, for large-scale SFT where the dataset is massive, packing is almost always beneficial since the compute savings far outweigh any minor differences in gradient frequency. However, for smaller or more diverse datasets—like domain-specific fine-tuning or instruction-tuning on limited human-curated data—it might be worth disabling packing to preserve sample granularity and ensure every example contributes cleanly to optimisation.

Ultimately, the best strategy is empirical: start with packing enabled, monitor both throughput and downstream evals, and adjust based on whether the speed gains translate into equivalent or improved model quality.

 **Tuning the learning rate** 

We now come to the last, but still important hyperparameter: the learning rate. Set it too high and training may diverge; too low and convergence is painfully slow. 

In SFT, the optimal learning rate is typically an order of magnitude (or more) smaller than the one used during pretraining. This is because we're initialising from a model with rich representations, and aggressive updates can lead to catastrophic forgetting. 

<Note title="Tuning the learning rate in post-training vs pretraining" variant="info">

Unlike pre-training, where hyperparameter sweeps on the full run are prohibitively expensive, post-training runs are short enough that we can actually do full learning rate sweeps.
</Note>

In our experiments, we've found that the "best" learning rate varies with both model family, size and the use of packing. Since a high learning rate can lead to exploding gradients, we find it's often safer to slightly decrease the learning rate when packing is enabled. You can see this below, where using a small learning rate of 3e-6 or 1e-5 gives better overall performance than large values:

<Sidenote>

When picking a range of learning rate values to scan over, we find is useful to pick an initial range like [1e-6, 3e-6, 1e-5, 3e-5, 1e-4]. This covers two orders of magnitude and allows us to hone in on a region where some additional tuning can be applied
</Sidenote>

<HtmlEmbed src="/embeds/d3-sft-lr-scan.html" />



Although a few points on average may not seem like much, if you look at individual benchmarks like AIME25, you'll see the performance drop dramatically when the learning rate is larger than 1e-5. 

 **Scaling the number of epochs** 

In our ablations, we usually train for a single epoch to iterate quickly. Once you've identified a good data mixture and tuned key parameters like the learning rate, the next step is to increase the number of epochs for final training.

For example, if we take our baseline data mixture and train for five epochs, we see it is possible to squeeze a few more percentage points of performance on average:

<HtmlEmbed src="/embeds/d3-sft-epoch-scan.html" />



As we saw with the learning rate scan, the average performance obscures the impact that scaling the number of epochs has on individual evals: in the case of LiveCodeBench v4 with extended thinking, we nearly double the performance over one epoch!

Once you've iterated on your SFT data mixture and the model has reached a reasonable level of performance, the next step is often to explore more advanced methods like [preference optimisation](#from-sft-to-preference-optimisation:-teaching-models-what- *better-* means) or [reinforcement learning](#going-online-and-beyond-supervised-labels). However, before diving into those, it's worth considering whether the additional compute would be better spent on strengthening the base model through  *continued pretraining.* 

<Note title="Optimisers in post-training" emoji="📍" variant="info">

Another important component we mentioned in the pre-training section is the optimiser. Similarly, AdamW remains the default choice for post-training. An open question is whether models pre-trained with alternative optimisers like Muon should be post-trained with the  *same*  optimiser. The Kimi team found that using the same optimise for pre- and post-training yielded best performance for their [Moonlight](https://arxiv.org/abs/2502.16982) model.
</Note>

#### Boosting reasoning through continued pretraining

Continued pretraining—or mid-training if you want to sound fancy—means taking a base model and training it further on large amounts of domain-specific tokens before doing SFT. Mid-training is useful when your target capabilities for SFT share a common core skill, such as coding or reasoning. In practice, this shifts the model toward a distribution that better supports reasoning, a specific language, or any other capability you care about. Starting SFT from a model that has already integrated that core skill allows your model to better focus on the specific topics in your SFT data rather than using compute to learn the core skill from scratch.

The mid-training approach traces back to ULMFit [[@ulmfit](https://arxiv.org/abs/1801.06146)], which pioneered the three-stage pipeline of general pretraining → mid-training → post-training that is now common in modern LLMs like FAIR's Code World Model [[@cwm](https://huggingface.co/papers/2510.02387)]:

<Wide>

<HtmlEmbed frameless src="embeds/training-pipeline.html"/>
</Wide>



This approach was also used in the training of Phi-4-Mini-Reasoning [[@phi4reasoning](https://huggingface.co/papers/2504.21233)], but with a twist: instead of doing continued pretraining on web data, the authors used distilled reasoning tokens from DeepSeek-R1 for the mid-training corpus. The results were compelling, showing consistent and large gains through multi-stage training: 

| Model | AIME24 | MATH-500 | GPQA Diamond |
| --- | --- | --- | --- |
| **Phi-4-Mini** | 10.0 | 71.8 | 36.9 |
| + Distill Mid-training | 30.0 | 82.9 | 42.6 |
| + Distill Fine-tuning | 43.3 | 89.3 | 48.3 |
| + Roll-Out DPO | 50.0 | 93.6 | 49.0 |
| + RL (Phi-4-Mini-Reasoning) | **57.5** | **94.6** | **52.0** |

These results prompted us to try a similar approach. From our prior experience with creating and evaluating reasoning datasets in Open-R1, we had three main candidates to work with:

- [ **Mixture of Thoughts** ](https://huggingface.co/datasets/open-r1/Mixture-of-Thoughts) **:** 350k reasoning samples distilled from DeepSeek-R1 across math, code, and science.
- [ **Llama-Nemotron-Post-Training-Dataset:** ](https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset) NVIDIA's large-scale dataset of distilled from a wide variety of models such as Llama3 and DeepSeek-R1. We filtered the dataset for the DeepSeek-R1 outputs, which resulted in about 3.64M samples or 18.7B tokens.
- [ **OpenThoughts3-1.2M:** ](https://huggingface.co/datasets/open-thoughts/OpenThoughts3-1.2M) one of the highest-quality reasoning datasets, with 1.2M samples distilled from QwQ-32B, comprising 16.5B tokens.

Since we planned to include reasoning data in the final SFT mix, we decided to keep Mixture of Thoughts for that stage the others for mid-training. We used ChatML as the chat template to avoid "burning in" the SmolLM3 one too early on. We also trained for 5 epochs with a learning rate of 2e-5, using 8 nodes to accelerate training with an effective batch size of 128.

<Note title="When to mid-train?" emoji="📍" variant="info">

You might wonder why we're discussing mid-training  *after*  we did some SFT runs. Chronologically, mid-training happens before SFT on the base model. But the decision to do mid-training only becomes clear after you've run initial SFT experiments and identified performance gaps. In practice, you'll often iterate: run SFT to identify weak areas, then do targeted mid-training, then run SFT again. Think of this section as <b>"what to do when SFT alone isn't enough."</b>
</Note>

 **The mystery of the melting GPUs** 

Running these experiments turned out to be a surprising challenge on our cluster: the aging GPUs would get throttled at various points which would lead to hardware failures and forced restarts of each run. To give you a taste of what it was like, here's the logs from one of the runs, where each colour represents a restart:

<Image src={GtU8DnoWsAAruEG_28e1384e_bcac_8051_8122_ed6cacf8f632} alt="Image" />

We initially thought DeepSpeed might be the culprit, since the accelerator is highly optimised for throughput. To test this, we switched to DP, which helped somewhat, but then the loss was dramatically different!

<Sidenote>

Finding bugs in your code at midnight is more common than you might think. In hindsight, for long runs at this scale, it would have made more sense to use nanotron since it was battle-tested and had faster throughput.
</Sidenote>

<Image src={Screenshot_2025_10_01_at_11_31_19_28e1384e_bcac_8005_8c5e_f0af3bf70372} alt="Image" />

As we later discovered, a bug with DP in Accelerate meant that the weights and gradients were stored in the model's native precision (BF16 in this case), which led to numerical instability and loss of gradient accuracy during accumulation and optimisation.

<Sidenote>

To prevent this, most accelerators use FP32 for the "master weights" and optimiser states, and only cast back to BF16 for the forward and backward passes.
</Sidenote>

So we switched back to DeepSpeed and added aggressive checkpointing to minimise the time lost from GPUs overheating and "falling off the bus". This strategy proved successful and is something we recommend more generally:

<Note title="Rule" emoji="☝️" variant="danger">

As we emphasized in pre-training, save model checkpoints frequently during a training run, and ideally push them to the Hugging Face Hub to avoid accidental overwrites. Also, make your training framework robust to failures and capable of automatic restarts. Both of these strategies will save you time, especially for long-running jobs like mid-training ones.
</Note>

After babysitting the runs for a week or so, we finally had our results:

<HtmlEmbed src="/embeds/d3-sft-mid-train.html" />



Overall, we found that NVIDIA's post-training dataset gave better performance than OpenThoughts, but the combination was best overall. 

Now let's take a look at the effect of taking one of these checkpoints and applying our same baseline data mixture:

<HtmlEmbed src="/embeds/d3-sft-base-vs-mid.html" />



The effect of using a mid-trained reasoning model instead of the pre-trained one are dramatic: with extended thinking, we nearly triple the performance on AIME25 and LiveCodeBench v4, while GPQA-D receives a full 10 point gain. Somewhat surprisingly, the reasoning core also translated partially to the  `/no_think`  reasoning mode, with about 4-6 point improvements on the reasoning benchmarks. These results gave us clear evidence that for reasoning models, it almost always makes sense to perform some amount of mid-training if your base model hasn't already seen a lot of reasoning data during pre-training.

<Note title="When not to mid-train" emoji="📍" variant="info">

Mid-training shines when your model must learn a new, core skill. It's less useful when the base model already has the skill or if you're trying to elicit shallow capabilities like style or conversational chit-chat. In these cases, we recommend skipping mid-training and allocating your compute to other methods like preference optimisation or reinforcement learning.
</Note>

Once you're confident in your SFT data mixture and the model's broad capabilities, the focus naturally shifts from learning skills to refining them. In most cases, the most effective way forward is  *preference optimisation.* 

### From SFT to preference optimisation: teaching models what  *better*  means

Although you can keep scaling SFT with more data, at some point you'll observe diminishing gains or failure modes like your model being unable to fix its own buggy code. Why?  Because SFT is a form of  *imitation learning* , and so the model only learns to reproduce patterns in the data it was trained on. If the data doesn't already contain good fixes, or if the desired behaviour is hard to elicit with distillation, the model has no clear signal for what counts as "better".

<Sidenote>

The problem persists even if your dataset contains an even mix of traces (i.e. some that reach the correct solution immediately, and others where the model first makes a mistake and corrects it). In this case, the model may simply learn that making an initial error is part of the desired pattern. What we actually want, of course, is a model that can produce the correct solution from the start.
</Sidenote>

This is where preference optimisation comes in. Instead of just copying demonstrations, we give the model comparative feedback like "response A is better than response B". These preferences provide a more direct training signal for quality and enable to model performance to scale beyond the limits of SFT alone. 

Another benefit of preference optimisation is that you typically need far less data than SFT, since the starting point is already a pretty good model that can follow instructions and has knowledge from previous training stages.

Let's take a look at how these datasets are created.

#### Creating preference datasets

Historically, preference datasets were created by providing human annotators with pairs of model responses and asking them to grade which one is better (possibly on a scale). This approach is still used by LLM providers to collect  *human preference*  labels, but it is extremely expensive and scales poorly. Recently, LLMs have become capable of producing high-quality responses, and often in a cost-effective way. These advances make it practical for LLMs to  *generate*  preferences for many applications. In practice, there are two common approaches:

 **Strong vs. weak** 

1. Take a fixed set of prompts  $x$  (often curated for coverage and difficulty).
1. Generate one response from a weaker or baseline model, and another from a high-performing model.
1. Label the stronger model's output as the chosen response  $y_c$  and the weaker one as rejected  $y_r$ .

This produces a dataset of "stronger vs. weaker" comparisons  $(\lbrace{x,y_c,y_r\rbrace})$  , which is simple to construct because we assume the stronger model's output is reliably better.

Below is a popular example from Intel, who took an SFT dataset with responses from gpt-3.5 and gpt-4 and converted it into a preference dataset by selecting the gpt-4 responses as chosen and the gpt-3.5 ones as rejected:

<iframe
  src="https://huggingface.co/datasets/Intel/orca_dpo_pairs/embed/viewer/default/train"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>
 **On-policy with grading** 

1. Use the  *same model*  you will train to generate multiple candidate responses to the same prompt. This creates data that is "on-policy" because it reflects the distribution of outputs the model would naturally produce.
1. Instead of relying on a stronger model as the reference, introduce an  *external grader:*  either a verifier or a reward model that scores responses along one or more quality axes (e.g., helpfulness or factual accuracy).
1. The grader then assigns preference labels among the candidate responses, producing a more nuanced and flexible preference dataset.

This method allows ongoing bootstrapping of preference data as the model improves, but its quality depends heavily on the evaluator's reliability and calibration.

A nice example of such a dataset is from SnorkelAI, who took the prompts from a popular preference dataset called [UltraFeedback](https://huggingface.co/datasets/openbmb/UltraFeedback), partitioned them into 3 sets, and then applied the above recipe iteratively to improve their model:

<iframe
  src="https://huggingface.co/datasets/snorkelai/Snorkel-Mistral-PairRM-DPO-Dataset/embed/viewer/default/train_iteration_1"
  frameborder="0"
  width="100%"
  height="560px"
></iframe>
At the time of SmolLM3's development, there did not exist any preference data with reasoning traces, so we decided to generate some of our own using the "strong vs weak" approach. We used the prompts from Ai2's Tulu 3 preference mixture to generate responses from Qwen3-0.6B and Qwen3-32B in the  `/think`  mode. The result was a [large-scale dataset of 250k+ LLM-generated preferences](https://huggingface.co/datasets/HuggingFaceTB/smoltalk2/viewer/Preference/tulu_3_8b_pref_mix_Qwen3_32B_Qwen3_0.6B_think), ready to simultaneously improve our SFT checkpoint across multiple axes using preference optimisation algorithms.

#### Which algorithm do I pick?

Direct Preference Optimization (DPO) [[@dpo](https://arxiv.org/abs/2305.18290)] was the first preference optimisation algorithm to gain widespread adoption in open source.

<Sidenote>

When the DPO paper came out in mid-2023, there was heated debate online about whether it could match RL methods, and there were no recipes showing it's effectiveness in industrial settings. To address that, we released [Zephyr 7B](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) a few months later, training the entire model on synthetic data and showing significant performance gains from DPO.
</Sidenote>

Its appeal came from being simple to implement, stable in practice, and effective even with modest amounts of preference data. As a result, DPO has become the default method to improve SFT models before reaching for more complex techniques like RL.

But researchers quickly discovered there are many ways to improve upon DPO, and nowadays there is a wide variety of alternatives to explore. Below we list a few of the ones we've found most effective:

-  **Kahneman-Tversky Optimisation (KTO) [** [ **@kto** ](https://huggingface.co/papers/2402.01306) **]:** Instead of relying on preference pairs, KTO models whether an individual response is "desirable or not", using ideas from human decision making. This is a good choice if you don't have access to paired preference data (e.g. raw responses like 👍 or 👎 collected from end-users).
-  **Odds Ratio Preference Optimisation (ORPO) [** [ **@orpo** ](https://huggingface.co/papers/2403.07691) **]:** integrates preference optimisation directly into SFT by adding an odds ratio to the cross-entropy loss. As a result there is no need for a reference model or SFT stage, which makes this method more computationally efficient. 
-  **Anchored Preference Optimisation (APO) [** [ **@apo** ](https://huggingface.co/papers/2408.06266) **]:** this is a more controllable objective the explicitly regularises how much the model's likelihoods for chosen vs. rejected outputs should shift, rather than just optimising their difference. There are two variants (APO-zero and APO-down) who's choice depends on the relationship between your model and the preference data, i.e. whether the chosen outputs are better than the model or worse.

Luckily, many of these choices are just a one-line change in TRL's  `DPOTrainer` , so for our initial baseline we did the following:

- Use the prompts and completions from Ai2's [Tülu3 Preference Personas IF dataset](https://huggingface.co/datasets/allenai/tulu-3-pref-personas-instruction-following) to measure the improvements for instruction-following on IFEval with the  `/no_think`  reasoning mode.
- Re-use the prompts from the above, but now generate "strong vs. weak" preference pairs with Qwen3-32B and Qwen3-0.6B. This gave us preference data for the  `/think`  reasoning mode.
- Train for 1 epoch and measure the  *in-domain*  improvements on IFEval, along with the  *out-of-domain*  impact on other evals like AIME25 which are directly correlated with instruction-following.

As shown in the figure below, the in-domain improvements for both reasoning modes were significant: on IFEval, APO-zero improved over the SFT checkpoint by 15-20 percentage points! 

<HtmlEmbed src="/embeds/d3-po-loss-ablations.html" />



Since APO-zero also had the best overall out-of-domain performance, we settled on using it for the remainder of our ablations.

<Note title="Preference optimisation works for reasoning" emoji="📍" variant="info">

As our results above show, preference optimisation doesn't just make models more helpful or aligned, it teaches them to  *reason better* . If you need a quick way to improve your reasoning model, try generating strong-vs-weak preferences and ablate different loss functions: you may find significant gains over vanilla DPO!
</Note>

#### Which hyperparameters matter most for preference optimisation?

For preference optimisation, there are typically only three hyperparameters that impact the training dynamics:

- The learning rate, typically a factor of 10-100x smaller than the one used for SFT.
- The β parameter, which typically controls the size of the margin between preference pairs.
- The batch size.

Let's take a look at how these played out for SmolLM3, starting from the [SFT checkpoint](https://huggingface.co/HuggingFaceTB/SmolLM3-3B-checkpoints/tree/it-SFT) we trained over the whole of  `smoltalk2` .

 **Use small learning rates for best performance** 

The first ablation we ran was to check the influence of the learning rate on model performance. We ran experiments to determine the influence of learning rates between ~200x smaller (1e-7) and ~2x smaller (1e-5) than the SFT learning rate (2e-5). Previous projects like Zephyr 7B had taught us that the best learning rate for preference optimisation methods is around 10x smaller than the one used for SFT, and the ablations we ran for SmolLM3 confirmed this rule of thumb. 

As shown in the figure below, learning rates ~10x smaller improve the performance of the SFT model in both reasoning modes, but all learning rates beyond that 10x limit result in worse performance for the extended thinking mode:

<HtmlEmbed src="/embeds/d3-po-lr-ablation.html" />



The trend for the  `/no_think`  reasoning mode is more stable, with the best learning rate at 5e-6. This is mostly driven by a single benchmark (LiveCodeBench v4), so we opted for 1e-6 in our SmolLM3 runs. 

Our recommendation for your training runs is to run scans of your learning rate at a range of 5x to 20x smaller than your SFT learning rate. It is highly likely that you will find your optimal performance within that range!

 **Tune your β** 

The experiments we ran for the ß parameter ranged from 0.01 to 0.99 to explore values that encourage different degrees of alignment to the reference model. As a reminder, lower values of beta encourage staying close to the reference model while higher values allow the model to match the preference data more closely. The model performance for β=0.1 is the highest for both reasoning modes and improves compared to the metrics from the SFT checkpoint. Using a low beta value hurts model performance and results in a worse model than the SFT checkpoint, while performance remains stable across multiple ß values without extended thinking.

These results suggest that values greater than 0.1 are preferable for preference optimisation, and that aligning the model with the preference data is more beneficial than staying close to the reference model. However, we suggest exploring ß values in the range 0.01 and 0.5. Higher values may erase capabilities from the SFT checkpoint that we might not be capturing in the evals shown on the plot.

<HtmlEmbed src="/embeds/d3-po-beta-ablation.html" />



 **Scaling the preference data** 

We also ran experiments to determine how dataset size influences results, testing values from 2k to 340k preference pairs. Across this range, performance remained stable. Performance drops in the extended thinking mode occur for datasets beyond 100k preference pairs, but the drop is not as pronounced as we saw with different learning rate values. The dataset we used for the SmolLM3 training run was 169k preference pairs, but the results show that smaller datasets also show improvements over the SFT checkpoint. For future projects, we know we can experiment with smaller datasets during the iteration phase, as it is important to try multiple ideas and quickly identify the most promising configurations.

<HtmlEmbed src="/embeds/d3-po-size-ablation.html" />



 **Bringing it all together** 

Bringing all these threads together produced the final SmolLM3-3B model: best-in-class for its size and sat on the Pareto front with Qwen's own hybrid reasoning models. 

<Wide>

<HtmlEmbed src="embeds/instruct-models-without-reasoning.html" title="Instruct models without reasoning"/>
</Wide>



Not too shabby for a few weeks work!

#### Rules of engagement

To summarise our findings about preference optimisation that could be useful for your future projects:

- Don't be afraid to create your own preference data! With inference becoming "too cheap to meter", it's nowadays simple and cost-effective to generate LLM preferences from various [inference providers](https://huggingface.co/docs/inference-providers/en/index).
- Pick DPO as your initial baseline and iterate from there. We've found that depending on the type of preference data, other algorithms like ORPO, KTO, or APO can provide significant gains over DPO.
- Use a learning rate that's around 10x smaller than the one used for SFT.
- Scan over β, usually in the range 0.01 to 0.5
- Since most preference algorithms overfit after one epoch, partition your data and train iteratively for best performance.

Preference optimisation is often the sweet spot between simplicity and performance, but it still inherits a key limitation: it's only as good as the offline preference data you can collect. At some point, static datasets run out of signal and you need methods that can generate fresh training feedback online as the model interacts with prompts and environment. That's where preference optimisation meets the broader family of  *on-policy and RL-based methods.* 

### Going on-policy and beyond supervised labels

If you want your model to consistently solve math problems, generate executable code, or plan across multiple steps, you often need a  **reward signal**  rather than just "A is better than B".

This is where RL starts to make sense. Instead of supervising the model with preferences, you let it interact with an environment (which could be a math verifier, a code executor, or even real user feedback), and learn directly from the outcomes. RL shines when:

-  **You can check correctness automatically,** e.g., unit tests, mathematical proofs, API calls, or have access to a high-quality verifier or reward model.
-  **The task requires multi-step reasoning or planning** , where local preferences may not capture long-term success.
-  **You want to optimise for objectives beyond preference labels** , like passing unit tests for code or maximising some objective.

When it comes to LLMs, there are two main flavours of RL:

-  **Reinforcement Learning from Human Feedback (RLHF):**  this is the approach that was popularised by OpenAI's InstructGPT paper [[@instructgpt](https://huggingface.co/papers/2203.02155)] and the basis for gpt-3.5 and many modern LLMs. Here, human annotators compare model outputs (e.g. "A is better than B") and a reward model is trained to predict those preferences. The policy is then fine-tuned with RL to maximise the learned reward.

<Sidenote>

Because the reward model only approximates human preferences, it can sometimes encourage reward hacking, where the policy emits an out-of-distribution sequence like "the the the the" which is given a spurious high reward and is then baked into the model via the RL loop.
</Sidenote>

-  **Reinforcement Learning with Verifiable Rewards (RLVR):** this is the approach that was popularised by DeepSeek-R1 and involves the use of verifiers that check whether a model's output meets some clearly defined correctness criteria (e.g. does the code compile and pass all tests, or is the mathematical answer correct?). The policy is then fine-tuned with RL to produce more verifiably-correct outputs. 

Both RLHF and RLVR define  *what* the model is being optimised for, but they don't tell us  *how*  that optimisation should be carried out. In practice, the efficiency and stability of RL-based training depends heavily on whether the learning algorithm is  **on-policy** or  **off-policy** .

Methods such as GRPO typically fall into the category of on-policy optimisation algorithms, where the model (the policy) that generates the completions is the same as the one being optimised. While it is broadly the case that GRPO is an on-policy algorithm, there are a few caveats. First, to optimise the generation step, several batches of generations may be sampled and then  $k$  updates are made to the model, with the first batch being on-policy with the next few batches being slightly off-policy. 

To account for policy-lag between the model used for generation and the current model being optimised, importance sampling and clipping are used to re-weight the token probabilities and restrict the size of the updates.

<Sidenote>

We mention here off-policy RL, but there are several truly off-policy RL algorithms, such as Q-learning, where the policy used for generate trajectories can be totally different to the policy being optimized. When GRPO is applied to LLMs, the policy used for generation can lag behind the one used for optimization, but typically there are less than 16 steps difference between the two.
</Sidenote>

As autoregressive generation from LLMs is slow, many frameworks like [verl](https://github.com/volcengine/verl) and [PipelineRL](https://github.com/ServiceNow/PipelineRL) have added asynchronous generation of completions and "in-flight" updates of model weights to maximise training throughput. These approaches require more complex and careful implementation, but can achieve training speeds that are 4-5x higher than synchronous training methods. As we'll see later, these improvements in training efficiency are especially pronounced for reasoning models, which have long-tail token distributions.

For SmolLM3, we skipped RL altogether, mostly due to time constraints and having a model that was already best-in-class with offline preference optimisation. However, since the release, we have revisited the topic and will close out the post-training chapter by sharing some of our lessons from applying RLVR to hybrid reasoning models.

#### Applying RLVR to hybrid reasoning models

Hybrid reasoning models pose additional complexity for RLVR because generation lengths vary considerably depending on the reasoning mode. For example, in the figure below, we plot the token length distributions on AIME25 for the [final APO checkpoint](https://huggingface.co/HuggingFaceTB/SmolLM3-3B-checkpoints/tree/it-soup-APO) from SmolLM3:

<HtmlEmbed src="/embeds/d3-rl-token-histogram.html" />



As you can see, the  `/no_think`  mode generates solutions with a median length of around 2k tokens, while the  `/think`  mode is much larger with 16k tokens and a fat-tailed distribution. Ideally, we would like to improve the overall performance of both modes with RLVR, without changing their respective length distributions too radically.

To explore this, we focused on optimising the  `/no_think`  mode first and took a subset of prompts from [Big-Math](https://huggingface.co/datasets/SynthLabsAI/Big-Math-RL-Verified), a dataset of over 250k math problems with verified answers.

To our surprise, naively applying GRPO leads to a form of  *reward hacking* : despite never being prompted to emit a long CoT, the model learns to exploit its base reasoning capabilities to increase the reward. As shown in the figure below, not only does the reward go up, but so too does the length of the completions:

<HtmlEmbed src="embeds/d3-rl-full-length.html" caption="Naively applying RLVR to SmolLM3 achieves high reward and improved downstream performance, but comes at the cost of exploding completion lengths."/>



In other words, RLVR with GRPO has converted the  `/no_think`  mode of our reasoning model into one which looks very similar to the  `/think`  mode!

When we review the model's completions, we see that the model now generates a long CoT and even includes some of the cognitive behaviours [@cognitivebehaviours] like  "Wait, …"  that are associated with reasoning models. An example of this is shown below:

```
However, since the jogger and the train are moving in the same direction, the relative speed between them is the difference of their speeds:
\\( v_{\\text{rel}} = v_t - v_j = 12.7778 - 2.7778 = 10 \\) m/s

 **Wait,**  let me double-check the calculation:

 **Wait,**  46 km/hr to m/s:
46 * 1000 m / 3600 s = (46 * 10)/36 = 460/36 ≈ 12.7778 m/s (correct)
10 km/hr is:
10 * 1000 / 3600 = 10000 / 3600 ≈ 2.7778 m/s (correct)
Difference: 12.7778 - 2.7778 = 10 m/s (exact, since 46-10=36 km/hr → 36*(1000/3600)=10 m/s exactly)

The jogger is 340 meters ahead of the train's engine. When the train passes the jogger, the distance the train needs to cover relative to the jogger is 340 meters plus the length of the train? Wait, actually, let me think about the problem again.

 **Wait,**  actually, when the problem says the train takes 46 seconds to pass the jogger, that usually means the time from the front of the train to the end passing the jogger. The length of the train would then be equal to the relative speed multiplied by the time.

But first, the initial position: the jogger is 340 meters ahead of the engine. So the engine is 340 meters ahead of the jogger. Wait no, the wording says "340 m ahead of the engine", so the jogger is ahead by 340 meters. So when the jogger is running alongside, the engine of the train is 340 m away behind the jogger? Wait, maybe I need to clarify.

 **Wait**  the problem says: "A jogger running at 10 km/hr alongside a railway track is 340 m ahead of the engine of a train running at 46 km/hr in the same direction. The train takes 46 seconds to pass the jogger. How long is the train in meters?"

Hmm, so the jogger is 340 m ahead of the engine along the track. Since they're moving in the same direction, the train is behind the jogger by 340 meters. To pass the jogger, the train has to cover that 340 meters plus the length of the train itself?
```
 **Mitigating reward hacking with overlong penalties** 

This issue can mitigated by including an  *overlong completion penalty* , that penalises completions over a certain length. The penalty is parameterised by two arguments max completion length  $L_{max}$  and soft punishment cache  $L_{cache}$ . This penalty was one of the improvements proposed in the DAPO paper [[@dapo](https://huggingface.co/papers/2503.14476)] and amounts to applying a reward function as follows:

$$

R_{\text{length}}(y) = \begin{cases}
0, & |y| \le L_{\text{max}} - L_{\text{cache}} \\
\frac{(L_{\text{max}} - L_{\text{cache}} - |y|)}{L_{\text{cache}}}, & L_{\text{max}} - L_{\text{cache}} < |y| \le L_{\text{max}} \\
-1, & L_{\text{max}} < |y|
\end{cases}

$$

Using this penalty, we can directly control the model's output distribution and measure the tradeoff between increasing response length and performance. An example is shown in the figure below, where we vary the overlong penalty from 1.5k to 4k in steps of 512 tokens:

<HtmlEmbed src="/embeds/d3-rl-reward-curves.html" caption="Applying an overlong penalty constrains the length of each rollout, while also reducing the average reward." />



The tradeoff between response length and performance is clearer when we examine the improvements on AIME25:

<HtmlEmbed src="/embeds/d3-rl-aime25.html" caption="Downstream performance of Smollm3 with RLVR on AIME25."/>



Now we can clearly see how the overlong penalty impacts downstream performance, with penalties in the range 2-4k producing significant improvements, while keeping the token distribution in check. As shown in the figure below, if we take the checkpoints from step 400, we can compare the output token distributions between the initial policy and final model across a range of different penalties:

<HtmlEmbed src="/embeds/d3-rl-token-comparison.html" />



 **Bringing it all together** 

We find that applying a length penalty in the range 2.5-3k gives the best tradeoff between performance and response length, with the figure below showing that GRPO nearly doubles the performance on AIME 2025 over offline methods like APO:

<HtmlEmbed src="/embeds/d3-rl-aime25-comparisons.html" />



Now that we know how to improve performance in the  `/no_think`  reasoning mode, the next step in the RL training pipeline would be  *joint training*  of the model in both reasoning modes at once. However, we have found this to be quite a tough nut to crack because each mode requires it's own length penalty and the interplay has thus far produced unstable training. This highlights the main challenge with trying to apply RL on hybrid reasoning models, and we can see this reflected in a new trend from model developers like Qwen to release the [instruct](https://huggingface.co/Qwen/Qwen3-4B-Instruct-2507) and [reasoning](https://huggingface.co/Qwen/Qwen3-4B-Thinking-2507) variants separately.

Our experiments show that RLVR can steer reasoning behaviour effectively, but only with careful reward shaping and stability mechanisms. Given this complexity, it's worth asking whether reinforcement learning is the only viable path forward. In fact, several lighter-weight, on-policy optimisation strategies have been proposed in recent literature, yet remain surprisingly under explored by the open-source community. Let's close out this chapter by taking a look at some of them.

#### Is RL the only game in town?

Other approaches to on-policy learning extend preference optimisation and distillation into iterative loops that refresh the training signal as the model evolves:

-  **Online DPO:**  rather than training once on a fixed preference dataset, the model continually samples new responses, collects fresh preference labels (from reward models or LLM graders), and updates itself. This keeps the optimisation  *on-policy*  and reduces drift between training data and the model's current behaviour [@onlinedpo].
-  **On-policy distillation:**  instead of preferences, the signal comes from a stronger teacher model. The student samples responses at every training step and the KL divergence between the student and teacher logits on these samples provides the learning signal. This allows the student to continuously absorb the teacher's capabilities, without needing explicit preference labels or verifiers [@gkd].

These methods blur the line between static preference optimisation and full RL: you still get the benefits of adapting to the model's current distribution, but without the full complexity of designing and stabilising a reinforcement learning loop.

#### Which method do I pick?

Although there are a gazillion research papers about which on-policy method is "best", in practice the decision depends on a few factors shown in the table below:

<Wide>

| Algorithm | When to Use | Tradeoffs | Best for Model Size |
| --- | --- | --- | --- |
| **Online DPO** | You can get preference labels cheaply. Best for aligning behaviour with evolving distributions. | Easy to scale iteratively, more stable than RL, but depends on label quality and coverage. Supported in few training frameworks. | Any size, where preferences capture improvements beyond imitation. |
| **On-policy distillation** | You have access to a stronger teacher model and want to transfer capabilities efficiently. | Simple to implement, cheap to run, inherits teacher biases, ceiling limited by teacher. Supported only in TRL and NemoRL | Most effective for small to mid-sized models (\<30B). |
| **Reinforcement learning** | Best when you have verifiable rewards or tasks requiring multi-step reasoning/planning. Can be used with reward models, but there are challenges like reward-hacking, where the model takes advantage in weaknesses in the reward model. | Flexible and powerful, but costly and harder to stabilise; requires careful reward shaping. Supported in most post-training frameworks. | Mid to large models (20B+), where extra capacity lets them exploit structured reward signals. |
</Wide>

In the open-source ecosystem, reinforcement learning methods like GRPO and REINFORCE tend to be the most widely used, although the Qwen3 tech report [[@qwen3](https://arxiv.org/abs/2505.09388)] highlighted the use of on-policy distillation to train the models under 32B parameters:

<Wide>

```mermaid

flowchart LR
    subgraph Flagship ["Flagship Models"]
        Base1["Base Models"] --> Stage1["Stage 1:<br/>Long-CoT Cold Start"]
        Stage1 --> Stage2["Stage 2:<br/>Reasoning RL"]
        Stage2 --> Stage3["Stage 3:<br/>Thinking Mode Fusion"]
        Stage3 --> Stage4["Stage 4:<br/>General RL"]
        Stage4 --> FlagshipOut["Qwen3-235B-A22B<br/>Qwen3-32B"]
    end
    
    subgraph Lightweight ["Lightweight Models"]
        Base2["Base Models"] --> Distillation["Strong-to-Weak<br/>Distillation"]
        FlagshipOut --> Distillation
        Distillation --> LightweightOut["Qwen3-30B-A3B<br/>14B/8B/4B/1.7B/0.6B"]
    end
    
    classDef flagshipStage fill:#ffd0c5
    classDef lightweightStage fill:#fef3c7
    classDef output fill:#f8d7da
    
    class Stage1,Stage2,Stage3,Stage4 flagshipStage
    class Distillation lightweightStage
    class FlagshipOut,LightweightOut output
```
</Wide>

One interesting property of on-policy distillation with small models is that it typically outperforms RL-based methods at a fraction of the compute cost. This is because instead of generating multiple rollouts per prompt, we only sample one, which is then graded by the teacher in a single forward-backward pass. As the Qwen3 tech report shows, the gains over GRPO can be significant:

<Wide>

| Method | AIME'24 | AIME'25 | MATH500 | LiveCodeBench v5 | MMLU -Redux | GPQA -Diamond | GPU Hours |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Off-policy Distillation | 55.0 | 42.8 | 92.4 | 42.0 | 86.4 | 55.6 | - |
| + Reinforcement Learning | 67.6 | 55.5 | 94.8 | 52.9 | 86.9 | 61.3 | 17,920 |
| + On-policy Distillation | **74.4** | **65.5** | **97.0** | **60.3** | **88.3** | **63.3** | 1,800 |
</Wide>

More recently, [Thinking Machines](https://thinkingmachines.ai/blog/on-policy-distillation/) have shown that on-policy distillation is also effective at mitigating  *catastrophic forgetting* , where a post-trained model is further trained on a new domain and it's prior performance regresses. In the table below, they show that although the chat performance of Qwen3-8b (IFEval) tanks when it's fine-tuned on internal data, the behaviour can be restored with cheap distillation:

<Image src={Screenshot_2025_10_30_at_13_02_36_29c1384e_bcac_80d6_a72d_ff34bc221b60} alt="Image" />

We ourselves are quite excited by on-policy distillation as there's a huge diversity of capable, open-weight LLMs that can be distilled into smaller, task-specific models. However, one weakness with all on-policy distillation methods is that the teacher and student must share the same tokenizer. To address that, we've developed a new method called General On-Policy Logit Distillation (GOLD), which allows any teacher to be distilled into any student. We recommend checking out our [technical write-up](https://huggingface.co/spaces/HuggingFaceH4/on-policy-distillation) if you're interested in these topics.

Similarly, researchers at FAIR have compared the effect of being fully off-policy to on-policy for DPO and shown that it's possible to match the performance of GRPO using far less compute [[@online-offline](https://huggingface.co/papers/2506.21495)]:

<Wide>

<HtmlEmbed frameless src="embeds/rl-training-sync.html"/>
</Wide>



As shown in their paper, online DPO works well for math tasks and even the semi-on-policy variant achieves comparable performance despite being many steps off-policy:

| Training method | Math500 | NuminaMath | AMC23 |
| --- | --- | --- | --- |
| Seed (Llama-3.1-8B-Instruct) | 47.4 | 33.9 | 23.7 |
| Offline DPO (s = ∞) | 53.7 | 36.4 | 28.8 |
| Semi-online DPO (s = 100) | **58.9** | 39.3 | **35.1** |
| Semi-online DPO (s = 10) | 57.2 | 39.4 | 31.4 |
| Online DPO (s = 1) | 58.7 | **39.6** | 32.9 |
| GRPO | 58.1 | 38.8 | 33.6 |

Overall, we feel that there still remains much to be done with both scaling RL effectively [@scalerl] and exploring other methods for computational efficiency. Exciting times indeed!

### Wrapping up post-training

If you've made it this far, congrats: you now have all the core ingredients needed for success with post-training. You're now ready to run many experiments and test different algorithms to get SOTA results. 

But as you've probably realised, knowing how to train great models is only half the story. To actually bring those models to life, you need the right infrastructure. Let's finish this opus with the unsung hero of LLM training.

[^f1]: The idea to compute these statistics comes from the Llama 3 tech report [[@llama3](https://arxiv.org/abs/2407.21783)].

[^f2]: For vLLM see:  [Reasoning parsers, ](https://docs.vllm.ai/en/v0.10.1.1/features/reasoning_outputs.html)[Tool parsers](/2421384ebcac80fbaa7cf939fc39269d). For SGLang, see: [Reasoning parsers, ](https://docs.sglang.ai/advanced_features/separate_reasoning.html)[Tool parsers](https://docs.sglang.ai/advanced_features/tool_parser.html)

[^f3]: The Transformers team has recently added [parsers](https://huggingface.co/docs/transformers/main/en/chat_response_parsing) for extract tool calling and reasoning outputs. If adopted by engines like vLLM, the compatibility criterion may become less important in the future.

## Infrastructure - the unsung hero

Now that you know everything we know about model creation and training, let's address the critical yet  *underrated*  component that can make or break your project (and your bank account): infrastructure. Whether you focus on frameworks, architecture, or data curation, understanding infrastructure basics helps identify training bottlenecks, optimise parallelism strategies, and debug throughput issues. (At the minimum, it improves communication with infrastructure teams 😉).

Most people training models care deeply about architecture and data, yet very few understand the infrastructure details. Infrastructure expertise typically lives with framework developers and cluster engineers, and gets treated by the rest as a solved problem: rent some GPUs, install PyTorch, and you're good to go. We trained SmolLM3 on 384 H100s for nearly a month, processing a total of 11 trillion tokens… and this was not a smooth ride! During that time, we dealt with node failures, storage issues and run restarts (see the [training marathon section](#the-training-marathon)). You need to have good contingency plans and strategies to prepare for these issues, and keep training smooth and low-maintenance. 

This chapter aims to bridge that knowledge gap. Think of it as a practical guide to the hardware layer, focused on the questions that matter for training. (Note: Each subsection starts with a TL;DR so you can choose your depth level.)

The first two sections tackle the fundamentals of how hardware works: what does a GPU actually consist of? How does memory hierarchy work? How do CPUs and GPUs communicate? We'll also go over you what to consider when acquiring GPUs and how to test them before committing to long training runs. Most importantly, we'll show you at each step how to measure and diagnose these systems yourself. The next sections are then more applied, and we'll see how to make your infra resilient to failure, and how to maximally optimize your training throughput. 

The name of the game of this chapter is to find and fix the bottlenecks!

Think of this as building your intuition for why certain design decisions matter. When you understand that your model's activations need to flow through multiple levels of cache, each with different bandwidth and latency characteristics, you'll naturally start thinking about how to structure your training to minimise data movement. When you see that inter-node communication is orders of magnitude slower than intra-node, you'll understand why parallelism strategies matter so much.

Let's start by cracking open a GPU and seeing what's inside.

### Inside a GPU: Internal Architecture

A GPU is fundamentally a massively parallel processor optimised for throughput over latency. Unlike CPUs, which excel at executing a few complex instruction streams quickly, GPUs achieve performance by executing thousands of simple operations simultaneously. 

The key to understanding GPU performance lies in recognising that it's not just about raw compute power, it's about the interplay between computation and data movement. A GPU can have teraflops of theoretical compute, but if data can't reach the compute units fast enough, that potential goes unused. This is why we need to understand both the memory hierarchy (how data moves) and the compute pipelines (how work gets done).

At the highest level, a GPU therefore performs two essential tasks:

1.  **Move and store data**  (the memory system)
1.  **Do useful work with the data**  (the compute pipelines)

#### Compute Units and FLOPs

<Note>

<b>TL;DR:</b> GPUs measure performance in FLOPs (floating-point operations per second). Modern GPUs like the H100 deliver dramatically higher throughput at lower precision: 990 TFLOPs at BF16 vs 67 TFLOPs at FP32. However, real-world performance is 70-77% of theoretical peaks due to memory bottlenecks. State-of-the-art training achieves 20-41% end-to-end efficiency, also known as model flops utilization (MFU). Use realistic numbers, not marketing specs, when planning training runs.
</Note>

GPU compute performance is measured in  **FLOPs**  (floating-point operations per second). A FLOP is a single arithmetic operation, typically a floating numbers addition like  `a + b` , and modern GPUs can execute trillions of these per second (TFLOPs).

The fundamental building blocks of GPU compute are  **Streaming Multiprocessors (SMs)** , independent processing units that execute instructions in parallel. Each SM contains two types of  **cores** :  **CUDA cores**  for standard floating-point operations, and specialized  **Tensor Cores**  optimized for matrix multiplication, the workhorse operation in deep learning (critical for transformer performance).

Modern GPUs organize hundreds of these SMs across the chip! For example, the [H100](https://developer.nvidia.com/blog/nvidia-hopper-architecture-in-depth/) SXM5 version (which is the GPU we're using on our cluster) contains 132 SMs. Each SM operates independently, executing groups of 32 threads called  **warps**  in lockstep. To help there, the SMs rely on another component, the  **warp schedulers:**  by balancing instructions to different warps, they enable the SM to "hide latency" by switching between warps when one is held up. This  **SIMT**  (Single Instruction, Multiple Thread) execution model means all threads in a warp execute the same instruction simultaneously on different data. 

<Sidenote>

Warps are named in reference to weaving, "the first parallel thread technology", according to [Lindholm et al., 2008](https://www.cs.cmu.edu/afs/cs/academic/class/15869-f11/www/readings/lindholm08_tesla.pdf). The equivalent of warps in other GPU programming models include [subgroups ](https://github.com/gpuweb/gpuweb/pull/4368) in WebGPU, [waves ](https://microsoft.github.io/DirectX-Specs/d3d/HLSL_SM_6_6_WaveSize.html) in DirectX, and [simdgroups ](https://developer.apple.com/documentation/metal/compute_passes/creating_threads_and_threadgroups#2928931) in Metal.
</Sidenote>

<HtmlEmbed frameless src="embeds/gpu-sm-architecture.html" caption="Multiple SMs within a single GPU -  <a href='https://www.youtube.com/watch?v=ZQKMZIP3Fzg'>

Source</a>"/>


With hundreds of SMs each executing multiple warps concurrently, a single GPU can run tens of thousands of threads simultaneously. This massive parallelism is what enables GPUs to excel at the matrix operations that dominate deep learning workloads!

 **Precision matters significantly**  when discussing FLOPs. Tensor Cores can operate at different precisions (FP64, FP32, FP16/BF16, FP8, FP4 - see [here for a reminder on floating point numbers](https://en.wikipedia.org/wiki/Floating-point_arithmetic)). The achievable throughput therefore varies dramatically, often by orders of magnitude, depending on the data type. Lower precision formats enable higher throughput because they require less data movement and can pack more operations into the same silicon area, but were formerly avoided because of training instabilities. However, nowadays, both training and inference are increasingly pushed toward lower precision, reaching FP8 and FP4, thanks to a range of new techniques.

<Sidenote>

If you want to learn more about our experience with FP8 mixed precision training check out the [Ultra Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook).
</Sidenote>

The table below shows theoretical peak performance across different NVIDIA GPU generations and precision:

| Precision\GPU Type | A100 | H100 | H200 | B100 | B200 |
| --- | --- | --- | --- | --- | --- |
| **FP64** | 9.7 | 34 | 34 | 40 | 40 |
| **FP32** | 19.5 | 67 | 67 | 80 | 80 |
| **FP16/BF16** | 312 | 990 | 990 | 1750 | 2250 |
| **FP8** | - | 3960 | 3960 | 4500 | 5000 |
| **FP4** | - | - | - | 9000 | 10000 |

 *Table showing theoretical TFLOPs depending on precision and GPU generation. Source: Nvidia, SemiAnalysis* 

The dramatic increase in throughput at a lower precision isn't just about raw speed, it reflects a fundamental shift in how we think about numerical computation. FP8 and FP4 enable models to perform more operations per  **watt**  and per  **second** , making them essential for both training and inference at scale. The H100's 3960 TFLOPs at FP8 represents a 4x improvement over FP16/BF16, while the B200's 10,000 TFLOPs at FP4 pushes this even further.

 **Understanding the numbers** : These theoretical peak FLOPs represent the  *maximum computational throughput achievable under ideal conditions* , when all compute units are fully utilized and data is readily available. In practice, actual performance depends heavily on how well your workload can keep the compute units fed with data and whether your operations can be efficiently mapped to the available hardware.

For SmolLM3, we were going to train on NVIDIA H100 80GB HBM3 GPUs, so we first wanted to test the H100's theoretical TFLOPs specifications against real world performance. For this, we used the [SemiAnalysis GEMM benchmark](https://www.ray.so/#theme=prisma&darkMode=false&code=IyBBTUQgVklQIGltYWdlCmFsaWFzIGRydW49InN1ZG8gZG9ja2VyIHJ1biAtLXByaXZpbGVnZWQgLS1uZXR3b3JrPWhvc3QgLS1kZXZpY2U9L2Rldi9rZmQgLS1kZXZpY2U9L2Rldi9kcmkgLS1ncm91cC1hZGQgdmlkZW8gLS1jYXAtYWRkPVNZU19QVFJBQ0UgLS1zZWN1cml0eS1vcHQgc2VjY29tcD11bmNvbmZpbmVkIC0taXBjPWhvc3QgLS1zaG0tc2l6ZT0xOTI2IC0tcm0gLWl0IgpkcnVuIHNlbWlhbmFseXNpc3dvcmsvYW1kLW1hdG11bDpsYXRlc3QKRElTQUJMRV9BREROX0hJUF9MVD0wIFBZVE9SQ0hfVFVOQUJMRV9PUF9FTkFCTEVEPTEgcHl0aG9uIG1hdG11bC5weQoKI0FNRCBweXBpIG5pZ2h0bHkKZHJ1biBhbWQtbGF0ZXN0LXB5cGktbmlnaHRseS1tYXRtdWwKUFlUT1JDSF9UVU5BQkxFX09QX0VOQUJMRUQ9MSBweXRob24gbWF0bXVsLnB5CgojIEFNRCBweXBpIHN0YWJsZSBQeVRvcmNoIDIuNS4xCmRydW4gc2VtaWFuYWx5c2lzd29yay9hbWQtbGF0ZXN0LXB5cGktc3RhYmxlLW1hdG11bApQWVRPUkNIX1RVTkFCTEVfT1BfRU5BQkxFRD0xIHB5dGhvbiBtYXRtdWwucHkKCiMgTnZpZGlhIHN0YWJsZSAyNC4wOQphbGlhcyBkcnVuPSJkb2NrZXIgcnVuIC0tcm0gLWl0IC0tZ3B1cyBhbGwgLS1pcGM9aG9zdCAtLW5ldD1ob3N0IC0tc2htLXNpemU9MTkyNiIKZHJ1biBzZW1pYW5hbHlzaXN3b3JrL252aWRpYS1tYXRtdWw6bGF0ZXN0CnB5dGhvbiBtYXRtdWwucHkKCg&language=shell): it [tests throughput on real-world matrix multiplication shapes from Meta's Llama 70B training](https://newsletter.semianalysis.com/p/mi300x-vs-h100-vs-h200-benchmark-part-1-training#general-matrix-multiply-gemm-performance).

<Wide>

<Reference caption="Table showing achieved TFLOPs on H100 80GB depending on precision and matrix shape from the Llama 70B training workload">

| Shape (M, N, K) | FP64 torch.matmul | FP32 torch.matmul | FP16 torch.matmul | BF16 torch.matmul | FP8 TE.Linear (autocast, bias=False) | FP8 torch._scaled_mm (e5m2/e4m3fn) | FP8 torch._scaled_mm (e4m3) |
| --- | --- | --- | --- | --- | --- | --- | --- |
| (16384, 8192, 1280) | 51.5 TFLOPS | 364.5 TFLOPS | 686.5 TFLOPS | 714.5 TFLOPS | 837.6 TFLOPS | 1226.7 TFLOPS | 1209.7 TFLOPS |
| (16384, 1024, 8192) | 56.1 TFLOPS | 396.1 TFLOPS | 720.0 TFLOPS | 757.7 TFLOPS | 547.3 TFLOPS | 1366.2 TFLOPS | 1329.7 TFLOPS |
| (16384, 8192, 7168) | 49.5 TFLOPS | 356.5 TFLOPS | 727.1 TFLOPS | 752.9 TFLOPS | 1120.8 TFLOPS | 1464.6 TFLOPS | 1456.6 TFLOPS |
| (16384, 3584, 8192) | 51.0 TFLOPS | 373.3 TFLOPS | 732.2 TFLOPS | 733.0 TFLOPS | 952.9 TFLOPS | 1445.7 TFLOPS | 1370.3 TFLOPS |
| (8192, 8192, 8192) | 51.4 TFLOPS | 372.7 TFLOPS | 724.9 TFLOPS | 729.4 TFLOPS | 1029.1 TFLOPS | 1404.4 TFLOPS | 1397.5 TFLOPS |
</Reference>

</Wide>

 **Validating theoretical performance** : Our experiments revealed the gap between theoretical peaks and achievable performance. 

For  **FP64 Tensor Core**  operations, we achieved 49-56 TFLOPs, representing 74-84% of the theoretical peak (67 TFLOPs). For  **TF32**  (TensorFloat-32, which PyTorch uses by default for FP32 tensors on Tensor Cores), we achieved 356-396 TFLOPs, representing 72-80% of the theoretical peak (~495 TFLOPs dense). While these show excellent hardware utilization, these precisions are rarely used in modern deep learning training: FP64 due to its computational cost, and TF32 because lower precisions like BF16 and FP8 offer better performance.

<Sidenote>

[NVIDIA specs](https://www.nvidia.com/en-us/data-center/h100/) often list sparse performance (989 TFLOPs for TF32) which assumes 2:4 structured sparsity patterns. Dense operations, which our benchmark tests, achieve roughly half the sparse peak (~495 TFLOPs).
</Sidenote>

For  **BF16**  operations, we consistently achieved 714-758 TFLOPs across different matrix shapes, approximately 72-77% of the H100's theoretical 990 TFLOPs peak. This is, in practice, an excellent utilisation rate for a real-world workload!

<Note title="Model FLOPs Utilization (MFU)" emoji="📊" variant="info">

<p>While kernel benchmarks measure raw TFLOPS, end-to-end training efficiency is captured by <b>Model FLOPs Utilization (MFU)</b>: the ratio of useful model computation to theoretical peak hardware performance.</p>

<p>Our BF16 matmul benchmarks showed we achieved 72-77% of the H100's theoretical peak. This represents the upper bound for what's achievable at the kernel level for our setup. End-to-end training MFU will necessarily be lower due to more complex non-matmul operations, communication overhead, and other auxiliary computations.</p>

<p><b>State-of-the-art MFU in training</b>: Meta achieved 38-41% when training Llama 3 405B, while DeepSeek-v3 reached ~20-30% on GPUs with tighter communication bottlenecks related to the MoE architecture. For SmolLM3, we achieved ~30% MFU as we'll see later. Much of the gap comes from inter-node communication overhead in distributed training. Given our kernel-level ceiling of ~77%, these end-to-end numbers represent roughly 50-55% efficiency relative to achievable matmul performance. Inference workloads can reach higher MFU >70%, closer to raw matmul performance, though published results from production deployments are scarce.</p>
</Note>

The  **FP8**  results are more nuanced. Let's look at our results on 3 different matrix multiplication methods/kernels.

<Sidenote>

A <a href="https://modal.com/gpu-glossary/device-software/kernel">kernel</a> is the unit of CUDA code.
</Sidenote>

Using PyTorch's  `torch._scaled_mm`  kernel with e4m3 precision, we achieved 1,210-1,457 TFLOPs depending on the matrix shape, roughly 31-37% of the theoretical 3,960 TFLOPs peak. 😮 Why? This lower utilization percentage (in FP8) actually doesn't indicate poor performance; rather, it reflects that these operations become increasingly memory-bound as compute throughput grows. The [Tensor Cores](https://www.nvidia.com/en-us/data-center/tensor-cores/) can process FP8 data faster than the memory system can deliver it, making memory bandwidth the limiting factor.

The [Transformer Engine](https://github.com/NVIDIA/TransformerEngine)'s  `TE.Linear`  achieved 547-1,121 TFLOPs depending on the shape, while  `torch._scaled_mm`  consistently delivered higher throughput. This highlights an important lesson:  ***kernel implementation matters***  *significantly, and the choice of API can impact performance by 2-3x even when targeting the same hardware capabilities.* 

For SmolLM3's training, these practical measurements helped us set realistic throughput expectations. When planning your own training runs, use these achievable numbers rather than theoretical peaks to set your expectations.

<Note title="Compute Capability" emoji="🔧" variant="info">

<p>Besides choosing the right kernel API, we also need to ensure those kernels are compiled for the right hardware generation. Compute Capability (CC) is NVIDIA's versioning system that abstracts physical GPU details from the PTX instruction set. It determines which instructions and features your GPU supports.</p>

<p><b>Why this matters</b>: Kernels compiled for a specific compute capability may not run on older hardware, and you might miss optimizations if your code isn't compiled for your target GPU's CC. Even worse, frameworks can silently select suboptimal kernels—we discovered PyTorch selecting sm_75 kernels (compute capability 7.5, designed for Turing GPUs) on our H100s, causing mysterious slowdowns. This is a <a href="https://discuss.pytorch.org/t/performance-issue-torch-matmul-selecting-cutlass-sm75-kernel-for-a100/220682/3" target="_blank">similar issue documented in the PyTorch community</a>, where frameworks often default to older, more compatible kernels rather than optimal ones. This seemingly minor detail can make the difference between getting 720 TFLOPS or 500 TFLOPS from the same hardware.</p>

<p>When using pre-compiled libraries or custom kernels, always verify they're built for your hardware's compute capability to ensure compatibility and optimal performance. For example, sm90_xmma_gemm_..._cublas indicates a kernel compiled for SM 9.0 (compute capability 9.0, used by H100).</p>

<p>You can check your GPU's compute capability with <code>nvidia-smi --query-gpu=compute_cap</code> or find the technical specifications in the <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#compute-capabilities" target="_blank">Compute Capability section of the NVIDIA CUDA C Programming Guide</a>.</p>
</Note>

As we saw, the GPU memory seems to become a bottleneck when computations get too fast at a low precision. Let's have a look at how GPU memory works, and at what causes bottlenecks to occur!

#### GPU Memory Hierarchy: From Registers to HBM

In order to make calculations, GPUs need to read/write to memory, so it's important to know at what speed these transfers happen. Understanding GPU memory hierarchy is crucial for writing high-performance kernels.

<Note>

<b>TL;DR:</b> GPUs organize memory in a hierarchy from fast-but-small (registers, shared memory) to slow-but-large (HBM main memory). Understanding this hierarchy is critical because modern AI is often memory-bound: the bottleneck is moving data, not computing on it. Operator fusion (like Flash Attention) achieves 2-4× speedups by keeping intermediate results in fast on-chip memory instead of writing to slow HBM. Benchmarks show H100's HBM3 delivers ~3 TB/s in practice, matching theoretical specs for large transfers.
</Note>

To visualize how memory operations flow through a GPU in practice, let's first look at the [Memory Chart from NVIDIA Nsight Compute](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#memory-chart), a profiling graph which shows a graphical representation of how data moves between different memory units for any kernel of your choice:

<Reference caption="Memory Chart showing data flow through GPU memory hierarchy during FP64 matrix multiplication on H100">

<Image src={image_2881384e_bcac_80d6_84fe_d705cb1eae0a} alt="Image" />
</Reference>

In general, a Memory Chart shows both  **logical units**  (in green) like Global, Local, Texture, Surface, and Shared memory, and  **physical units**  (in blue) like L1/TEX Cache, Shared Memory, L2 Cache, and Device Memory. Links between units represent the number of instructions (Inst) or requests (Req) happening between units, with colors indicating the percentage of peak utilization: from unused (0%) to operating at peak performance (100%). 

You can generate this memory chart for any kernel using [NVIDIA Nsight Compute](https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html):

```bash
## Profile a specific kernel with memory workload analysis
ncu --set full --kernel-name "your_kernel_name" --launch-skip 0 --launch-count 1 python your_script.py
## Once profiling is complete, open the results in the Nsight Compute GUI to view the Memory Chart
```
It provides several key insights:

-  **Bottleneck identification** : Saturated links (shown in red/orange) indicate where data movement is constrained
-  **Cache efficiency** : Hit rates for L1/TEX and L2 caches reveal how well your kernel utilizes the memory hierarchy
-  **Memory access patterns** : The flow between logical and physical units shows whether your kernel has good spatial/temporal locality
-  **Port utilization** : Individual memory ports may be saturated even when aggregate bandwidth appears underutilized

In our specific case above, you can see how kernel instructions flow through the memory hierarchy (for FP64 matrix multiplications on our hardware): global load instructions generate requests to the L1/TEX cache, which may hit or miss and generate further requests to L2, which ultimately accesses device memory (HBM) on misses. The colored rectangles inside units show port utilization, even if individual links operate below peak, the shared data port may be saturated.

<Note title="Optimizing Memory Hierarchy Access" emoji="💡" variant="tip">

For optimal performance, aim to minimize traffic to slower memory tiers (HBM) while maximizing utilization of faster tiers (shared memory, registers).
</Note>

Now let's understand the underlying memory hierarchy that makes this chart possible. Modern GPUs organize memory in a hierarchy that balances speed, capacity, and cost, a design dictated by fundamental physics and circuit constraints.

<Reference caption="Memory hierarchy of the H100 (SXM5) GPU. <a href='https://www.aleksagordic.com/blog/matmul'>

Source</a>">

<Image src={image_2881384e_bcac_801d_9f3d_c875181b9dd1} alt="Image" />
</Reference>

At the bottom of this hierarchy sits  **HBM (High Bandwidth Memory):** the GPU's main memory, also called global memory or device memory. The H100 features HBM3 with a theoretical bandwidth of 3.35 TB/s. HBM is the largest but slowest tier in the memory hierarchy.

Moving up the hierarchy toward the compute units, we find progressively faster but smaller memory tiers:

-  **L2 cache** : A large SRAM-based cache shared across the GPU, typically several tens of megabytes. On H100, this is 50 MB with a bandwidth of ~13 TB/s
-  **L1 cache and Shared Memory (SMEM)** : Each Streaming Multiprocessor (SM) has its own L1 cache and programmer-managed shared memory, which share the same physical SRAM storage. On H100, this combined space is 256 KB per SM with bandwidth of ~31 TB/s per SM
-  **Register File (RMEM)** : At the top of the hierarchy, registers are the fastest storage, located directly next to compute units. Registers are private to individual threads and offer bandwidth measured in ~100s TB/s per SM

This hierarchy exists because SRAM (used for caches and registers) is fast but physically large and expensive, while DRAM (used for HBM) is dense and cheap but slower. The result: fast memory comes in small quantities close to compute, backed by progressively larger pools of slower memory further away.

 **Why this matters** : Understanding this hierarchy is essential for kernel optimization. The key insight is that memory-bound operations are limited by how fast you can move data, not how fast you can compute. As [Horace He](https://upload.wikimedia.org/wikipedia/commons/b/b2/Hausziege_04.jpg) explains in [Making Deep Learning Go Brrrr From First Principles](https://horace.io/brrr_intro.html), *"load from memory" → "multiply by itself twice" → "write to memory"*  takes essentially the same time as  *"load from memory" → "multiply by itself once" → "write to memory"* : the computation is "free" compared to the memory access.

This is why  **operator fusion**  is so powerful: by combining multiple operations into a single kernel, you can keep intermediate results in fast SRAM instead of writing them back to slow HBM between operations. Flash Attention is a perfect example of this principle in action.

<Note title="Flash Attention: A Case Study in Memory Hierarchy Optimization" emoji="⚡" variant="info">

<p>Standard attention implementations are memory-bound because they materialize the full attention matrix in HBM:</p>

<ol>
  <li>Compute <code>Q @ K^T</code> → write N×N attention scores to HBM</li>
  <li>Apply softmax → read from HBM, compute, write back to HBM</li>
  <li>Multiply by V → read attention scores from HBM again</li>
</ol>

<p>Flash Attention achieves its 2-4× speedup by <b>fusing these operations</b> and keeping intermediate results in SRAM:</p>

<ul>
  <li>Instead of computing the full attention matrix, it processes attention in tiles that fit in SRAM</li>
  <li>Intermediate attention scores never leave the fast on-chip memory</li>
  <li>Only the final output is written back to HBM</li>
</ul>

<p><b>The result</b>: Flash Attention reduces HBM accesses from O(N²) to O(N), transforming a memory-bound operation into one that better utilizes the GPU's compute capabilities. This is the essence of efficient kernel design:  *minimize slow memory movement, maximize fast computation* .</p>
</Note>

 **Example: Validating our HBM3 Bandwidth in Practice** 

Now that we understand the memory hierarchy, let's put theory into practice and validate the actual bandwidth on our H100 GPUs! This is where benchmarking tools become essential.

 **NVBandwidth**  is NVIDIA's open-source benchmarking tool designed specifically for measuring bandwidth and latency across GPU systems. It evaluates data transfer rates for various memory copy patterns—host-to-device, device-to-host, and device-to-device operations—using both copy engines and kernel-based methods. The tool is particularly valuable for assessing inter-GPU communication (for example [NVLink](https://en.wikipedia.org/wiki/NVLink) and [PCIe](https://fr.wikipedia.org/wiki/PCI_Express), two types of connectors) and validating system performance in multi-GPU environments.

You can install NVBandwidth from [NVIDIA's GitHub repository](https://github.com/NVIDIA/nvbandwidth). The tool outputs detailed bandwidth matrices showing how efficiently data transfers between different devices, making it ideal for diagnosing performance bottlenecks or verifying healthy GPU interconnects.

Let's use it to measure our H100's local memory bandwidth using the  `device_local_copy`  test, which measures bandwidth of  `cuMemcpyAsync`  between device buffers local to the GPU across different message sizes.

<Sidenote>

cuMemcpyAsync is a CUDA driver API function that asynchronously copies data between two memory pointers, inferring the type of transfer (host-to-host, host-to-device, device-to-device, or device-to-host)
</Sidenote>

  

```
$ ./nvbandwidth -t device_local_copy -b 2048
memcpy local GPU(column) bandwidth (GB/s)
           0         1         2         3         4         5         6         7
 0   1519.07   1518.93   1519.07   1519.60   1519.13   1518.86   1519.13   1519.33
```

<HtmlEmbed 
  src="embeds/d3-line-chart.html" 
  config={{ 
    dataUrl: "./data/device_local_copy_bandwidth.csv",
    xColumn: "message_size",
    yColumn: "bandwidth",
    runColumn: "run_name",
    xScaleType: "log",
    xAxisLabel: "Message Size (bytes)",
    yAxisLabel: "Bandwidth (GB/s)",
    referenceLine: { value: 1975, label: "Max" },
    xFormatAsFileSize: true
  }}
  title="Measured H100 Local Memory Bandwidth"
/>



The results reveal an important characteristic of memory systems:  **for small message sizes (< 1 MB), we're latency-bound**  rather than bandwidth-bound. The overhead of initiating memory transfers dominates performance, preventing us from reaching peak bandwidth. However,  **for large message sizes (≥ 1 MB), we achieve sustained bandwidth of ~1,500 GB/s for both read and write operations** .

Since HBM bandwidth accounts for both reads and writes happening simultaneously, we sum these to get  **3 TB/s total bidirectional bandwidth**  (1,519 read + 1,519 write), which closely validates the H100's theoretical 3.35 TB/s HBM3 specification.

#### Roofline Model

Understanding whether your kernel is compute-bound or memory-bound determines which optimizations will help. 

There are two scenarios:

- If you're  **memory-bound**  (spending most time moving data), increasing compute throughput won't help: You need to reduce memory traffic through techniques like operator fusion. 
- If you're  **compute-bound**  (spending most time on FLOPs), optimizing memory access patterns won't help: You need more compute power or better algorithms.

The  *roofline model*  provides a visual framework for understanding these performance characteristics and identifying optimization opportunities. 

Let's apply it to a real kernel analysis. It's available in the NSight Compute profiling tool we mentioned before (under "roofline analysis view"). Here's what we get:

<Reference caption="Roofline chart showing kernel performance boundaries - Source: <a href='https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html'>

NVIDIA NSight Compute Profiling Guide</a>">

<Image src={image_2881384e_bcac_80ed_9bdf_c077977d77b8} alt="Image" />
</Reference>

Let's see how we can read this chart, which has two axes:

-  **Vertical axis (FLOP/s)** : Shows achieved floating-point operations per second, using a logarithmic scale to accommodate the large range of values
-  **Horizontal axis (Arithmetic Intensity)** : Represents the ratio of work (FLOPs) to memory traffic (bytes), measured in FLOPs per byte. This also uses a logarithmic scale.

The roofline itself consists of two boundaries:

-  **Memory Bandwidth Boundary**  (sloped line): Determined by the GPU's memory transfer rate (HBM bandwidth). Performance along this line is limited by how fast data can be moved.
-  **Peak Performance Boundary**  (flat line): Determined by the GPU's maximum compute throughput. Performance along this line is limited by how fast computations can be executed.

The  **ridge point**  where these boundaries meet represents the transition between memory-bound and compute-bound regimes.

We can interpret the performance by looking at the two divided regions of the chart:

-  **Memory Bound**  (below the sloped boundary): Kernels in this region are limited by memory bandwidth. The GPU is waiting for data, increasing compute power won't help. Optimizations should focus on reducing memory traffic through techniques like operator fusion, better memory access patterns, or increasing arithmetic intensity.
-  **Compute Bound**  (below the flat boundary): Kernels in this region are limited by compute throughput. The GPU has enough data but can't process it fast enough. Optimizations should focus on algorithmic improvements or leveraging specialized hardware like Tensor Cores.

The  **achieved value**  (the plotted point) shows where your kernel currently sits. The distance from this point to the roofline boundary represents your optimization headroom, the closer to the boundary, the more optimal your kernel's performance.

In our example, the kernel sits in the memory-bound region, indicating that there's still room for improvement by optimizing memory traffic!

For a deeper dive into GPU internals including detailed explanations of CUDA cores, Tensor Cores, memory hierarchies, and low-level optimization techniques, check out the [Ultrascale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)! Now that we understand what happens  *inside*  a GPU, let's zoom out and explore how GPUs communicate with the rest of the world.

### Outside a GPU: How GPUs Talk to the World

Now that we understand how a GPU performs computation using its internal memory hierarchy, we need to address a critical reality: a GPU doesn't operate in isolation. Before any computation can happen, data must be loaded into the GPU's memory. The CPU needs to schedule kernels and coordinate work. And in distributed training, GPUs must constantly exchange activations, gradients, and model weights with each other.

<Reference caption="DGX H100. Source: NVIDIA">

<Image src={h100_dgx_2891384e_bcac_80cf_9f86_ccf0653a79e5} alt="Image" />
</Reference>

This is where the external communication infrastructure becomes crucial. No matter how powerful your GPU's compute units are, if data can't reach them fast enough, whether from the CPU, from storage, or from other GPUs, your expensive hardware sits idle. Understanding these communication pathways and their bandwidth characteristics is essential for maximizing hardware utilization and minimizing bottlenecks.

In this section, we'll look at four critical communication links that connect a GPU to the outside world:

-  **GPU-CPU** : How the CPU schedules work and transfers data to GPUs
-  **GPU-GPU intra-node** : How GPUs on the same machine communicate
-  **GPU-GPU inter-node** : How GPUs across different machines communicate over the network
-  **GPU-Storage** : How data flows from storage to GPU memory

Each of these links has different bandwidth and latency characteristics, and understanding them will help you identify where your training pipeline might be bottlenecked. To make this easier to understand, we've created a simplified diagram that highlights the most important components and communication links:

<Wide>

<Reference caption="Simplified diagram of the key components and communication links in our AWS p5 instance setup">

<HtmlEmbed src="/embeds/aws-bandwidth-bottleneck.html" />
</Reference>

</Wide>



If this looks overwhelming, don't worry. We'll dive into each of these connections in detail and measure their actual bandwidths to understand the performance characteristics of each link.

#### GPU-to-CPU 

<Note>

<b>TL;DR:</b> The CPU orchestrates GPU work via PCIe connections, which bottleneck at ~14.2 GB/s (PCIe Gen4 x8) for CPU-to-GPU transfers in our p5 instance. CPU-GPU latency is ~1.4 microseconds, which adds kernel launch overhead that is problematic for workloads with many small kernels. CUDA Graphs can reduce this overhead by batching operations. NUMA affinity is critical on multi-socket systems; running GPU processes on the wrong CPU socket adds significant latency. Modern architectures like Grace Hopper eliminate PCIe bottlenecks with NVLink-C2C (900 GB/s vs 128 GB/s).
</Note>

The CPU is the orchestrator of GPU computation. It's responsible for launching kernels, managing memory allocations, and coordinating data transfers. But how fast can the CPU actually communicate with the GPU? This is determined by the  **PCIe (Peripheral Component Interconnect Express)**  connection between them.

Understanding this link is crucial because it affects:

-  **Kernel launch latency** : How quickly the CPU can schedule work on the GPU
-  **Data transfer speed** : How fast we can move data between CPU and GPU memory
-  **Synchronization overhead** : The cost of CPU-GPU coordination points

In modern GPU servers, the CPU-GPU connection has evolved significantly. While earlier systems used direct PCIe connections, modern high-performance systems like the DGX H100 use more sophisticated topologies with PCIe  *switches*  to manage multiple GPUs efficiently. And with the latest [GB200 architecture](https://newsletter.semianalysis.com/p/nvidias-blackwell-reworked-shipment), NVIDIA has taken this even further by placing the CPU and GPU on the same printed circuit board, eliminating the need for external switches altogether.

Let's examine the physical topology of our p5 instance using  `lstopo`  and then measure the actual performance of this critical link, to identify potential bottlenecks.

```
$ lstopo -v
...
HostBridge L#1 (buses=0000:[44-54])
    PCIBridge L#2 (busid=0000:44:00.0 id=1d0f:0200 class=0604(PCIBridge) link=15.75GB/s buses=0000:[45-54] PCISlot=64)
        PCIBridge L#3 (busid=0000:45:00.0 id=1d0f:0200 class=0604(PCIBridge) link=15.75GB/s buses=0000:[46-54] PCISlot=1-1)
            ...
            PCIBridge L#12 (busid=0000:46:01.4 id=1d0f:0200 class=0604(PCIBridge) link=63.02GB/s buses=0000:[53-53])
                PCI L#11 (busid=0000:53:00.0 id=10de:2330 class=0302(3D) link=63.02GB/s PCISlot=86-1)
                    Co-Processor(CUDA) L#8 (Backend=CUDA GPUVendor="NVIDIA Corporation" GPUModel="NVIDIA H100 80GB HBM3" CUDAGlobalMemorySize=83295872 CUDAL2CacheSize=51200 CUDAMultiProcessors=132 CUDACoresPerMP=128 CUDASharedMemorySizePerMP=48) "cuda0"
                    GPU(NVML) L#9 (Backend=NVML GPUVendor="NVIDIA Corporation" GPUModel="NVIDIA H100 80GB HBM3" NVIDIASerial=1654922006536 NVIDIAUUID=GPU-ba136838-6443-7991-9143-1bf4e48b2994) "nvml0"
            ...
...
```
From the  `lstopo`  output, we can see two key PCIe bandwidth values in our system:

-  **15.75GB/s** : corresponds to PCIe Gen4 x8 links (CPU to PCIe switches)
-  **63.02GB/s** : corresponds to PCIe Gen5 x16 links (PCIe switches to GPUs)

To have a better understanding of the whole topology, we can visualize it using:

```
$ lstopo --whole-system lstopo-diagram.png
```

<Image src={lstopo_29c1384e_bcac_80c9_9715_cbfe9e73d86b} alt="Image" />

This diagram showcases the hierarchical structure of our system:

- It contains two  **NUMA**  (Non-Uniform Memory Access) nodes (NUMA is memory zone per CPU socket)
- Each  **CPU socket**  connects to four  **PCIe switches**  via  **PCIe Gen4**  x8 links (15.75GB/s)
- Each  **PCIe switch**  connects to one  **H100 GPU** via  **PCIe Gen5**  x16 links (63.02GB/s)
- … (We'll explore the other components like NVSwitch, EFA network cards and NVMe drives in next sections.)

The PCIe specification differs between generations, each doubling the transfer rate per lane. Note that Transfer Rate is measured in GT/s (GigaTransfers per second), which represents the raw signaling rate, while Throughput is measured in GB/s (Gigabytes per second), which accounts for encoding overhead and represents the actual usable bandwidth:

<Reference caption="Theoretical PCIe bandwidths. Source: https://en.wikipedia.org/wiki/PCI_Express">

| PCIe Version | Transfer Rate (per lane) | Throughput (GB/s) |
| --- | --- | --- |
| ×1 | ×2 | ×4 | ×8 | ×16 |
| 1.0 | 2.5 GT/s | 0.25 | 0.5 | 1 | 2 | 4 |
| 2.0 | 5.0 GT/s | 0.5 | 1 | 2 | 4 | 8 |
| 3.0 | 8.0 GT/s | 0.985 | 1.969 | 3.938 | 7.877 | 15.754 |
| 4.0 | 16.0 GT/s | 1.969 | 3.938 | 7.877 | **15.754** | 31.508 |
| 5.0 | 32.0 GT/s | 3.938 | 7.877 | 15.754 | 31.508 | **63.015** |
| 6.0 | 64.0 GT/s | 7.563 | 15.125 | 30.25 | 60.5 | 121 |
| 7.0 | 128.0 GT/s | 15.125 | 30.25 | 60.5 | 121 | 242 |
</Reference>

<HtmlEmbed
src="/embeds/aws-bandwidth-bottleneck.html"
config={{ initialFilter: 'cpu-gpu' }}
caption="CPU-to-GPU communication path."
/>

From the topology diagram and the PCIe bandwidth table, we can see that the CPU-to-GPU path goes through two PCIe hops: first from the CPU to the PCIe switch via  **PCIe Gen4** x8 (15.754 GB/s), then from the PCIe switch to the GPU via  **PCIe Gen5** x16 (63.015 GB/s).  *This means the bottleneck for CPU-GPU communication is the first hop at 15.754 GB/s* . Let's validate this with another util,  `nvbandwidth` !

The  `host_to_device_memcpy_ce`  command measures bandwidth of  `cuMemcpyAsync`  from host (CPU) memory to device (GPU) memory using the GPU's copy engines. 

```markdown
./nvbandwidth -t host_to_device_memcpy_ce -b <message_size> -i 5
```

<HtmlEmbed 
src="embeds/d3-line-chart.html"
title="CPU->

GPU measured bandwidth" caption="CPU-to-GPU bandwidth measured with nvbandwidth's host_to_device test, showing the PCIe Gen4 x8 bottleneck at ~14.2 GB/s for large transfers"
config={{
dataUrl: "./data/host_device_memcpy_bandwidth.csv",
xColumn: "message_size",
yColumn: "bandwidth",
runColumn: "run_name",
xScaleType: "log",
xAxisLabel: "Message Size (bytes)",
yAxisLabel: "Bandwidth (GB/s)",
referenceLine: { value: 14.2, label: "Max" },
xFormatAsFileSize: true
}}
/>


The results indeed show that for small message sizes we're latency-bound, but for large message sizes we achieve  **~14.2 GB/s** , which is about 90% of the theoretical 15.754 GB/s bandwidth for PCIe Gen4 x8. This confirms that in  **CPU-GPU**  communication, the CPU-to-PCIe switch link is indeed our bottleneck.

Beyond bandwidth,  **latency**  is equally important for CPU-GPU communication since it determines how quickly we can schedule kernels. To measure this, we use  `nvbandwidth` 's  `host_device_latency_sm`  test, which employs a pointer-chase kernel to measure round-trip latency. The  `host_device_latency_sm`  test measures round-trip latency by allocating a buffer on the host (CPU) and accessing it from the GPU using a pointer-chase kernel. This simulates the real-world latency of CPU-GPU communication.

```markdown
./nvbandwidth -t host_device_latency_sm -i 5
```

<HtmlEmbed
src="embeds/d3-line-chart.html"
config={{
dataUrl: "./data/host_device_latency.csv",
xColumn: "message_size",
yColumn: "latency",
runColumn: "run_name",
xScaleType: "log",
xAxisLabel: "Message Size (bytes)",
yAxisLabel: "Latency (μs)",
referenceLine: { value: 1440, label: "Max" },
xFormatAsFileSize: true
}}
title="CPU->

GPU measured latency" 
caption="CPU-to-GPU latency measured with nvbandwidth's host_device_latency_sm test (adapted to make buffer size variable), showing approximately 1.4 microseconds round-trip latency"
/>


The results show that  **latency**  is approximately  **1.4 microseconds** . This explains the kernel launch overhead of a few microseconds we often observe in ML workloads. For workloads launching many small kernels, the added latency can become a bottleneck; otherwise, overhead is hidden by overlapping execution.

<Sidenote>

For example for small models or with small batches we can see inference saturate on the GPU because of the kernel launches. FlashFormer addresses this by fusing a whole layer to gain speedups [@nrusimha2025flashformerwholemodelkernelsefficient].
</Sidenote>

<Note title="CUDA Graphs for Reducing Launch Overhead" emoji="🚀" variant="info">

CUDA Graphs can significantly reduce kernel launch overhead by capturing a sequence of operations and replaying them as a single unit, eliminating microseconds of CPU-GPU round-trip latency for each kernel launch. This is particularly beneficial for workloads with many small kernels or frequent CPU-GPU synchronization. For more details on understanding and optimizing launch overhead, see <a href="https://developer.nvidia.com/blog/understanding-the-visualization-of-overhead-and-latency-in-nsight-systems/">Understanding the Visualization of Overhead and Latency in NVIDIA Nsight Systems</a>.
</Note>

<Note title="MoE Models and CPU-GPU Synchronization Overhead" emoji="⚠️" variant="warning">

Some implementations of Mixture-of-Experts (MoE) models require CPU-GPU synchronization in each iteration to schedule the appropriate kernels for the selected experts. This introduces kernel launch overhead that can significantly affect throughput, especially when the CPU-GPU connection is slow. For example, in <a href="https://www.mako.ai/blog/mako-generate-achieves-1-83x-performance-over-torch-compile-on-deepseek-moe-kernels">MakoGenerate's optimization of DeepSeek MOE kernels</a>, the reference implementation dispatched 1,043 kernels with 67 CPU-GPU synchronization points per forward pass. By restructuring the expert routing mechanism, they reduced this to 533 kernel launches and just 3 synchronization points, achieving a 97% reduction in synchronization overhead and 44% reduction in end-to-end latency. Note that not all MoE implementations require CPU-GPU synchronization (modern implementations often keep routing entirely on the GPU), but for those that do, efficient CPU-GPU communication becomes critical for performance.
</Note>

<Note title="Grace Hopper Superchips: A Different Approach to CPU-GPU Communication" emoji="🔗" variant="info">

NVIDIA's Grace Hopper superchips take a fundamentally different approach to CPU-GPU communication compared to traditional x86+Hopper systems. Key improvements include:
<ul>
  <li><b>1:1 GPU to CPU ratio</b> (compared to 4:1 for x86+Hopper), providing 3.5x higher CPU memory bandwidth per GPU</li>
  <li><b>NVLink-C2C</b> replacing PCIe Gen5 lanes, delivering 900 GB/s vs 128 GB/s (7x higher GPU-CPU link bandwidth)</li>
  <li><b>NVLink Switch System</b> providing 9x higher GPU-GPU link bandwidth than InfiniBand NDR400 NICs connected via PCIe Gen4</li>
</ul>
For more details, see the <a href="https://download.deltacomputer.com/NVIDIA%20Grace%20Hopper%20Superchip%20Architecture%20Whitepaper.pdf">NVIDIA Grace Hopper Superchip Architecture Whitepaper</a> (page 11).
</Note>

 **⚠️ NUMA Affinity: Critical for Multi-Socket Performance** 

On multi-socket systems like our AMD EPYC 7R13 nodes (2 sockets, 48 cores each), ****  **NUMA affinity**is crucial for GPU performance** . It refers to running processes on CPU cores that share the same socket as their target devices (like GPUs). When your GPU process runs on CPUs from a different NUMA node than where the GPU is attached, operations must traverse the CPU interconnect (AMD Infinity Fabric), adding significant latency and bandwidth constraints.

 **First, let's examine the NUMA topology and node distances to understand the performance implications** :

```bash
$ numactl --hardware
node distances:
node   0   1 
  0:  10  32 
  1:  32  10 
```
The distance values show that accessing memory on the same NUMA node (distance 10) is much faster than crossing to the other NUMA node (distance 32). This  **3.2x difference**  in memory access  **latency**  can significantly impact GPU performance when your process is pinned to the wrong NUMA node.

For detailed steps on diagnosing and resolving NUMA-related performance issues, see the Troubleshooting Interconnect Performance section.

#### GPU-to-GPU Intranode

In distributed training, GPUs must frequently exchange gradients, weights, and activations, often gigabytes of data per iteration. This huge amount of data requires careful handling of communication. While the H100's internal HBM can read at about 3 TB/s, accidentally using the wrong flags can completely flunk your GPU-to-GPU communication bandwidth!

Let's see why by examining all the ways you can do communication between GPUs within the same node (and all the flags you should - or should not - set) 🙂

<Note>

<b>TL;DR:</b> GPUs within a node can communicate three ways: through CPU (slowest, ~3 GB/s, bottlenecked by PCIe), via GPUDirect RDMA over EFA NICs (~38 GB/s), or GPUDirect RDMA via NVLink (~786 GB/s bidirectional). NVLink is 9-112x faster and bypasses CPU/PCIe entirely. NCCL automatically prioritizes NVLink when available. NVLink SHARP (NVLS) provides hardware-accelerated collectives, boosting allreduce performance by 1.3x to 480 GB/s. However, alltoall operations (340 GB/s) don't benefit from NVLS acceleration.
</Note>

####  **Through CPU** 

The naive approach uses host memory (SHM): data travels from GPU1 through the PCIe switch to the CPU, into host memory, back through the CPU, through the PCIe switch again, and finally to GPU2. This can be achieved (although not recommended) using  `NCCL_P2P_DISABLE=1`  and  `FI_PROVIDER=tcp`  environment variable by NCCL. When this mode is activated, you can verify it by setting  `NCCL_DEBUG=INFO` , which will show messages like:

```
NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
```

<HtmlEmbed
src="/embeds/aws-bandwidth-bottleneck.html"
config={{ initialFilter: 'gpu-gpu-cpu' }}
caption="GPU-to-GPU communication path through CPU and main memory, showing the inefficient roundtrip through the PCIe switch and CPU."
/>



This roundabout path involves multiple memory copies and saturates both PCIe and CPU memory buses, causing congestion. In our topology where 4 H100s share the same CPU memory buses, this congestion becomes even more problematic when multiple GPUs attempt simultaneous communication, as they compete for the same limited CPU memory bandwidth… 😢

With this CPU-mediated approach, we're fundamentally bottlenecked by the PCIe Gen4 x8 link at ~16 GB/s between the CPU and PCIe switch. Fortunately, there's a better way for our GPUs to communicate without involving the CPU:  **GPUDirect RDMA** .

#### Through Libfabric EFA

 **GPUDirect RDMA**  (Remote Direct Memory Access or GDRDMA) is a technology that enables direct communication between NVIDIA GPUs by allowing direct access to GPU memory. This eliminates the need for data to pass through the system CPU and avoids buffer copies via system memory, resulting in up to 10x better performance compared to traditional CPU-mediated transfers. GPUDirect RDMA works over PCIe to enable fast GPU-to-GPU communication both within a node (as we see here) and across nodes using  **NICs**  (network interface cards, as we'll see in a future section) with RDMA capabilities. For more details, see [NVIDIA GPUDirect](https://developer.nvidia.com/gpudirect).

Looking back at our topology diagram, we can see that each  **PCIe switch**  has 4 EFA (Elastic Fabric Adapter) NICs, meaning each GPU has access to 4 EFA adapters.  **EFA**  is AWS's custom high-performance network interface for cloud instances, designed to provide low-latency, high-throughput inter-instance communication. On p5 instances, EFA exposes a  **libfabric**  interface (a specific communication API for high performance computations) that applications can use, and provides RDMA-like capabilities that enable  **GPUDirect RDMA**  for direct GPU-to-GPU communication across nodes.

<Sidenote>

EFA uses a [reliable Ethernet-based transport protocol called Scalable Reliable Datagram (SRD)](https://ieeexplore.ieee.org/document/9167399), designed to use commodity datacenter networks (with a large number of network paths). Learn about its importance [here](https://aws.amazon.com/blogs/hpc/in-the-search-for-performance-theres-more-than-one-way-to-build-a-network/).
</Sidenote>

  

```
$ lstopo -v
...
## We can see 4 such EFA devices per each PCIe switch
PCIBridge L#8 (busid=0000:46:01.0 id=1d0f:0200 class=0604(PCIBridge) link=15.75GB/s buses=0000:[4f-4f] PCIVendor="Amazon.com, Inc.")
PCI L#6 (busid=0000:4f:00.0 id=1d0f:efa1 class=0200(Ethernet) link=15.75GB/s PCISlot=82-1 PCIVendor="Amazon.com, Inc.")
    OpenFabrics L#4 (NodeGUID=cd77:f833:0000:1001 SysImageGUID=0000:0000:0000:0000 Port1State=4 Port1LID=0x0 Port1LMC=1 Port1GID0=fe80:0000:0000:0000:14b0:33ff:fef8:77cd) "rdmap79s0"
...

$ fi_info --verbose
        fi_link_attr:
            address: EFA-fe80::14b0:33ff:fef8:77cd
            mtu: 8760            # maximum packet size is 8760 bytes
            speed: 100000000000  # each EFA link provides 100 Gbps of bandwidth
            state: FI_LINK_UP
            network_type: Ethernet

```
Each  **EFA link**  provides 100 Gbps (12.5 GB/s) of bandwidth. With  **4 EFA NICs**  per GPU and 8 GPUs per node, this gives an aggregate bandwidth of 100 × 4 × 8 =  **3200 Gbps per node**  (400GB/s). 

<Sidenote>

For a detailed exploration of how to fully harness this 3200 Gbps bandwidth using libfabric and EFA, see Lequn Chen's excellent blog series: [Harnessing 3200 Gbps Network: A Journey with RDMA, EFA, and libfabric](https://le.qun.ch/en/blog/2024/12/25/libfabric-efa-0-intro/).
</Sidenote>

  

To make sure we're enabling GPUDirect RDMA over EFA, you should set the  `FI_PROVIDER=efa`  and  `NCCL_P2P_DISABLE=1`  environment variables. When this mode is activated, you can verify it that it's working by setting  `NCCL_DEBUG=INFO` , which will show messages like:

```
NCCL INFO Channel 01/1 : 1[1] -> 0[0] [receive] via NET/Libfabric/0/GDRDMA/Shared
```

<Wide>

<HtmlEmbed
src="/embeds/aws-bandwidth-bottleneck.html"
config={{ initialFilter: 'gpu-gpu-efa-intranode' }}
caption="GPU-to-GPU communication path through Libfabric EFA. Note that this is less efficient for intranode communications compared to using NVLink."
/>
</Wide>



While GPUDirect RDMA over EFA provides significant improvements over CPU-mediated transfers, achieving around  **50 GB/s**  with 4 EFA cards per GPU, can we do even better? This is where NVLink comes into play.

#### Through NVLink

 **NVLink**  is NVIDIA's high-speed, direct GPU-to-GPU interconnect technology that enables fast multi-GPU communication within servers. The  **H100**  employs fourth-generation NVLink (NVLink 4.0), providing 900 GB/s bidirectional bandwidth per GPU through 18 links, each operating at 50 GB/s bidirectional ([NVIDIA H100 Tensor Core GPU Datasheet](https://resources.nvidia.com/en-us-hopper-architecture/nvidia-h100-tensor-c)). 

In the  **DGX H100**  architecture, 4 third-generation NVSwitches connect the 8 GPUs using a layered topology where each GPU connects with 5+4+4+5 links across the switches. This configuration ensures multiple direct paths between any GPU pair with a constant hop count of just 1 NVSwitch, resulting in 3.6 TB/s total bidirectional NVLink network bandwidth.

| NVLink 2.0 (Volta) | NVLink 3.0 (Ampere) | NVLink 4.0 (Hopper) | NVLink 5.0 (Blackwell) |
| --- | --- | --- | --- |
| Bandwidth | 300 GB/s | 600 GB/s | 900 GB/s | 1800 GB/s |

 *Table: NVLink bandwidth comparison across generations, showing theoretical specifications* 

By default, NCCL prioritizes NVLink for intra-node GPU communication when available, as it provides the lowest latency and highest bandwidth path between GPUs on the same machine. However, if you did not set your flags properly, you could be preventing the use of NVLink! 😱

NVLink enables direct GPU-to-GPU memory access without involving the CPU or system memory. When NVLink is unavailable, NCCL falls back to GPUDirect P2P over PCIe, or uses Shared Memory (SHM) transport when inter-socket PCIe transfers would be suboptimal.

To verify that NVLink is being used, set  `NCCL_DEBUG=INFO`  and look for messages like:

```
NCCL INFO Channel 00/1 : 0[0] -> 1[1] via P2P/CUMEM
```

<Sidenote>

**CUMEM**  indicates that the peer-to-peer operations use  **CUDA memory handles (cuMem API).**  [Learn more here.](https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/troubleshooting.html#cumem-host-allocations)
</Sidenote>

  

The following diagram illustrates the direct path that data takes when using NVLink:

<HtmlEmbed
src="/embeds/aws-bandwidth-bottleneck.html"
config={{ initialFilter: 'gpu-gpu-nvswitch' }}
caption="GPU-to-GPU communication path through NVLink."
/>



With  **NVLink 4.0** 's theoretical bandwidth of 900 GB/s compared to  **EFA** 's ~50 GB/s, we expect an 18x advantage for intra-node communication. To validate this in practice, we ran [NCCL's SendRecv performance test](https://github.com/NVIDIA/nccl-tests/blob/master/src/sendrecv.cu) to measure actual bandwidth across different communication paths:

```markdown
$ FI_PROVIDER=XXX NCCL_P2P_DISABLE=X sendrecv_perf -b 8 -e 8G -f 2 -g 1 -c 1 -n 100
```

<HtmlEmbed
  src="embeds/d3-line-chart.html"
  config={{
    dataUrl: "./data/nccl_sendrecv_p2p_comparison.csv",
    xColumn: "message_size",
    yColumn: "bandwidth",
    runColumn: "run_name",
    xScaleType: "log",
    xAxisLabel: "Message Size (bytes)",
    yAxisLabel: "Bandwidth (GB/s)"
  }}
  title="GPU->

GPU measured bandwidth with NCCL's SendRecv test (H100 GPUs, 1 Node, 2 GPUs)"
/>


This shows without a doubt how much more efficient NVLink is: it achieves 364.93 GB/s compared to EFA's 38.16 GB/s (9x faster, or 18x bidirectional) and the CPU baseline's 3.24 GB/s (112.6x faster). These measurements confirm why NCCL prioritizes NVLink for intra-node GPU communication, but to further check NVLink's performance, let's use  `nvbandwidth`  to measure bidirectional bandwidth between all GPU pairs using simultaneous copies in both directions:

```
./nvbandwidth -t device_to_device_bidirectional_memcpy_write_ce -b <message_size> -i 5
memcpy CE GPU(row) <-> GPU(column) Total bandwidth (GB/s)
           0         1         2         3         4         5         6         7
 0       N/A    785.81    785.92    785.90    785.92    785.78    785.92    785.90
 1    785.83       N/A    785.87    785.83    785.98    785.90    786.05    785.94
 2    785.87    785.89       N/A    785.83    785.96    785.83    785.96    786.03
 3    785.89    785.85    785.90       N/A    785.96    785.89    785.90    785.96
 4    785.87    785.96    785.92    786.01       N/A    785.98    786.14    786.08
 5    785.81    785.92    785.85    785.89    785.89       N/A    786.10    786.03
 6    785.94    785.92    785.99    785.99    786.10    786.05       N/A    786.07
 7    785.94    786.07    785.99    786.01    786.05    786.05    786.14       N/A

SUM device_to_device_bidirectional_memcpy_write_ce_total 44013.06
```
The measured bidirectional bandwidth of  **786 GB/s**  represents 85% of  **NVLink 4.0** 's theoretical 900 GB/s specification. Using NVLink has bypassed the CPU bottleneck entirely (for gpu-to-gpu communication)!

But how does this translate to collective communication patterns? Let's measure  `allreduce`  performance within a single node with the  `all_reduce_perf`  [benchmark from NCCL tests.](https://github.com/NVIDIA/nccl-tests/blob/master/src/all_reduce.cu) 

<Sidenote>

For a quick refresher on collective communication patterns, see the [UltraScale Playbook Appendix](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=ring_allreduce).
</Sidenote>

```
$ ./all_reduce_perf -b 8 -e 16G -f 2 -g 1 -c 1 -n 100
```

<HtmlEmbed src="/embeds/nccl-all-reduce-singlenode-bandwidth-plot.html" title="NCCL's All-Reduce performance test intranode" />



<Sidenote>

For comprehensive benchmarking scripts and configurations, see the excellent collection at [AWS Distributed Training Samples](https://github.com/aws-samples/awsome-distributed-training/tree/main/micro-benchmarks/nccl-tests).
</Sidenote>

But wait… We are achieving 480 GB/s, which exceeds the theoretical unidirectional bandwidth of 450 GB/s for NVLink 4.0 😮 What is this sorcery, and how is this possible? 

Diving a bit in the docs, it seems like the answer lies in  **NVLink SHARP (NVLS)** , NVIDIA's hardware-accelerated collective operations technology. They provide approximately 1.3x speedup for allreduce operations on a single node with H100 GPUs!

<Image src={image_2891384e_bcac_80e2_9cc5_c2c46c7ab39b} alt="Image" />

For technical details on how NVSwitch enables these hardware-accelerated collective operations, see the [NVSwitch architecture presentation](https://hc34.hotchips.org/assets/program/conference/day2/Network%20and%20Switches/NVSwitch%20HotChips%202022%20r5.pdf).

Can they help in other places too? Let's examine [alltoall performance](https://github.com/NVIDIA/nccl-tests/blob/master/src/alltoall.cu):

```
$ ./all_to_all_perf -b 8 -e 16G -f 2 -g 1 -c 1 -n 100
```

<HtmlEmbed src="/embeds/nccl-alltoall-singlenode-bandwidth-plot.html" title="NCCL's All-to-All performance test intranode" />



We achieve  **340 GB/s**  for alltoall operations, which aligns with published benchmarks showing similar performance characteristics for H100 systems with NVLink 4.0 ([source](https://juser.fz-juelich.de/record/1019178/files/02-NCCL_NVSHMEM.pdf#page=20.00)). Unlike allreduce, alltoall operations don't benefit from NVLS hardware acceleration, which explains why we see 340 GB/s here compared to the 480 GB/s achieved with allreduce. The alltoall pattern requires more complex point-to-point data exchanges between all GPU pairs, relying purely on NVLink's base bandwidth rather than NVSwitch's collective acceleration features.

<Sidenote>

For custom NVLink communication patterns, keep an eye on PyTorch's [SymmetricMemory API](https://dev-discuss.pytorch.org/t/pytorch-symmetricmemory-harnessing-nvlink-programmability-with-ease/2798), which enables fine-grained control over NVLink and NVLS operations.
</Sidenote>

<Note title="Advanced Kernel Optimization" emoji="⚡" variant="info">

Some optimized kernels separate NVLink communication from compute by assigning dedicated warps to handle transfers. For example, ThunderKittens uses a warp-level design where specific warps issue NVLink transfers and wait for completion, while other warps continue compute operations. This fine-grained overlap of SM compute and NVLink communication can hide most inter-GPU communication latency. For implementation details, see the <a href="https://hazyresearch.stanford.edu/blog/2025-09-22-pgl#fine-grained-overlap-of-sm-compute-and-nvlink-communication-with-thunderkittens" target="_blank">ThunderKittens blog post on multi-GPU kernels</a>.
</Note>

While NVLink provides exceptional bandwidth within a single node, training frontier models requires scaling across multiple nodes. 

This introduces a new potential bottleneck: the inter-node network interconnect, which operates at significantly lower bandwidths than NVLink.

#### GPU-to-GPU Internode

<Note>

<b>TL;DR</b> Multi-node GPU communication uses high-speed networks like InfiniBand (400 Gbps) or RoCE (100 Gbps). Allreduce scales well (320-350 GB/s stable across nodes), enabling massive training clusters. Alltoall degrades more sharply due to algorithm complexity. Latency jumps from ~13μs intra-node to 55μs+ inter-node. For MoE workloads requiring frequent all-to-all operations, NVSHMEM offers asynchronous GPU-initiated communication with significantly better performance than CPU-orchestrated transfers.
</Note>

As models scale beyond what a single node can accommodate, training requires distributing computation across multiple nodes connected via high-speed networks. Before diving into the benchmarks, let's look at the 3 key networking technologies connecting nodes you'll encounter in multi-node GPU clusters:

-  **Ethernet**  has evolved from 1 Gbps to 100+ Gbps speeds and remains widely used in HPC and data center clusters.
-  **RoCE (RDMA over Converged Ethernet)**  brings RDMA capabilities to Ethernet networks, using ECN for congestion control instead of traditional TCP mechanisms.
-  **InfiniBand**  is NVIDIA's industry-standard switch fabric, providing up to 400 Gbps bandwidth and sub-microsecond latency with RDMA support that enables direct GPU-to-GPU memory access while bypassing the host CPU through GPUDirect RDMA.

As a summary:

| Name | Ethernet (25–100 Gbps) | Ethernet (200–400 Gbps) | RoCE | Infiniband |
| --- | --- | --- | --- | --- |
| Manufacturer | Many | Many | Many | NVIDIA/Mellanox |
| Unidirectional Bandwidth (Gbps) | 25–100 | 200–400 | 100 | 400 |
| End to End Latency (μs) | 10-30 | N/A | ~1 | \<1 |
| RDMA | No | No | Yes | Yes |

Table: Comparison of Interconnects. Source: [https://www.sciencedirect.com/science/article/pii/S2772485922000618](https://www.sciencedirect.com/science/article/pii/S2772485922000618)

For AWS p5 instances we have [Elastic Fabric Adapter (](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html)[ **EFA** ](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html)[)](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html) as the  **NIC**  (Network Interface Card), where each GPU connects to four 100 Gbps EFA network cards through PCIe Gen5 x16 lanes, as we've seen before.

<Wide>

<HtmlEmbed src="/embeds/aws-bandwidth-bottleneck.html" config={{ initialFilter: 'gpu-gpu-efa-internode' }} caption="Internode GPU-to-GPU communication path through Libfabric EFA" />
</Wide>



As illustrated above, when GPUs and network cards are connected to the same PCIe switch,  **GPUDirect RDMA**  enables their communication to occur solely through that switch. This setup allows for full utilization of the PCIe Gen5 x16 bandwidth and avoids involving other PCIe switches or the CPU memory bus.
Theoretically, 8 PCIe Switches per node x 4 EFA NICs per switch x 100 Gbps each EFA NIC gives  **3200 Gbps**(400GB/s)**  of bandwidth which is the bandwidth we find in [AWS p5's specs).](https://aws.amazon.com/ec2/instance-types/p5/) So how does it hold in practice? Let's find out by running the same benchmarks as before but across different nodes!

 **Bandwidth Analysis** 

<Wide>

<HtmlEmbed src="/embeds/nccl-bandwidth-comparison.html" caption="Bandwidth scaling of collective operations across different number of nodes on our AWS p5 instances, using recommendations from <a href='https://github.com/aws-samples/awsome-distributed-training/blob/main/micro-benchmarks/nccl-tests/slurm/nccl-tests-container.sbatch'>

aws-samples/awsome-distributed-training.</a>"/>
</Wide>



Point-to-point send/receive operations achieve around  **42-43 GB/s**  for 2-4 nodes but drop to approximately 21 GB/s for 5+ nodes. This performance degradation occurs because NCCL automatically reduces the number of point-to-point channels per peer from 2 to 1 when scaling beyond 4 nodes, effectively halving the available bandwidth utilization, while theoretical maximum remains ~50 GB/s (4 EFA NICs × 12.5 GB/s each). We successfully managed to restore the full throughput for this test on 5+ nodes by setting  `NCCL_NCHANNELS_PER_NET_PEER=2` , though this flag should be used with caution as it may degrade all-to-all performance for example (see [GitHub issue #1272](https://github.com/NVIDIA/nccl/issues/1272) for details).

The all-reduce operation demonstrates excellent performance within a single node, achieving  **480 GB/s**  of bus bandwidth. When scaling to 2 nodes, bandwidth remains nearly identical at 479 GB/s, after which it stabilizes at around 320-350 GB/s for 3-16 nodes. This pattern reveals an important characteristic: while there's an initial drop when crossing node boundaries due to the transition from NVLink to the inter-node network fabric,  *the bandwidth then scales almost constantly as we add more nodes.* 

<Note title="Scaling All-Reduce Across Nodes" emoji="💡" variant="tip">

<p>
This near-constant scaling behavior beyond 2 nodes is actually quite encouraging for large-scale training. The relatively stable 320-350 GB/s across 3-16 nodes suggests that parallelism strategies relying on all-reduce operations (for example, in data parallelism) can scale to hundreds or even thousands of GPUs without significant per-GPU bandwidth degradation. This logarithmic scaling characteristic is typical of well-designed multi-tier network topologies using 8-rail optimized fat trees, where each of the 8 GPUs connects to a separate switch rail to maximize bisection bandwidth. Modern frontier training clusters routinely operate at 100,000+ GPUs, and this stable scaling behavior is what makes such massive deployments feasible.
</p>
<p>
When working with different bandwidth links (NVLink within nodes vs. inter-node network), consider adapting your parallelism strategy to each bandwidth tier to fully utilize all available bandwidths. See the <a href="[https://huggingface.co/spaces/nanotron/ultrascale-playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)">Ultrascale playbook</a> for detailed guidance on optimizing parallelism configurations for heterogeneous network topologies.
</p>
</Note>

The all-to-all operation shows more dramatic scaling challenges: starting at 344 GB/s for a single node, bandwidth drops to 81 GB/s at 2 nodes and continues declining to approximately 45-58 GB/s for larger clusters. This steeper degradation reflects the all-to-all pattern's intensive network demands, where each GPU must communicate with every other GPU across nodes, creating significantly more network congestion than all-reduce operations.

 **Latency Analysis** 

<Wide>

<HtmlEmbed src="/embeds/nccl-latency-comparison.html" caption="Latency scaling of collective operations across different number of nodes on our AWS p5 instances, using recommendations from <a href='https://github.com/aws-samples/awsome-distributed-training/blob/main/micro-benchmarks/nccl-tests/slurm/nccl-tests-container.sbatch'>

[aws-samples/awsome-distributed-training](https://github.com/aws-samples/awsome-distributed-training/blob/main/micro-benchmarks/nccl-tests/slurm/nccl-tests-container.sbatch)</a>." />
</Wide>



 **Latency**  measurements reveal the fundamental cost of crossing node boundaries. Send/receive operations maintain relatively stable latencies of  **40-53 μs**  across all multi-node configurations, demonstrating that point-to-point communication latency is primarily determined by the base network round-trip time rather than cluster size, though some variation suggests network topology and routing effects still play a role.

All-reduce operations show minimal latency of  **12.9 μs**  within a single node, but this jumps to  **55.5 μs**  for 2 nodes and continues increasing nearly linearly with cluster size, reaching  **235 μs** at 16 nodes. This progression reflects both the increased communication distance and the growing complexity of the reduction tree across more nodes.

All-to-all operations exhibit similar trends, starting at  **7.6 μs**  for single-node communication but climbing to  **60 μs**  at 2 nodes and reaching  **621**  μs at 16 nodes. The superlinear growth in latency for all-to-all operations indicates that network congestion and coordination overhead compound as more nodes participate in the collective.

<Note title="NVSHMEM for Optimized GPU Communication" emoji="🚀" variant="info">

<p>
With the rise of Mixture of Experts (MoE) architectures, which require frequent all-to-all communication patterns for expert routing, optimized GPU communication libraries have become increasingly critical. 
</p>
<p>
<a href="https://developer.nvidia.com/nvshmem" target="_blank">NVSHMEM</a> is gaining significant traction as a high-performance communication library that combines the memory of multiple GPUs into a partitioned global address space (PGAS). Unlike traditional MPI-based approaches that rely on CPU-orchestrated data transfers, NVSHMEM enables asynchronous, GPU-initiated operations that eliminate CPU-GPU synchronization overhead. 
</p>
<p>
NVSHMEM offers several key advantages for GPU communication: Through technologies like GPUDirect Async, GPUs can bypass the CPU entirely when issuing internode communication, achieving up to 9.5x higher throughput for small messages (\<1 KiB). This is particularly beneficial for collective operations that require intensive network communication patterns. 
</p>
<p>
The library currently supports InfiniBand/RoCE with Mellanox adapters (CX-4 or later), Slingshot-11 (Libfabric CXI), and Amazon EFA (Libfabric EFA). For applications requiring strong scaling with fine-grain communication, NVSHMEM's low-overhead, one-sided communication primitives can significantly improve performance compared to traditional CPU-proxy methods. 
</p>
<p>
Learn more in the <a href="https://developer.nvidia.com/nvshmem" target="_blank">NVSHMEM documentation</a> and this detailed <a href="https://developer.nvidia.com/blog/improving-network-performance-of-hpc-systems-using-nvidia-magnum-io-nvshmem-and-gpudirect-async/" target="_blank">blog post on GPUDirect Async</a>.
</p>
</Note>

When bandwidth measurements fall short of expectations, several factors could be limiting performance. Understanding these potential bottlenecks is essential for achieving optimal interconnect utilization.

#### Troubleshooting Interconnect

If you're experiencing lower than expected bandwidth, systematically check the following areas:

 **Library Versions** 

Outdated NCCL, EFA, or CUDA libraries may lack critical performance optimizations or bug fixes. Always verify you're running recent, compatible versions of all communication libraries. e.g. AWS regularly updates their Deep Learning AMIs with optimized library versions for their hardware. It's also recommended to log these library versions for important experiments.

 **CPU Affinity Configuration** 

Improper CPU affinity settings can significantly impact NCCL performance by causing unnecessary cross-NUMA traffic. Each GPU should be bound to CPUs on the same NUMA node to minimize memory access latency. In practice, [this Github issue](https://github.com/NVIDIA/nccl/issues/1017#issuecomment-1751385723) demonstrates how using  `NCCL_IGNORE_CPU_AFFINITY=1`  and  `--cpu-bind none`  helped reduce container latency significantly. [You can read more about it here.](https://enterprise-support.nvidia.com/s/article/understanding-numa-node-for-performance-benchmarks#Mapping-between-PCI-device-driver-port-and-NUMA)

 **Network Topology and Placement** 

Understanding your network topology is crucial for diagnosing performance issues. Cloud placement groups, while helpful, don't guarantee minimal network hops between instances. In modern datacenter fat-tree topologies, instances placed under different top-level switches will experience higher latency and potentially lower bandwidth due to additional network hops in the routing path.

For  **AWS EC2**  users, the [Instance Topology API](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/how-ec2-instance-topology-works.html) provides valuable visibility into network node placement. Instances sharing t"he same network node at the bottom layer (directly connected to the instance) are physically closest and will achieve the lowest latency communication.

<FullWidth>

<Reference caption="Network topology visualization showing instance placement.">

```mermaid
graph TD
    %% Single Level 1 node
    L1["Level 1<br/>nn-4aed5..."]:::level1

    %% Single Level 2 node
    L2["Level 2<br/>nn-48b0a..."]:::level2

    %% 12 nodes of Level 3
    L3_1["Level 3<br/>nn-d2ad4..."]:::level3
    L3_2["Level 3<br/>nn-2d36a..."]:::level3
    L3_3["Level 3<br/>nn-65fc9..."]:::level3
    L3_4["Level 3<br/>nn-fbb73..."]:::level3
    L3_5["Level 3<br/>nn-65290..."]:::level3
    L3_6["Level 3<br/>nn-27373..."]:::level3
    L3_7["Level 3<br/>nn-5adeb..."]:::level3
    L3_8["Level 3<br/>nn-dbe4f..."]:::level3
    L3_9["Level 3<br/>nn-fde84..."]:::level3
    L3_10["Level 3<br/>nn-3c5c0..."]:::level3
    L3_11["Level 3<br/>nn-94247..."]:::level3
    L3_12["Level 3<br/>nn-8f3c1..."]:::level3

    %% L1 -> L2
    L1 --> L2

    %% L2 -> L3 (12 arrows)
    L2 --> L3_1
    L2 --> L3_2
    L2 --> L3_3
    L2 --> L3_4
    L2 --> L3_5
    L2 --> L3_6
    L2 --> L3_7
    L2 --> L3_8
    L2 --> L3_9
    L2 --> L3_10
    L2 --> L3_11
    L2 --> L3_12

    %% Distribution Level 3 -> Leaf nodes (instance info)
    %% 1st Level 3 has 2 leaves
    L3_1 --> L4_1["ID: 02e1b4f9<br/>ip-26-0-171-102<br/>p5.48xlarge"]:::level4
    L3_1 --> L4_2["ID: 05388ebf<br/>ip-26-0-171-230<br/>p5.48xlarge"]:::level4

    %% 2nd, 3rd, 4th have 1 each
    L3_2 --> L4_3["ID: 03bfac00<br/>ip-26-0-168-30<br/>p5.48xlarge"]:::level4
    L3_3 --> L4_4["ID: d92bab46<br/>ip-26-0-168-95<br/>p5.48xlarge"]:::level4
    L3_4 --> L4_5["ID: 97a542e4<br/>ip-26-0-163-158<br/>p5.48xlarge"]:::level4

    %% 5th has 3
    L3_5 --> L4_6["ID: e2c87e43<br/>ip-26-0-167-9<br/>p5.48xlarge"]:::level4
    L3_5 --> L4_7["ID: afa887ea<br/>ip-26-0-168-120<br/>p5.48xlarge"]:::level4
    L3_5 --> L4_8["ID: 66c12e70<br/>ip-26-0-167-177<br/>p5.48xlarge"]:::level4

    %% 6th, 7th, 8th have 1 each
    L3_6 --> L4_9["ID: 9412bdf3<br/>ip-26-0-168-52<br/>p5.48xlarge"]:::level4
    L3_7 --> L4_10["ID: 87bd4dc8<br/>ip-26-0-167-111<br/>p5.48xlarge"]:::level4
    L3_8 --> L4_11["ID: b001549b<br/>ip-26-0-166-244<br/>p5.48xlarge"]:::level4

    %% 9th has 2
    L3_9 --> L4_12["ID: 10ed8172<br/>ip-26-0-107-245<br/>p5.48xlarge"]:::level4
    L3_9 --> L4_13["ID: 7c1d0a09<br/>ip-26-0-168-238<br/>p5.48xlarge"]:::level4

    %% 10th, 11th, 12th have 1 each
    L3_10 --> L4_14["ID: 925ce932<br/>ip-26-0-167-217<br/>p5.48xlarge"]:::level4
    L3_11 --> L4_15["ID: c9bc34db<br/>ip-26-0-171-168<br/>p5.48xlarge"]:::level4
    L3_12 --> L4_16["ID: 328d5d04<br/>ip-26-0-167-127<br/>p5.48xlarge"]:::level4

    %% Styles
    classDef level1 fill:#c8e6c9
    classDef level2 fill:#e1f5fe
    classDef level3 fill:#fff9c4
    classDef level4 fill:#ffcdd2
```
</Reference>

</FullWidth>



Minimizing network hops between communicating nodes directly translates to better interconnect performance. For small-scale experiments and ablations, ensuring your instances are co-located on the same network switch can make a measurable difference in both latency and bandwidth utilization.

 **Correct Environment Variables** 

Missing or incorrect environment variables for your network adapter can severely limit bandwidth utilization. Communication libraries like NCCL rely on specific configuration flags to enable optimal performance features such as adaptive routing, GPU-initiated transfers, and proper buffer sizing.

For example, when using AWS EFA (Elastic Fabric Adapter), ensure you're setting the recommended NCCL and EFA environment variables for your instance type. The [AWS EFA cheatsheet](https://github.com/aws-samples/awsome-distributed-training/blob/main/1.architectures/efa-cheatsheet.md) provides comprehensive guidance on optimal flag configurations for different scenarios.

 **Container-specific Considerations** 

When using containers (Docker/Enroot), several configuration steps are critical for optimal NCCL performance:

-  **Shared and Pinned Memory** : Docker containers default to limited shared and pinned memory resources. Launch containers with  `-shm-size=1g --ulimit memlock=-1`  to prevent initialization failures.
-  **NUMA Support** : Docker disables NUMA support by default, which can prevent cuMem host allocations from working correctly. Enable NUMA support by invoking Docker with  `-cap-add SYS_NICE` .
-  **PCI Topology Discovery** : Ensure  `/sys`  is properly mounted to allow NCCL to discover the PCI topology of GPUs and network cards. Having  `/sys`  expose a virtual PCI topology can result in sub-optimal performance.

<Note title="Community Troubleshooting" emoji="🤗" variant="tip">

We're gathering troubleshooting findings here as a community effort. If you've encountered performance issues or discovered effective debugging methods, please jump to the [Discussion Tab](https://huggingface.co/spaces/HuggingFaceTB/smol-training-playbook/discussions) and share your experience to help others optimize their interconnect utilization.
</Note>

Now that you know how to debug bottlenecks in GPU-CPU and GPU-GPU communication let's have a look at a GPU communication part that typically gets less attention, namely communication with the storage layer! 

#### GPU-to-Storage

The connection between GPUs and storage systems is often overlooked but can significantly impact training efficiency. During training, GPUs need to continuously read data from storage (data loading, especially for multimodal data with large image/video files) and periodically write model states back to storage (aka checkpointing). For modern large-scale training runs, these I/O operations can become bottlenecks if not properly optimized.

<Note>

<b>TL;DR:</b> GPU-storage I/O impacts training through data loading and checkpointing. GPUDirect Storage (GDS) enables direct GPU-to-storage transfers, bypassing CPU for better performance. Even without GDS enabled in our cluster, local NVMe RAID (8×3.5TB drives in RAID 0) delivers 26.59 GiB/s and 337K IOPS (6.3x faster than network storage), making it ideal for checkpoints.
</Note>

 **Understanding Storage Topology** 

The physical connection between GPUs and storage devices follows a similar hierarchical structure to GPU interconnects. Storage devices connect through PCIe bridges, and understanding this topology helps explain performance characteristics and potential bottlenecks.

Looking at the system topology from  `lstopo` , we can see how NVMe drives connect to the system. In our p5 instance, we have 1 NVMe SSD per GPU:

```
PCIBridge L#13 (busid=0000:46:01.5 id=1d0f:0200 class=0604(PCIBridge) link=15.75GB/s buses=0000:[54-54] PCIVendor="Amazon.com, Inc.")
PCI L#11 (busid=0000:54:00.0 id=1d0f:cd01 class=0108(NVMExp) link=15.75GB/s PCISlot=87-1 PCIVendor="Amazon.com, Inc." PCIDevice="NVMe SSD Controller")
    Block(Disk) L#9 (Size=3710937500 SectorSize=512 LinuxDeviceID=259:2 Model="Amazon EC2 NVMe Instance Storage" Revision=0 SerialNumber=AWS110C9F44F9A530351) "nvme1n1"
```
A natural question would be whether GPUs can directly access NVMe drives without involving the CPU. The answer is yes, through  **GPUDirect Storage (GDS)** .

 **GPUDirect Storage**  is part of NVIDIA's [GPUDirect](https://developer.nvidia.com/gpudirect) family of technologies that enables a direct data path between storage (local NVMe or remote NVMe-oF) and GPU memory. It eliminates unnecessary memory copies through CPU bounce buffers by allowing the DMA engine near the storage controller to move data directly into or out of GPU memory. This reduces CPU overhead, decreases latency, and significantly improves I/O performance for data-intensive workloads like training on large multimodal datasets.

To verify if GPUDirect Storage is properly configured on your system, you can check the GDS configuration file and use the provided diagnostic tools:

```
$ /usr/local/cuda/gds/tools/gdscheck.py -p
 =====================
 DRIVER CONFIGURATION:
 =====================
 NVMe               : Supported   
 NVMeOF             : Unsupported
 SCSI               : Unsupported
 ScaleFlux CSD      : Unsupported
 NVMesh             : Unsupported
 DDN EXAScaler      : Unsupported
 IBM Spectrum Scale : Unsupported
 NFS                : Unsupported
 BeeGFS             : Unsupported
 WekaFS             : Unsupported
 Userspace RDMA     : Unsupported
 --Mellanox PeerDirect : Enabled
 --rdma library        : Not Loaded (libcufile_rdma.so)
 --rdma devices        : Not configured
 --rdma_device_status  : Up: 0 Down: 0
 =====================
```
We see  `NVMe: Supported`  which informs that GDS is currently configured to work for NVMe drives, and all other storage types are not properly configured as apparent from the Unsupported flag. If GDS is not properly configured for your storage type, refer to the [NVIDIA GPUDirect Storage Configuration Guide](https://docs.nvidia.com/gpudirect-storage/configuration-guide/index.html) for instructions on modifying the configuration file at  `/etc/cufile.json` .

 **Block Storage Devices** 

To understand the storage devices available on your system, you can use  `lsblk`  to display the block device hierarchy:

```
$ lsblk --fs -M
    NAME        FSTYPE            LABEL                   UUID                                 FSAVAIL FSUSE% MOUNTPOINT
...
    nvme0n1
    └─nvme0n1p1 ext4              cloudimg-rootfs         24ec7991-cb5c-4fab-99e5-52c45690ba30  189.7G    35% /
┌┈▶ nvme1n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
├┈▶ nvme2n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
├┈▶ nvme3n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
├┈▶ nvme8n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
├┈▶ nvme5n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
├┈▶ nvme4n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
├┈▶ nvme6n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
└┬▶ nvme7n1     linux_raid_member ip-26-0-164-236:MY_RAID d0795631-71f0-37e5-133b-e748befec126
 └┈┈md0         xfs                                       dddb6849-e5b5-4828-9034-96da65da27f0   27.5T     1% /scratch
```
This output shows the block device hierarchy on the system. The key observations:

-  `nvme0n1p1`  is the root [Amazon EBS](https://aws.amazon.com/ebs/) filesystem mounted at  `/` , using 35% of its full ~300GB capacity
- Eight NVMe drives ( `nvme1n1`  through  `nvme8n1` ) are configured as a RAID array named  `MY_RAID` 
- The RAID array is exposed as  `/dev/md0` , formatted with XFS, and mounted at  `/scratch`  with 28TB available (8x3.5TB)

The arrows (┈▶) indicate that multiple NVMe devices are members of the same RAID array, which then combines into the single  `md0`  device.

<Sidenote>

[Amazon Elastic Block Store (EBS)](https://aws.amazon.com/ebs/) is a high-performance, scalable block storage service designed for use with Amazon EC2 instances.
</Sidenote>

  

 **Network Storage** 

In addition to local NVMe storage, the system has access to network-attached storage systems:

```
$ df -h
Filesystem                                         Size  Used Avail Use% Mounted on
/dev/root                                          291G  101G  190G  35% /
weka-hopper.hpc.internal.huggingface.tech/default  393T  263T  131T  67% /fsx
10.53.83.155@tcp:/fg7ntbev                         4.5T  2.9T  1.7T  63% /admin
/dev/md0                                            28T  206G   28T   1% /scratch
```
This output shows:

-  `/dev/root`  (291GB [Amazon EBS](https://aws.amazon.com/ebs/)) is the root filesystem at 35% capacity
-  `/fsx`  (393TB WekaFS) is 67% full with 131TB available
-  `/admin`  (4.5TB FSx Lustre) is 63% full with 1.7TB available
-  `/dev/md0`  (28TB local NVMe RAID) is only 1% full with 28TB available at  `/scratch` . This is our 8×3.5TB SSD NVMe instance store drives in RAID.

<Sidenote>

Note that  `/fsx`  isn't actually Amazon FSx, but WekaFS. We kept the same mount point name for convenience when we migrated from FSx to WekaFS.
</Sidenote>

The local NVMe RAID array ( `/scratch` ) provides the fastest I/O performance, while the network filesystems offer larger capacity for shared data storage.

<Note title="Storage Technologies" emoji="💾" variant="info">

<p>
    <b>RAID (Redundant Array of Independent Disks)</b>: Combines multiple drives
    to improve performance and/or reliability through data striping, parity, or
    mirroring.
  </p>
  <p>
    <b>NVMe (Non-Volatile Memory Express)</b>: High-performance storage protocol
    for SSDs that connects directly to PCIe, delivering higher throughput and
    lower latency than SATA/SAS.
  </p>
  <p>
    <b>WekaFS</b>: A high-performance parallel file system designed for AI/ML
    workloads, providing low-latency access and high throughput across multiple
    nodes.
  </p>
  <p>
    <b>FSx Lustre</b>: A parallel file system designed for HPC that separates
    metadata and data services across different servers to enable parallel
    access. While effective for large files, it can struggle with
    metadata-intensive AI/ML workloads involving many small files.
  </p>
</Note>

 **Benchmarking Storage Bandwidth** 

To understand the performance characteristics of each storage system, we can benchmark their read/write speeds using GPUDirect Storage (GDS). Here's a comprehensive parametric benchmark script that tests various configurations:

```
gdsio -f /<disk_path>/gds_test.dat -d 0 -w <n_threads> -s 10G -i <io_size> -x 1 -I 1 -T 10
```
The benchmark evaluates storage system performance across Throughput, Latency, IOPS but also:

 **Scalability** : How performance changes with different thread counts and I/O sizes. This reveals optimal configurations for different workload patterns:

- Small I/O sizes (64K to 256K) typically maximize IOPS but may not saturate bandwidth
- Large I/O sizes (2M to 8M) typically maximize throughput but reduce IOPS
- Thread count affects both: more threads can increase total IOPS and throughput up to hardware limits

 **Transfer Method Efficiency** : Comparing GPU_DIRECT vs CPU_GPU vs CPUONLY shows the benefit of bypassing CPU memory:

-  **GPU_DIRECT** : Uses RDMA to transfer data directly to GPU memory, bypassing CPU entirely (lowest latency, highest efficiency, best IOPS for small operations)
-  **CPU_GPU** : Traditional path where data goes to CPU memory first, then copied to GPU (adds CPU overhead and memory bandwidth contention, reduces effective IOPS)
-  **CPUONLY** : Baseline CPU-only I/O without GPU involvement

<Note title="IOPS (I/O Operations Per Second)" emoji="📊" variant="info">

<p>
    IOPS is the number of individual I/O operations completed per second.
    Calculated as <code>ops / total_time</code> from the gdsio output. IOPS is
    particularly important for:
  </p>
  <ul>
    <li>Random access patterns with small I/O sizes</li>
    <li>Workloads with many small files or scattered data access</li>
    <li>
      Database-like operations where latency per operation matters more than raw
      bandwidth
    </li>
    <li>
      Higher IOPS indicates better ability to handle concurrent, fine-grained
      data access
    </li>
  </ul>
</Note>

<HtmlEmbed src="/embeds/gds-interactive-heatmaps.html" caption="Benchmark results comparing storage system performance across varying thread counts and I/O sizes. The heatmaps visualize throughput (GiB/s) and IOPS patterns, revealing optimal configurations for each storage tier. Note: GPUDirect Storage (GDS) is not currently supported in this cluster configuration."/>



The benchmarks reveal dramatic performance differences across our four storage systems:

 **/scratch (Local NVMe RAID)**  dominates with  **26.59 GiB/s**  throughput and  **337K IOPS** , making it 6.3× faster than FSx for throughput and 6.6× better for IOPS. This local RAID array of 8×3.5TB NVMe drives delivers the lowest latency (190μs at peak IOPS) and scales exceptionally well with thread count, achieving peak performance at 64 threads with 1M I/O sizes for throughput.

 **/fsx (WekaFS)**  provides solid network storage performance at  **4.21 GiB/s**  and  **51K IOPS** , making it the best choice for shared data that needs reasonable performance. FSx achieves its best throughput (4.21 GiB/s) using CPUONLY transfer, while its best IOPS (51K) uses GPUD transfer type.

 **/admin (FSx Lustre)**  and  **/root (EBS)**  filesystems show similar modest performance around  **1.1 GiB/s**  throughput, but differ significantly in IOPS capability. Admin achieves its peak throughput (1.13 GiB/s) with GPUD transfer and peaks at 17K IOPS with CPU_GPU transfer (24× better than Root), making it more suitable for workloads with many small operations. Root's poor IOPS performance (730) confirms it's best suited for large sequential operations only.

<Note>

<b>Note on GPU_DIRECT Results:</b> GPUDirect Storage (GDS) is not currently enabled in our cluster, which explains why GPUD results for NVMe storage (Scratch and Root) underperform compared to CPUONLY transfers. With GDS properly configured, we would expect GPUD to show significant advantages for direct GPU-to-storage transfers, particularly for the high-performance NVMe arrays.
</Note>

 **Optimal Configuration Patterns** : Across all storage types, maximum throughput occurs at 1M I/O sizes, while maximum IOPS occurs at the smallest tested size (64K). This classic tradeoff means choosing between raw bandwidth (large I/O) and operation concurrency (small I/O) based on workload characteristics. For ML training with large checkpoint files, the 1M-8M range on Scratch provides optimal performance.

#### Summary

If you've made it this far, congratulations! You now have a comprehensive understanding of the storage hierarchy and how different components interact in our training infrastructure. But here's the key insight we hope you take home:  **identifying bottlenecks is what separates theoretical knowledge from practical optimization** .

Throughout this guide, we've measured actual bandwidths at every level of the stack: HBM3's 3TB/s within a single GPU, NVLink's 786 GB/s between GPUs in a node, PCIe Gen4 x8's 14.2 GB/s for CPU-GPU transfers, inter-node network's 42 GB/s for point-to-point communication, and storage systems ranging from 26.59 GB/s (local NVMe) down to 1.1 GB/s (shared filesystems). These measurements reveal where your training pipeline will slow down and are essential for achieving high Model FLOPs Utilization (MFU).

However, raw bandwidth numbers alone don't tell the complete story. Modern training systems can  **overlap computation with communication** , effectively hiding communication costs behind compute operations. This parallelization helps alleviate the bottleneck even when interconnects is slow. For detailed strategies on overlapping compute and communication to maximize throughput, see the Ultra-Scale Playbook.

The diagram below synthesizes all our benchmarked measurements into a single view, showing how bandwidth decreases dramatically as we move further from the GPU:

<Wide>

<HtmlEmbed
src="/embeds/aws-bandwidth-bottleneck.html"
config={{ showRealBandwidths: true }}
/>
</Wide>

Now that we know how to identify bottlenecks in our hardware and software setup, lets see how we can go one step further and ensure we have a resilient system that can run stable for months.

### Building Resilient Training Systems

Having fast hardware is just the entry ticket to having good and stable infrastructure for LLM training. To go from a training amateur to a professional, we need to think beyond raw speed and focus on the less glamorous but critical infrastructure pieces that make the entire training experience smoother and with minimal downtime. 

In this section, we shift from hardware & software optimization to  **production readiness** : building systems  **robust**  enough to survive inevitable failures,  **automated**  enough to run without constant babysitting, and  **flexible**  enough to adapt when things go wrong.

####  **Node Health Monitoring and Replacement** 

Having enough fast GPUs is important for training, but since LLM trainings run for weeks or months rather than single days, tracking GPU health over time becomes critical. GPUs that pass initial benchmarks can develop thermal throttling, memory errors, or performance degradation during extended training runs. In this section, we will share how we approach this challenge and the tools we use.

 **Upfront tests:**  Before launching SmolLM3, we ran comprehensive GPU diagnostics using multiple tools. We used [GPU Fryer](https://github.com/huggingface/gpu-fryer), an internal tool that stress-tests GPUs for thermal throttling, memory errors, and performance anomalies. We also ran [NVIDIA's DCGM diagnostics](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html), a widely-used tool for validating GPU hardware, monitoring performance, and identifying root causes of failures or power anomalies through deep diagnostic tests covering compute, PCIe connectivity, memory integrity, and thermal stability. These upfront tests caught two problematic GPUs that would have caused issues during training.

You can see in the following table what can be tested with the DCGM diagnostic tool:

| Test Level | Duration | Software | PCIe + NVLink | GPU Memory | Memory Bandwidth | Diagnostics | Targeted Stress | Targeted Power | NVBandwidth | Memory Stress | Input EDPp |
| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |
| r1 (Short) | Seconds | ✓ | ✓ | ✓ |
| r2 (Medium) | \< 2 mins | ✓ | ✓ | ✓ | ✓ | ✓ |
| r3 (Long) | \< 30 mins | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |
| r4 (Extra Long) | 1-2 hours | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |

 *DCGM diagnostic run levels. Source:* [ *NVIDIA DCGM Documentation* ](https://docs.nvidia.com/datacenter/dcgm/latest/user-guide/dcgm-diagnostics.html#run-levels-and-tests)

```
$ dcgmi diag -r 2 -v -d VERB
Successfully ran diagnostic for group.
+---------------------------+------------------------------------------------+
| Diagnostic | Result |
+===========================+================================================+
| -----  Metadata  ----------+------------------------------------------------ |
| DCGM Version | 3.3.1 |
| Driver Version Detected | 575.57.08 |
| GPU Device IDs Detected | 2330,2330,2330,2330,2330,2330,2330,2330 |
| -----  Deployment  --------+------------------------------------------------ |
| Denylist | Pass |
| NVML Library | Pass |
| CUDA Main Library | Pass |
| Permissions and OS Blocks | Pass |
| Persistence Mode | Pass |
| Environment Variables | Pass |
| Page Retirement/Row Remap | Pass |
| Graphics Processes | Pass |
| Inforom | Pass |

+-----  Integration  -------+------------------------------------------------+
| PCIe | Pass - All |
| Info | GPU 0 GPU to Host bandwidth:  14.26 GB/s, GPU |
| 0 Host to GPU bandwidth:  8.66 GB/s, GPU 0 b |
| idirectional bandwidth: 10.91 GB/s, GPU 0 GPU |
| to Host latency:  2.085 us, GPU 0 Host to GP |
| U latency:  2.484 us, GPU 0 bidirectional lat |
| ency:  3.813 us |

...
+-----  Hardware  ----------+------------------------------------------------+
| GPU Memory | Pass - All |
| Info | GPU 0 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 1 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 2 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 3 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 4 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 5 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 6 Allocated 83892938283 bytes (98.4%) |
| Info | GPU 7 Allocated 83892938283 bytes (98.4%) |

+-----  Stress  ------------+------------------------------------------------+

```
 **Node reservation:**  Because SmolLM3 was trained on a Slurm managed cluster, we booked a fixed 48-node reservation for the entire run. This setup allowed us to track the health and performance of the exact same nodes over time, it was also necessary to solve a data storage issues we discussed. We also reserved a spare node (like a car's spare wheel) so if one failed, we could swap it in immediately without waiting for repairs. While idle, the spare node ran eval jobs or dev experiments.

 **Continuous monitoring:**  During training, we tracked key metrics across all nodes such as GPU temperatures, memory usage, compute utilization and throughput fluctuations. We use [Prometheus](https://prometheus.io/) to collect [DCGM](https://github.com/NVIDIA/DCGM) metrics from all GPUs and visualize them in [Grafana](https://grafana.com/) dashboards for real-time monitoring. For detailed setup instructions on deploying Prometheus and Grafana for GPU monitoring on AWS infrastructure, see [this example setup guide](https://github.com/aws-samples/awsome-distributed-training/tree/3ae961d022399021cc4053c3ba19b182ca6b8dc8/4.validation_and_observability/4.prometheus-grafana). A Slack bot alerted us when any node showed suspicious behavior, allowing us to proactively replace failing hardware before it crashed the entire training run.



<a href="/screencapture-grafana-huggingface.pdf" target="_blank">Access to dashboard</a>
This multi-layered approach meant hardware issues became manageable interruptions.

 **Thermal Reality Check: When GPUs Slow Down** 

Marketing specs assume perfect cooling, but reality is messier. GPUs automatically reduce clock speeds when they overheat, cutting performance below theoretical maximums even in well-designed systems.

<Reference caption="This Grafana dashboard shows thermal throttling events across our GPU cluster. The bars in the bottom panel indicate when GPUs automatically reduced clock speeds due to overheating.">

<Image src={image_27d1384e_bcac_80b1_9ffb_ec29d0021ccc} alt="Image" />
</Reference>

We monitor the  `DCGM_FI_DEV_CLOCK_THROTTLE_REASONS`  metric from [NVIDIA's DCGM](https://github.com/NVIDIA/DCGM/tree/master) to detect thermal throttling. When this metric shows non-zero values, the GPUs are automatically reducing clock speeds due to overheating. The dashboard above shows how these throttling events manifest in practice.

Thermal throttling doesn't just hurt the affected GPU; it cascades across your entire distributed training setup. During our testing, we observed how a single throttling node can dramatically impact collective communication performance.

<HtmlEmbed 
  src="/embeds/nccl-all-reduce-bandwidth-plot-throttling.html" 
  caption="AllReduce bandwidth degradation across nodes during our stress testing. The sharp drop after 14 nodes (from 350 GB/s to 100 GB/s) was caused by a single thermally throttled GPU, demonstrating how one slow node can bottleneck the entire distributed training pipeline."
/>



The chart above shows AllReduce bandwidth degrading as we scale from 1 to 16 nodes. Notice the sharp drop after 14 nodes, from  **350 GB/s**  to  **100 GB/s** while we expect the bandwidth to stay above 300GB/s as we've seen before. This wasn't a network issue: a single node with thermal throttling became the bottleneck, forcing all other nodes to wait during gradient synchronization. In distributed training, you're only as fast as your slowest node.

👉  **Key lesson:**  Before committing to long training runs, stress-test your hardware using the tools mentioned earlier to identify thermal and power limitations. Monitor temperatures continuously using DCGM telemetry and plan for real-world thermal limits. It's also good practice to verify that GPU clocks are set to maximum performance. For a deeper dive into why GPUs can't sustain their advertised performance due to power constraints, see [this excellent analysis on power throttling](https://www.thonking.ai/p/strangely-matrix-multiplications).

####  **Checkpoint Management** 

Checkpoints are our safety net during long training runs. We save them regularly for three practical reasons: recovering from failures, monitoring training progress through evaluation and sharing intermediate models with the community for research. The recovery aspect matters most. If our run fails, we want to restart from the latest saved checkpoint so we lose at most the save interval if we resume immediately (e.g., 4 hours of training if we save every 4 hours).

<Note title="Automate your resume process" emoji="💡" variant="tip">

Try to automate your resume process. On Slurm, for example, you can you can just use `SBATCH --requeue` so the job restarts automatically from the latest checkpoint. That way, you avoid losing time waiting for someone to notice the failure and manually restart.
</Note>

There's two important details to keep in mind when implementing your resume mechanism:

- Checkpoint saving should happen in the background without impacting training throughput.
- Watch your storage, over a 24-day run, saving every 4 hours means ~144 checkpoints. With large models and optimizer states, this adds up fast. In our case, we store only one local checkpoint (the latest saved) at a time and offload the rest to S3 to avoid filling up cluster storage.

 **A painful lesson from the past:** 

During our first large-scale run (StarCoder 15B), training proceeded smoothly through multiple restarts. On the final day, we discovered the entire checkpoint folder had been deleted by a leftover  `rm -rf $CHECKPOINT_PATH`  command at the very end of the script from old throughput tests. This destructive command only triggered when the Slurm job actually finished, which hadn't happened in previous restarts.

Luckily, we had the checkpoint from the day before saved, so it only cost us one day of retraining. The takeaways were clear: never leave destructive commands in production scripts, and automate checkpoint backups immediately after saving rather than relying on manual intervention.

In our nanotron trainings, we save checkpoints every 2 hours locally, immediately upload each one to S3, then delete the local copy once backup is confirmed. On resume, we pull from S3 if the latest checkpoint isn't available locally. This approach saves storage, ensures backups, and enables quick recovery.

####  **Automated Evaluations** 

Running evaluations manually becomes a bottleneck fast. They look simple until you're doing them repeatedly. Running benchmarks, tracking and plotting results for every run adds up to significant overhead. The solution? Automate everything upfront.

For SmolLM3, we use [LightEval](https://github.com/huggingface/lighteval) to run evaluations on nanotron checkpoints. Every saved checkpoint triggers an evaluation job on the cluster. The results are pushed directly to Weights & Biases or [Trackio](https://github.com/gradio-app/trackio), so we just open the dashboard and watch the curves evolve. This saved us a huge amount of time and kept eval tracking consistent throughout the run.

If you can automate only one thing in your training setup, automate evaluations. 

Finally, lets have a look at how we can optimize the training layout, ie how the model is distributed across the available GPUs, to maximize the throughput.

### Optimizing Training Throughput

####  **How Many GPUs Do We Need?** 

Great question! After all this talk about specs and benchmarks, you still need to figure out the practical question: how many GPUs should you actually rent or buy?

Determining the right number of GPUs requires balancing training time, cost, and scaling efficiency. Here's the framework we used:

 **Basic Sizing Formula:** 

$$
\text{GPU Count} = \frac{\text{Total FLOPs Required}}{\text{Per-GPU Throughput} \times \text{Target Training Time}}
$$

This formula breaks down the problem into three key components:

-  **Total FLOPs Required** : The computational work needed to train your model (depends on model size, training tokens, and architecture)
-  **Per-GPU Throughput** : How many FLOPs per second each GPU can actually deliver (not theoretical peak!)
-  **Target Training Time** : How long you're willing to wait for training to complete

The key insight: you need to estimate  **realistic throughput** , not peak specs. This means accounting for Model FLOPs Utilization (MFU): the percentage of theoretical peak performance you actually achieve in practice.

For SmolLM3, our calculation looked like this:

-  **Model size** : 3B parameters
-  **Training tokens** : 11 trillion tokens
-  **Target training time** : ~4 weeks
-  **Expected MFU** : 30% (based on similar scale experiments)

First, we calculate total FLOPs needed using the standard transformer approximation of  **6N FLOPs per token**  (where N = parameters):

$$
\text{Total FLOPs} = 6 \times 3 \times 10^9 \text{ params} \times 11 \times 10^{12} \text{ tokens} = 1.98 \times 10^{23} \text{ FLOPs}
$$

With our expected MFU of 30%, our effective per-GPU throughput becomes:

$$
\text{Effective Throughput} = 720 \times 10^{12} \text{ FLOPs/sec} \times 0.30 = 216 \times 10^{12} \text{ FLOPs/sec}
$$

Now plugging into our sizing formula:

$$
\text{GPU Count} = \frac{1.98 \times 10^{23} \text{ FLOPs}}{216 \times 10^{12} \text{ FLOPs/sec} \times 4 \text{ weeks} \times 604,800 \text{ sec/week}}
$$

$$
= \frac{1.98 \times 10^{23}}{5.23 \times 10^{20}} \approx 379 \text{ GPUs}
$$

This calculation pointed us toward 375-400 H100s, and we secured 384 H100s, a number that aligned well with our parallelism strategy and gave us a realistic 4-week timeline with some buffer for unexpected issues like node failures and restarts.

 **Why More GPUs Isn't Always Better: Amdahl's Law in Action** 

Here's a counterintuitive truth:  **adding more GPUs can actually make your training slower** . This is where [Amdahl's Law](https://en.wikipedia.org/wiki/Amdahl%27s_law) comes into play.

Amdahl's Law states that the speedup from parallelization is fundamentally limited by the serial (non-parallelizable) portion of your workload. In LLM training, this "serial" portion is primarily  **communication overhead:** the time spent synchronizing gradients/weights/activations across GPUs that can't be parallelized away (read more [here](https://acenet-arc.github.io/ACENET_Summer_School_General/05-performance/index.html)).

The formula is:  **Maximum Speedup = 1 / (Serial Fraction + Parallel Fraction / Number of Processors)** 

<Wide>

<HtmlEmbed 
  src="/embeds/amdahl-law-gpu-scaling.html" 
  title="GPU Scaling in distributed LLM Training: Amdahl's Law in action" 
/>
</Wide>



For SmolLM3's 3B model, if communication takes 10% of each training step, then no matter how many GPUs you add, you'll never get more than 10x speedup. Worse, as you add more GPUs, the communication fraction often  *increases*  because:

- More GPUs = more AllReduce participants = longer synchronization
- Network latency/bandwidth becomes the bottleneck
- Small models can't hide communication behind compute

For SmolLM3, we used weak scaling principles: our global batch size scaled with our GPU count, maintaining roughly 8K tokens per GPU globally. This kept our communication-to-computation ratio reasonable while maximizing throughput.

#### Finding the Optimal Parallelism Configuration

Once you've secured your GPUs, the next challenge is configuring them to actually train efficiently. For this the parallelism strategy becomes critical.

We follow the [ **Ultra-Scale Playbook** ](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=step_1%3A_fitting_a_training_step_in_memory)['s approach to finding optimal training configurations](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=step_1%3A_fitting_a_training_step_in_memory). The playbook breaks the problem into three sequential steps: first ensure the model fits in memory, then achieve your target batch size, and finally optimize for maximum throughput. Let's walk through how we applied this to SmolLM3.

<Sidenote>

For detailed explanations of different parallelism strategies (Data Parallelism, Tensor Parallelism, Pipeline Parallelism, ZeRO, etc.), we urge you once again to checkout the [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook) \*inserts Bernie meme\*
</Sidenote>

#### Step 1: Fitting a training step in memory

The first question is simple: does our SmolLM3 3B model even fit in a single H100's 80GB of memory? To answer this, we use [nanotron's ](https://huggingface.co/spaces/nanotron/predict_memory)[ `predict_memory` ](https://huggingface.co/spaces/nanotron/predict_memory)[ tool](https://huggingface.co/spaces/nanotron/predict_memory), which estimates memory consumption for model parameters, optimizer states, gradients, and activations.

<Wide>

<HtmlEmbed src="/embeds/d3-memory-analysis.html" caption="Memory timeline from <a href='https://huggingface.co/spaces/nanotron/predict_memory'>

nanotron's predict_memory tool</a> showing SmolLM3 3B peaks at 74GB, approaching the H100's 80GB limit." />
</Wide>



The results show we're pushing close to the 80GB limit. This means we need some form of parallelism that reduces per-GPU memory footprint, whether that's Tensor Parallelism (splitting model layers across GPUs), Pipeline Parallelism (splitting model depth across GPUs), or ZeRO optimizer sharding (distributing optimizer states). Without at least one of these strategies, we won't be able to train efficiently or at all.

#### Step 2: Achieving the target global batch size

Now that we know the model fits in memory with some form of parallelism, we need to determine how to achieve our target global batch size (GBS) of approximately 2 million tokens. This constraint gives us our first equation:

$$
\text{GBS} = \text{DP} \times \text{MBS} \times \text{GRAD\_ACC} \times \text{SEQLEN} \approx 2\text{M tokens}
$$

Where:

-  **DP (Data Parallelism)** : Number of data-parallel replicas
-  **MBS (Micro Batch Size)** : Tokens processed per GPU per micro-batch
-  **GRAD_ACC (Gradient Accumulation)** : Number of forward-backwards before optimizer step
-  **SEQLEN (Sequence Length)** : Tokens per sequence (4096 for the 1st pretraining stage)

We also have a hardware constraint from our 384 H100s:

$$
\text{DP} \times \text{TP} \times \text{PP} = 384 = 2^7 \times 3
$$

Where:

-  **TP (Tensor Parallelism)** : GPUs per model layer (splits weight matrices)
-  **PP (Pipeline Parallelism)** : GPUs per model depth (splits layers vertically)

These two equations define our search space. We need to find values that satisfy both constraints while maximizing training throughput.

#### Step 3: Optimizing training throughput

With our constraints established, we need to find the parallelism configuration that maximizes training throughput. The search space is defined by our hardware topology and model architecture.

Our hardware setup presents two distinct types of interconnects, as we saw in the section above: NVLink for intra-node communication (900 GB/s) and EFA for inter-node communication (~50 GB/s). This topology naturally suggests using at least two forms of parallelism to match our network characteristics. The dramatic bandwidth difference between these interconnects will heavily influence which parallelism strategies work best.

From a model perspective, SmolLM3's architecture constrains our options. Since we're not using a Mixture-of-Experts architecture, we don't need  **Expert Parallelism** . Similarly, training with a 4096 sequence length in the first stage means  **Context Parallelism**  isn't required. This leaves us with three primary parallelism dimensions to explore:  **Data Parallelism**  (DP),  **Tensor Parallelism**  (TP), and  **Pipeline Parallelism**  (PP).

Given our constraints from Step 2, we need to sweep across several parameters:

-  **DP with ZeRO variants**  (ZeRO-0, ZeRO-1, ZeRO-3): Values from 1 to 384, constrained to multiples of 2 and/or 3
-  **TP**  (1, 2, 3, 4, 6, 8): Keep within a single node to fully leverage NVLink's high bandwidth
-  **PP**  (1..48): Split model depth across GPUs
-  **MBS**  (2, 3, 4, 5): Depending on memory savings from parallelism, we can increase MBS to better utilize Tensor Cores
-  **Activation checkpointing**  (none, selective, full): Trade additional compute for reduced memory and communication
-  **Kernel optimizations** : CUDA graphs and optimized kernels where available

While this may seem like an overwhelming number of combinations, a practical approach is to benchmark each dimension independently first, then eliminate configurations that significantly hurt throughput. The key insight is that not all parallelism strategies are created equal. Some introduce communication overhead that far outweighs their benefits, especially at our scale.

In our case,  **Pipeline Parallelism**  showed poor performance characteristics. PP requires frequent pipeline bubble synchronization across nodes, and with our relatively small 3B model, the communication overhead dominated any potential benefits. Additionally, we didn't have access to highly efficient PP schedules that could eliminate the pipeline bubble entirely, which further limited PP's viability. Similarly,  **ZeRO**  levels above 0 introduced significant all-gather and reduce-scatter operations that hurt throughput more than they helped with memory. These early benchmarks allowed us to narrow our search space dramatically, focusing on configurations that combined  **Data Parallelism** with modest  **Tensor Parallelism** .

👉 To evaluate each configuration, we run benchmarks for 5 iterations and record  **tokens per second per GPU (tok/s/gpu)** , which is ultimately the metric we care about. We use Weights & Biases and Trackio to log throughputs and configurations, making it easy to compare different parallelism strategies.

After systematically benchmarking the available options in nanotron, we settled on  **DP = 192** , which leverages inter-node EFA bandwidth for data-parallel gradient synchronization. This means 192 independent model replicas, each processing different batches of data. For Tensor Parallelism, we chose  **TP = 2** , keeping tensor-parallel communication within a single node to fully exploit NVLink's high bandwidth. This splits each layer's weight matrices across two GPUs, requiring fast communication for the forward and backward passes.

Our  **Micro Batch Size = 3**  strikes a balance between memory usage and compute efficiency. Larger batch sizes would better utilize Tensor Cores, but we're already pushing close to memory limits. Finally, we opted for ZeRO-0, meaning no optimizer state sharding. While ZeRO-1 or ZeRO-3 could reduce memory footprint, the communication overhead from gathering and scattering optimizer states across our 384 GPUs would significantly hurt throughput.

<Sidenote>

Many of these parallelism decisions were influenced by the state of libraries at the time of experiments. For instance, nanotron didn't support ZeRO-3 yet, and we lacked access to highly optimized Pipeline Parallelism schedules that could eliminate pipeline bubbles. As the framework evolves, some of these trade-offs may shift. Contributions are always welcome!
</Sidenote>

This configuration achieves our target global batch size of approximately 2 million tokens (192 × 3 × 1 × 4096 ≈ 2.3M) while maximizing throughput on our 384 H100 cluster. You can see the full training configuration in our [stage1_8T.yaml](https://github.com/huggingface/smollm/blob/main/text/pretraining/smollm3/stage1_8T.yaml).

## Conclusion

We started this journey with a simple question: what does it actually take to train a high-performance LLM in 2025? After walking through the complete pipeline—from pretraining to post-training—we've shown you not just the techniques, but the methodology that makes them work.

 **Pretraining at scale.**  We walked through the Training Compass framework for deciding whether to train at all, then showed how to translate goals into concrete architectural decisions. You've seen how to set up reliable ablation pipelines, test changes individually, and scale from few-billion-token experiments to multi-trillion-token runs. We documented the infrastructure challenges that can emerge at scale (throughput collapses, dataloader bottlenecks, subtle bugs) and how monitoring and systematic derisking help you catch them early and debug quickly.

 **Post-training in practice.**  We showed that going from a base model to a production assistant requires its own systematic approach: establishing evals before training anything, iterating on SFT data mixtures, applying preference optimization, and optionally pushing further with RL. You've seen how vibe testing catches bugs that metrics miss, how chat templates can silently break instruction-following, and why data mixture balance matters as much in post-training as it does in pretraining.

Throughout both phases, we kept coming back to the same core insights: validate everything through experiments, change one thing at a time, expect scale to break things in new ways, and let your use case drive decisions rather than chasing every new paper. Following this process, we trained SmolLM3: a competitive 3B multilingual reasoner with long context. Along the way, we learned a lot about what works, what breaks, and how to debug when things go wrong. We've tried to document it all, the successes and failures alike.

 **What's next?** 

This blog covers the fundamentals of modern LLM training, but the field evolves rapidly. Here are ways to go deeper:

-  **Run experiments yourself.**  Reading about ablations is useful; running your own teaches you what actually matters. Pick a small model, set up evals, and start experimenting.
-  **Read the source code.**  Training frameworks like nanotron, TRL, and others are open source. Understanding their implementations reveals details that papers gloss over.
-  **Follow recent work.**  Papers of recent state-of-the-art models show where the field is heading. The references section contains our curated list of impactful papers and resources.

We hope this blog helps you approach your next training project with clarity and confidence, whether you're at a large lab pushing the frontier or a small team solving a specific problem.

Now go train something. And when your loss spikes mysteriously at 2am, remember: every great model has debugging stories behind it. May the force of open source and open science always be with you!

####  **Acknowledgments** 

We thank [Guilherme](https://huggingface.co/guipenedo), [Hugo](https://huggingface.co/hlarcher) and [Mario](https://huggingface.co/mariolr) for their valuable feedback, and [Abubakar](https://huggingface.co/abidlabs) for his help with Trackio features.

## References

Below is a curated list of papers, books, and blog posts that have informed us the most on our LLM training journey.

#### LLM Architecture 

- Dense models: [Llama3](https://huggingface.co/papers/2407.21783), [Olmo2](https://huggingface.co/papers/2501.00656), [MobileLLM](https://huggingface.co/papers/2402.14905)
- MoEs: [DeepSeek V2](https://huggingface.co/papers/2405.04434), [DeepSeek V3](https://huggingface.co/papers/2412.19437), [Scaling Laws of Efficient MoEs](https://huggingface.co/papers/2507.17702)
- Hybrid: [MiniMax-01,](https://huggingface.co/papers/2501.08313) [Mamba2](https://huggingface.co/papers/2405.21060)

#### Optimisers & training parameters

-  [Muon is Scalable for LLM Training](https://huggingface.co/papers/2502.16982), [Fantastic pretraining optimisers](https://huggingface.co/papers/2509.02046)
- [Large Batch Training](https://arxiv.org/abs/1812.06162), [DeepSeekLLM](https://arxiv.org/abs/2401.02954)

#### Data curation 

- Web: [FineWeb & FineWeb-Edu](https://huggingface.co/papers/2406.17557), [FineWeb2](https://huggingface.co/papers/2506.20920), [DCLM](https://huggingface.co/papers/2406.11794)
- Code: [The Stack v2](https://huggingface.co/papers/2402.19173), [To Code or Not to Code](https://huggingface.co/papers/2408.10914)
- Math: [DeepSeekMath](https://huggingface.co/papers/2402.03300), [FineMath](https://huggingface.co/papers/2502.02737), [MegaMath](https://huggingface.co/papers/2504.02807)
- Data mixtures: [SmolLM2](https://huggingface.co/papers/2502.02737), [Does your data spark joy](https://huggingface.co/papers/2406.03476)

#### Scaling laws 

- [Kaplan](https://huggingface.co/papers/2001.08361), [Chinchilla](https://huggingface.co/papers/2203.15556), [Scaling Data-Constrained Language Models](https://huggingface.co/papers/2305.16264)

#### Post-training

- [InstructGPT:](https://huggingface.co/papers/2203.02155) OpenAI's foundational paper to turn base models into helpful assistants. The precursor to ChatGPT and a key step on humanity's path up the Kardashev scale.
- [Llama 2](https://huggingface.co/papers/2307.09288) & [3](https://huggingface.co/papers/2407.21783): Extremely detailed tech reports from Meta on the training behind their Llama models (may they rest in peace). They each contain many insights into human data collection, both for human preferences and model evaluation.
- Secrets of RLHF in LLMs, [Part I](https://huggingface.co/papers/2307.04964) & [II](https://huggingface.co/papers/2401.06080): these papers contain lots of goodies on the nuts and bolts for RLHF, specifically on how to train strong reward models. 
- [Direct Preference Optimisation:](https://huggingface.co/papers/2305.18290) the breakthrough paper from 2023 that convinced everyone to stop doing RL with LLMs.
- [DeepSeek-R1](https://huggingface.co/papers/2501.12948): the breakthrough paper from 2025 that convinced everyone to start doing RL with LLMs.
- [Dr. GRPO:](https://huggingface.co/papers/2503.20783) one of the most important papers on understanding the baked-in biases with GRPO and how to fix them.
- [DAPO: ](https://huggingface.co/papers/2503.14476)Bytedance shares many implementation details to unlock stable R1-Zero-like training for the community.
- [ScaleRL:](https://huggingface.co/papers/2510.13786) a massive flex from Meta to derive scaling laws for RL. Burns over 400k GPU hours to establish a training recipe that scales reliably over many orders of compute.
- [LoRA without Regret:](https://thinkingmachines.ai/blog/lora/) a beautifully written blog post which finds that RL with low-rank LoRA can match full-finetuning (a most surprising result).
- [Command A:](https://huggingface.co/papers/2504.00698) a remarkably detailed tech report from Cohere on various strategies to post-train LLMs effectively.

#### Infrastructure

- [UltraScale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
- [Jax scaling book](https://jax-ml.github.io/scaling-book/)
- [Modal GPU Glossary](https://modal.com/gpu-glossary/readme)

#### Training frameworks

- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
- [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)
- [Torchtitan](https://github.com/pytorch/torchtitan)
- [Nanotron](https://github.com/huggingface/nanotron/)
- [NanoChat](https://github.com/karpathy/nanochat)
- [TRL](https://github.com/huggingface/trl)

#### Evaluation

- [The LLM Evaluation Guidebook](https://github.com/huggingface/evaluation-guidebook)
- [OLMES](https://huggingface.co/papers/2406.08446)
- [FineTasks](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks)
- [Lessons from the trenches](https://huggingface.co/papers/2405.14782)
